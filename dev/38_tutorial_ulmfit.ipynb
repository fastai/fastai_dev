{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.torch_basics import *\n",
    "from local.test import *\n",
    "from local.core import *\n",
    "from local.layers import *\n",
    "from local.data.all import *\n",
    "from local.notebook.showdoc import show_doc\n",
    "from local.optimizer import *\n",
    "from local.learner import *\n",
    "from local.metrics import *\n",
    "from local.text.core import *\n",
    "from local.text.data import *\n",
    "from local.text.models.core import *\n",
    "from local.text.models.awdlstm import *\n",
    "from local.text.learner import *\n",
    "from local.callback.rnn import *\n",
    "from local.callback.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning in text\n",
    "\n",
    "> How to fine-tune a language model and train a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune a pretrained Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get our data and tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "df = pd.read_csv(path/'texts.csv')\n",
    "df_tok,count = tokenize_df(df, 'text')\n",
    "texts,lens = df_tok['text'],df_tok['text_lengths'].values.astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we put it in a `DataSource`. For a language model, we don't have targets, so there is only one transform to numericalize the texts. Note that `tokenize_df` returns the count of the words in the corpus to make it easy to create a vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = RandomSplitter()(texts)\n",
    "vocab = make_vocab(count)\n",
    "dsrc = DataSource(texts, [Numericalize(vocab)], filts=splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use that `DataSource` to create a `DataBunch`. Here the class of `TfmdDL` we need to use is `LMDataLoader` which will concatenate all the texts in a source (with a shuffle at each epoch for the training set), split it in `bs` chunks then read continuously through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbunch = LMDataLoader.dbunchify(dsrc, lens=lens, bs=64, seq_len=72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj fair drama / love story movie that focuses on the lives of blue collar people finding new life thru new xxunk acting here is good but the film fails in cinematography , screenplay , directing and xxunk story / script is only average at xxunk film will be enjoyed by xxmaj fonda and xxmaj de xxmaj niro fans and by people who love middle age love stories where in the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>strictly xxunk . a must see . xxbos a complete waste of time \\n\\n xxmaj xxunk xxmaj xxunk is a complete waste of time . xxmaj the script and dialogues are poorly written , the direction is xxunk and the acting borders on xxunk movie was clearly aiming for the xxmaj xxunk xxmaj de xxmaj xxunk crowd but it falls far short of the mark because it does not have even one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pay you enough . \" xxbos xxmaj this movie was probably the worst movie i have ever seen . xxmaj here are the things that immediately jump out at me : 1 . xxmaj the woods were more like hills in xxmaj los xxmaj angeles with a couple trees and xxunk . xxmaj not scary whatsoever . xxmaj news flash , if you are filming in the xxmaj southern xxmaj california area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>movie him and her are working together to take down the bad guys and every time they exchange words it just feels like the cheap lines given before a sex scene in a porn movie . xxmaj horrible . xxmaj do n't waste your time . xxbos xxmaj for the big xxunk among us , \" the xxmaj intruder \" is a xxunk incoherent movie from xxmaj france that gives so -</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>south ; not because you might witness a murder , but you may run across a xxunk plantation . ) xxmaj this is the point where the story , not the movie , actually comes close to being good . \\n\\n xxmaj while xxmaj tony xxmaj march will never have to practice his xxmaj oscar speech , his xxmaj sheriff xxmaj dean becomes a creepy xxunk of a normal guy torn by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bored with his career ; he xxunk agrees to play xxmaj othello . xxmaj he gets deep into character as a jealous and murderous man . xxmaj he begins walking a thin line between xxunk and reality and ends up confusing his role with his own life and eventually kills his xxunk xxmaj xxunk has no memory of the xxunk xxunk . \\n\\n xxmaj colman seems xxunk in this role . xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>these people can afford to live in a nice xxmaj manhattan xxunk considering what they do for a living ? xxup xxunk just loves to insult the viewer 's intelligence , even if they 're just around xxmaj xxunk 's level . i know a person who makes $ 100 , xxrep 3 0 a year as a web designer and lives in a xxunk one - bedroom apartment in xxmaj manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dead \" ; \" xxunk , bring me the big knife \" , \" who 's dead \" , \" do you love him xxmaj loretta … .. , good because when you do , they drive you crazy because they know they can \" . i always put xxmaj moonstruck on when there 's nothing good to watch because it makes me happy . xxbos xxmaj one more of those brilliant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n\\n xxmaj the heavy metal score -- was someone making a pun ? -- is , at times , xxunk annoying , but the cinematography by xxmaj frank xxmaj stanley is knockout , particularly the xxunk scenes . xxbos xxmaj the name ( xxunk ) of the main character is the xxmaj german word for \" woman \" . i do n't know if that was intentional or not , but if</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>, but that does n't mean it 's high quality . xxmaj yes , millions of xxmaj xxunk fans xxup can be wrong . \\n\\n xxmaj i 'll be the first to admit , that part of the problem for me was the xxunk - hype over the film . i waited a month or so to see it and ultimately , it did n't live up to the expectations set upon</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbunch.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have a convenience method to directly grab a `Learner` from it, using the `AWD_LSTM` architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(dbunch, AWD_LSTM, vocab, metrics=[accuracy, Perplexity()], path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.800079</td>\n",
       "      <td>4.339657</td>\n",
       "      <td>0.258123</td>\n",
       "      <td>76.681259</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.758967</td>\n",
       "      <td>4.313660</td>\n",
       "      <td>0.259611</td>\n",
       "      <td>74.713448</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.740184</td>\n",
       "      <td>4.302836</td>\n",
       "      <td>0.260510</td>\n",
       "      <td>73.909134</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.739078</td>\n",
       "      <td>4.301117</td>\n",
       "      <td>0.260835</td>\n",
       "      <td>73.782196</td>\n",
       "      <td>00:04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(4, 1e-2, moms=(0.8,0.7,0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have fine-tuned the pretrained language model to this corpus, we save the encoder since we will use it for the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use it to train a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification, we need to use two set of transforms: one to numericalize the texts and the other to encode the labels as categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = RandomSplitter()(range_of(df_tok))\n",
    "dsrc = DataSource(df_tok, filts=splits, tfms=[\n",
    "    [attrgetter(\"text\"), Numericalize(vocab)],\n",
    "    [attrgetter(\"label\"), Categorize()]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We opnce again use a subclass of `TfmdDL` for the dataloaders, since we want to sort the texts (sortish for the training set) by order of lengths. We also use `pad_collate` to create batches form texts of different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_dl = SortedDL(dsrc.train, create_batch=pad_collate, after_batch=[Cuda], shuffle=True, drop_last=True)\n",
    "val_dl = SortedDL(dsrc.valid, create_batch=pad_collate, after_batch=[Cuda])\n",
    "dbunch = DataBunch(trn_dl, val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj now that xxmaj che(2008 ) has finished its relatively short xxmaj australian cinema run ( extremely limited xxunk screen in xxmaj xxunk , after xxunk ) , i can xxunk join both xxunk of \" at xxmaj the xxmaj movies \" in taking xxmaj steven xxmaj soderbergh to task . \\n\\n xxmaj it 's usually satisfying to watch a film director change his style / subject , but xxmaj soderbergh 's most recent stinker , xxmaj the xxmaj girlfriend xxmaj xxunk ) , was also missing a story , so narrative ( and editing ? ) seem to suddenly be xxmaj soderbergh 's main challenge . xxmaj strange , after xxunk years in the business . xxmaj he was probably never much good at narrative , just xxunk it well inside \" edgy \" projects . \\n\\n xxmaj none of this excuses him this present , almost diabolical failure . xxmaj as xxmaj david xxmaj stratton xxunk , \" two parts of xxmaj che do n't ( even ) make a whole \" . \\n\\n xxmaj epic xxunk in name only , xxmaj che(2008 ) barely qualifies as a feature film ! xxmaj it certainly has no legs , xxunk as except for its xxunk ultimate resolution forced upon it by history , xxmaj soderbergh 's xxunk - long xxunk just goes nowhere . \\n\\n xxmaj even xxmaj margaret xxmaj xxunk , the more xxunk of xxmaj australia 's xxmaj at xxmaj the xxmaj movies duo , noted about xxmaj soderbergh 's xxunk waste of ( xxunk digital xxunk ) : \" you 're in the woods … you 're in the woods … you 're in the woods … \" . i too am surprised xxmaj soderbergh did n't give us another xxunk of xxup that somewhere between his xxunk two xxmaj parts , because he still left out massive xxunk of xxmaj che 's \" xxunk \" life ! \\n\\n xxmaj for a xxunk of an important but infamous historical figure , xxmaj soderbergh xxunk xxunk , if not deliberately insults , his audiences by \\n\\n 1 . never providing most of xxmaj che 's story ; \\n\\n 2 . xxunk xxunk film xxunk with mere xxunk xxunk ; \\n\\n 3 . xxunk both true hindsight and a narrative of events ; \\n\\n 4 . barely developing an idea , or a character ; \\n\\n 5 . remaining xxunk episodic ; \\n\\n 6 . xxunk proper context for scenes xxrep 3 - whatever we do get is xxunk in xxunk xxunk ; \\n\\n 7 . xxunk xxunk all audiences ( even xxmaj spanish - xxunk will be confused by the xxunk xxunk in xxmaj english ) ; and \\n\\n 8 . xxunk xxunk his main subject into one dimension . xxmaj why , at xxup this late stage ? xxmaj the xxmaj t - shirt franchise has been a success ! \\n\\n xxmaj our sense of xxunk is surely due to xxmaj peter xxmaj xxunk and xxmaj benjamin xxunk xxmaj xxunk xxunk their screenplay solely on xxmaj xxunk 's memoirs . xxmaj so , like a poor student who has read only xxup one of his xxunk xxunk for his xxunk , xxmaj soderbergh 's product is xxunk limited in perspective . \\n\\n xxmaj the audience is held captive within the same xxunk knowledge , scenery and circumstances of the \" revolutionaries \" , but that does n't xxunk our sympathy . xxmaj instead , it xxunk on us that \" ah , xxmaj soderbergh 's trying to xxunk his audiences the same as the xxmaj latino peasants were at the time \" . xxmaj but these are the xxup same illiterate xxmaj latino peasants who sold out the good doctor to his enemies . xxmaj why does xxmaj soderbergh feel the need to xxunk us with them , and keep us equally mentally captive ? xxmaj such audience xxunk must have a purpose . \\n\\n xxmaj part2 is more xxunk than xxmaj part1 , but it 's literally mind - numbing with its repetitive bush - bashing , misery of outlook , and lack of variety or character xxunk . deltoro 's xxmaj che has no opportunity to grow as a person while he struggles to xxunk his own ill - xxunk troops . xxmaj the only xxunk is the humour as xxmaj che deals with his sometimes deeply ignorant \" revolutionaries \" , some of whom xxunk lack self - control around local peasants or food . xxmaj we certainly get no insight into what caused the conditions , nor any xxunk xxunk of their xxunk xxunk , such as it was . \\n\\n xxmaj part2 's excruciating xxunk remains xxunk episodic : again , nothing is telegraphed or xxunk . xxmaj thus even the scenes with xxmaj xxunk xxmaj xxunk ( xxunk xxmaj xxunk ) are unexpected and disconcerting . xxmaj any xxunk events are portrayed xxunk and xxmaj latino - xxunk , with xxmaj part1 's interviews replaced by time - xxunk xxunk between the corrupt xxmaj xxunk president ( xxunk de xxmaj xxunk ) and xxup us xxmaj government xxunk promising xxup cia xxunk ( ! ) . \\n\\n xxmaj the rest of xxmaj part2 's \" woods \" and day - for - night blue xxunk just xxunk the audience until they 're xxunk the xxunk . \\n\\n xxmaj perhaps deltoro felt too xxunk the frustration of many non - american xxmaj latinos about never getting a truthful , xxunk history of xxmaj che 's exploits within their own countries . xxmaj when foreign xxunk still wo n't deliver a free press to their people -- for whatever reason -- then one can see how a popular xxmaj american indie producer might set out to entice the not - so - well - read ( \" i may not be able to read or write , but xxmaj i 'm xxup not xxunk xxmaj inspector xxmaj xxunk ) ) out to their own local cinemas . xxmaj the film 's obvious xxunk and gross over - xxunk hint very strongly that it 's aiming only at the xxunk of the less - informed xxup who xxup still xxup speak xxup little xxmaj english . xxmaj if they did , they 'd have read xxunk on the subject already , and xxunk the relevant social issues amongst themselves -- learning the lessons of history as they should . \\n\\n xxmaj such insights are precisely what societies still need -- and not just the remaining illiterate xxmaj latinos of xxmaj central and xxmaj south xxmaj america -- yet it 's what xxmaj che(2008 ) xxunk fails to deliver . xxmaj soderbergh xxunk his lead because he 's weak on narrative . i am xxunk why xxmaj xxunk deltoro deliberately chose xxmaj soderbergh for this project if he knew this . xxmaj it 's been xxunk , hindsight about xxmaj xxunk was xxunk wanted : it 's what i went to see this film for , but the director xxunk robs us of that . \\n\\n xxmaj david xxmaj stratton , writing in xxmaj the xxmaj australian ( xxunk ) observed that while xxmaj part1 was \" uneven \" , xxmaj part2 actually \" goes rapidly downhill \" from there , \" xxunk xxmaj che 's final campaign in xxmaj xxunk in excruciating detail \" , which \" … feels almost unbearably slow and turgid \" . \\n\\n che : the xxmaj xxunk aka xxmaj part2 is certainly no xxunk for xxmaj xxunk , painting it a picture of misery and xxunk . xxmaj the entire second half is only redeemed by the aforementioned humour , and the dramatic -- yet tragic -- capture and execution of the film 's subject . \\n\\n xxmaj the rest of this xxunk cinema xxunk is just confusing , irritating misery -- xxunk , for a xxmaj soderbergh film , to be avoided at all costs . xxmaj it is bound to break the hearts of all who know even just a xxunk about the xxunk / 10 )</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxmaj this film sat on my xxmaj xxunk for weeks before i watched it . i xxunk a self - indulgent xxunk flick about relationships gone bad . i was wrong ; this was an xxunk xxunk into the screwed - up xxunk of xxmaj new xxmaj xxunk . \\n\\n xxmaj the format is the same as xxmaj max xxmaj xxunk ' \" la xxmaj xxunk , \" based on a play by xxmaj arthur xxmaj xxunk , who is given an \" inspired by \" credit . xxmaj it starts from one person , a prostitute , standing on a street corner in xxmaj brooklyn . xxmaj she is picked up by a home contractor , who has sex with her on the hood of a car , but ca n't come . xxmaj he refuses to pay her . xxmaj when he 's off xxunk , she answers his cell phone and takes a message . xxmaj she runs away with his keys . \\n\\n xxmaj then the story switches to the contractor , who pays a professional call on a rich , bored xxmaj new xxmaj york woman , who plays with him until he is xxunk , then she pulls away . xxmaj she tells him how desperate and unhappy she is ; he tells her how beautiful she is , and lucky . xxmaj as he is leaving , she asks if he would have sex with her . xxmaj she sits on top of him , xxunk up and down . xxmaj this time he comes , the he leaves . \\n\\n xxmaj the woman and her husband throw a dinner party for their xxunk friends . xxmaj xxunk ( robert ) is talking business , wife ( ellen ) is bored , and switches the subject to sex , and how often men and women think about it . xxmaj husband switches conversation to desert . xxmaj later , after the xxunk leave , xxmaj ellen tries to entice xxmaj robert into sex . xxmaj robert wants none of it , and puts on a jazz record . xxmaj ellen turns on the radio ; xxmaj robert turns up the music ; xxmaj ellen turns on the xxup tv ; xxmaj robert turns on another xxup tv . xxmaj xxunk ensues . xxmaj ellen goes up on the roof , xxmaj robert joins her . xxmaj ellen confesses that she needs to experience more men , men other than xxmaj robert . xxmaj robert says that he too needs to experience men . \\n\\n xxmaj we next follow xxmaj robert as he visits an artist , xxmaj martin , played by xxmaj steve xxmaj buscemi . i wish xxmaj buscemi could have more roles like this , where he is a sexy , smart , totally xxunk guy . xxmaj robert xxunk xxmaj martin 's work , much more than it deserves , xxunk to get it into a show . xxmaj martin is excited , until it turns out that xxmaj robert is speaking out of his xxunk , it is all a xxunk dance . xxmaj robert tries to kiss xxmaj martin , on the lips , and xxmaj martin pulls back , saying that he is not gay . xxmaj robert xxunk that he 's not gay either , xxmaj martin xxunk . xxmaj both admit that the xxunk are bad . xxmaj robert is about to leave , when xxmaj martin allows xxmaj robert to kiss him . xxmaj they make out , and xxmaj robert goes down on xxmaj martin . \\n\\n xxmaj next we follow xxmaj martin , as he xxunk for an art show at a xxmaj manhattan gallery . xxmaj he is xxunk by the xxunk , xxmaj anna , played by xxmaj rosario xxmaj dawson . ( i had to cut some of this review to keep it under 1 xxrep 3 0 words ) … and they make love to each other . \\n\\n xxmaj we next follow xxmaj anna , who is sitting at a lunch stand . xxmaj her boyfriend , xxmaj nick ( adrian xxmaj xxunk ) , enters , bearing xxunk . xxmaj she is cold toward him ; he tries to figure out why . xxmaj he coaxes out of her the information that she has had sex with someone while he was in xxmaj san xxmaj francisco . xxmaj she coaxes out of him the fact that he has stayed with his ex - xxunk while in xxmaj san xxmaj francisco , and had sex with her . xxmaj the latter revelation turns out to be a lie . xxmaj the two of them make out in the xxunk , but she decides that they must break up . xxmaj nick is xxunk . \\n\\n xxmaj and we follow xxmaj nick , who confesses his xxunk to an older woman who he meets on a park bench , xxmaj joey ( carol xxmaj kane ) . xxmaj joey is sort of weird and child - like , but is a good audience for xxmaj nick , who needs a sympathetic ear . xxmaj the two of them go to xxmaj xxunk xxmaj island at night , and look at the stars . xxmaj nick falls under xxmaj joey 's spell , despite the age difference between them . xxmaj they go back to xxmaj joey 's apartment , and xxmaj nick gradually realizes that he is about to have sex with a crazy old woman . xxmaj she is on top of him , does n't want to let him go . xxmaj but he manages to escape . \\n\\n ( this is , by the way , the best xxmaj carol xxmaj kane role since she played xxmaj xxunk 's wife in xxmaj taxi . ) xxmaj joey 's phone rings , and it is a man calling the xxmaj psychic xxmaj friends xxmaj network , and xxmaj joey is one of the psychic friends . xxmaj although she is still hurting from xxmaj nick , she gradually gets into her psychic xxunk . xxmaj the man is at his office , late at night , and wants to have phone sex with her . xxmaj although that is not xxmaj joey 's business , xxmaj joey goes along , and coaxes the man to come . xxmaj she wants to keep talking , although the man want to get off the phone , and finds out that he has xxunk a lot of money from his company , and will be found out xxunk . xxmaj his life is ruined . xxmaj joey realizes that the man is going to commit suicide , and she tries to make him believe that she is his friend , that she cares about him . xxmaj and she does care about him . \\n\\n xxmaj but the man packs a gun into his xxunk , and goes off to seek a prostitute on the xxmaj brooklyn xxunk , and we come back to the beginning , to the same prostitute who started out xxmaj la xxmaj xxunk . xxmaj she wants to give him $ xxunk , xxrep 3 0 in cash if she will kill him . xxmaj he tried to kill himself , but could n't do it . xxmaj the prostitute does not want to do it , but he insists , holding her hand , holding the gun inside his mouth , telling her where to aim . xxmaj eventually , the gun goes off , and we see the prostitute walking down the street , and xxunk at the corner where she normally does business . xxmaj the contractor who did n't pay her earlier in the movie drives up , xxunk down the window . xxmaj they look at each other . xxup the xxup end .</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbunch.show_batch(max_n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we once again have a convenience function to create a classifier from this `DataBunch` with the `AWD_LSTM` architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(dbunch, AWD_LSTM, vocab, metrics=[accuracy], path=path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our pretrained encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = learn.load_encoder('enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can train with gradual unfreezing and differential learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.849841</td>\n",
       "      <td>0.601815</td>\n",
       "      <td>0.675000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.753493</td>\n",
       "      <td>0.568468</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.753582</td>\n",
       "      <td>0.548259</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.740121</td>\n",
       "      <td>0.533096</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>00:06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(4, moms=(0.8,0.7,0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.710222</td>\n",
       "      <td>0.536359</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.715015</td>\n",
       "      <td>0.531864</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.737472</td>\n",
       "      <td>0.531057</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>00:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.682374</td>\n",
       "      <td>0.526255</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.707951</td>\n",
       "      <td>0.531820</td>\n",
       "      <td>0.735000</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.723326</td>\n",
       "      <td>0.524388</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.685744</td>\n",
       "      <td>0.527304</td>\n",
       "      <td>0.745000</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.732956</td>\n",
       "      <td>0.520143</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>00:10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(8, np.array([1e-5,3e-4,1e-4,3e-3,1e-3]), moms=(0.8,0.7,0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
