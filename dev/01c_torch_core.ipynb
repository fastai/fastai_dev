{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp torch_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from local.test import *\n",
    "from local.basics import *\n",
    "from local.torch_imports import *\n",
    "from fastprogress import progress_bar,master_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.notebook.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_all_ = ['progress_bar','master_bar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "if torch.cuda.is_available(): torch.cuda.set_device(int(os.environ.get('DEFAULT_GPU') or 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch Core\n",
    "\n",
    "> Basic pytorch functions used in the fastai library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def __array_eq__(self:Tensor,b):\n",
    "    return torch.equal(self,b) if self.dim() else self==b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tensor(x, *rest, **kwargs):\n",
    "    \"Like `torch.as_tensor`, but handle lists too, and can pass multiple vector elements directly.\"\n",
    "    if len(rest): x = (x,)+rest\n",
    "    # Pytorch bug in dataloader using num_workers>0\n",
    "    if isinstance(x, (tuple,list)) and len(x)==0: return tensor(0)\n",
    "    res = (torch.tensor(x, **kwargs) if isinstance(x, (tuple,list))\n",
    "           else as_tensor(x.values, **kwargs) if isinstance(x, (pd.Series, pd.DataFrame))\n",
    "           else as_tensor(x, **kwargs) if hasattr(x, '__array__') or is_iter(x)\n",
    "           else None)\n",
    "    if res is None:\n",
    "        res = as_tensor(array(x), **kwargs)\n",
    "        if res.dtype is torch.float64: return res.float()\n",
    "    if res.dtype is torch.int32:\n",
    "        warn('Tensor is int32: upgrading to int64; for better performance use int64 input')\n",
    "        return res.long()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(tensor(array([1,2,3])), torch.tensor([1,2,3]))\n",
    "test_eq(tensor(1,2,3), torch.tensor([1,2,3]))\n",
    "test_eq_type(tensor(1.0), torch.tensor(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def set_seed(s):\n",
    "    \"Set random seed for `random`, `torch`, and `numpy` (where available)\"\n",
    "    try: torch.manual_seed(s)\n",
    "    except NameError: pass\n",
    "    try: np.random.seed(s%(2**32-1))\n",
    "    except NameError: pass\n",
    "    random.seed(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(2*33)\n",
    "a1 = np.random.random()\n",
    "a2 = torch.rand(())\n",
    "a3 = random.random()\n",
    "set_seed(2*33)\n",
    "b1 = np.random.random()\n",
    "b2 = torch.rand(())\n",
    "b3 = random.random()\n",
    "test_eq(a1,b1)\n",
    "test_eq(a2,b2)\n",
    "test_eq(a3,b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _fa_rebuild_tensor (cls, *args, **kwargs): return cls(torch._utils._rebuild_tensor_v2(*args, **kwargs))\n",
    "def _fa_rebuild_qtensor(cls, *args, **kwargs): return cls(torch._utils._rebuild_qtensor  (*args, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TensorBase(Tensor, metaclass=BypassNewMeta):\n",
    "    def _new_meta(self, *args, **kwargs): return tensor(self)\n",
    "\n",
    "    def __reduce_ex__(self,proto):\n",
    "        torch.utils.hooks.warn_if_has_hooks(self)\n",
    "        args = (type(self), self.storage(), self.storage_offset(), tuple(self.size()), self.stride())\n",
    "        if self.is_quantized: args = args + (self.q_scale(), self.q_zero_point())\n",
    "        f = _fa_rebuild_qtensor if self.is_quantized else  _fa_rebuild_tensor\n",
    "        return (f, args + (self.requires_grad, OrderedDict()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _patch_tb():\n",
    "    def get_f(fn):\n",
    "        def _f(self, *args, **kwargs):\n",
    "            cls = self.__class__\n",
    "            res = getattr(super(TensorBase, self), fn)(*args, **kwargs)\n",
    "            return cls(res) if isinstance(res,Tensor) else res\n",
    "        return _f\n",
    "\n",
    "    t = tensor([1])\n",
    "    skips = '__class__ __deepcopy__ __delattr__ __dir__ __doc__ __getattribute__ __hash__ __init__ \\\n",
    "        __init_subclass__ __new__ __reduce__ __reduce_ex__ __module__ __setstate__'.split()\n",
    "\n",
    "    for fn in dir(t):\n",
    "        if fn in skips: continue\n",
    "        f = getattr(t, fn)\n",
    "        if isinstance(f, (MethodWrapperType, BuiltinFunctionType, BuiltinMethodType, MethodType, FunctionType)):\n",
    "            setattr(TensorBase, fn, get_f(fn))\n",
    "\n",
    "_patch_tb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class TensorCategory(TensorBase): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _T(TensorBase): pass\n",
    "\n",
    "t = _T(range(5))\n",
    "test_eq_type(t[0], _T(0))\n",
    "test_eq_type(t[:2], _T([0,1]))\n",
    "test_eq_type(t+1, _T(range(1,6)))\n",
    "\n",
    "test_eq(type(pickle.loads(pickle.dumps(t))), _T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def tensored(self:L):\n",
    "    \"`mapped(tensor)`\"\n",
    "    return self.map(tensor)\n",
    "@patch\n",
    "def stack(self:L, dim=0):\n",
    "    \"Same as `torch.stack`\"\n",
    "    return torch.stack(list(self.tensored()), dim=dim)\n",
    "@patch\n",
    "def cat  (self:L, dim=0):\n",
    "    \"Same as `torch.cat`\"\n",
    "    return torch.cat  (list(self.tensored()), dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"L.tensored\" class=\"doc_header\"><code>L.tensored</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L2\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>L.tensored</code>()\n",
       "\n",
       "`mapped(tensor)`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(L.tensored)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are shortcuts for `torch.stack` and `torch.cat` if your `L` contains tensors or something convertible. You can manually convert with `tensored`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = L(([1,2],[3,4]))\n",
    "test_eq(t.tensored(), [tensor(1,2),tensor(3,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"L.stack\" class=\"doc_header\"><code>L.stack</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L6\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>L.stack</code>(**`dim`**=*`0`*)\n",
       "\n",
       "Same as `torch.stack`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(L.stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(t.stack(), tensor([[1,2],[3,4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"L.cat\" class=\"doc_header\"><code>L.cat</code><a href=\"https://github.com/fastai/fastai_dev/tree/master/dev/__main__.py#L10\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>L.cat</code>(**`dim`**=*`0`*)\n",
       "\n",
       "Same as `torch.cat`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(L.cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(t.cat(), tensor([1,2,3,4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def concat(*ls):\n",
    "    \"Concatenate tensors, arrays, lists, or tuples\"\n",
    "    if not len(ls): return []\n",
    "    it = ls[0]\n",
    "    if isinstance(it,torch.Tensor): res = torch.cat(ls)\n",
    "    elif isinstance(it,ndarray): res = np.concatenate(ls)\n",
    "    else:\n",
    "        res = itertools.chain.from_iterable(map(L,ls))\n",
    "        if isinstance(it,(tuple,list)): res = type(it)(res)\n",
    "        else: res = L(res)\n",
    "    return retain_type(res, it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = [1],[1,2],[1,1,2]\n",
    "test_eq(concat(a,b), c)\n",
    "test_eq_type(concat(tuple (a),tuple (b)), tuple (c))\n",
    "test_eq_type(concat(array (a),array (b)), array (c))\n",
    "test_eq_type(concat(tensor(a),tensor(b)), tensor(c))\n",
    "test_eq_type(concat(TensorBase(a),TensorBase(b)), TensorBase(c))\n",
    "test_eq_type(concat([1,1],1), [1,1,1])\n",
    "test_eq_type(concat(1,1,1), L(1,1,1))\n",
    "test_eq_type(concat(L(1,2),1), L(1,2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Chunks:\n",
    "    \"Slice and int indexing into a list of lists\"\n",
    "    def __init__(self, chunks, lens=None):\n",
    "        self.chunks = chunks\n",
    "        self.lens = L(map(len,self.chunks) if lens is None else lens)\n",
    "        self.cumlens = np.cumsum(0+self.lens)\n",
    "        self.totlen = self.cumlens[-1]\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        if isinstance(i,slice): return self.getslice(i)\n",
    "        di,idx = self.doc_idx(i)\n",
    "        return self.chunks[di][idx]\n",
    "\n",
    "    def getslice(self, i):\n",
    "        st_d,st_i = self.doc_idx(ifnone(i.start,0))\n",
    "        en_d,en_i = self.doc_idx(ifnone(i.stop,self.totlen+1))\n",
    "        res = [self.chunks[st_d][st_i:(en_i if st_d==en_d else sys.maxsize)]]\n",
    "        for b in range(st_d+1,en_d): res.append(self.chunks[b])\n",
    "        if st_d!=en_d and en_d<len(self.chunks): res.append(self.chunks[en_d][:en_i])\n",
    "        return concat(*res)\n",
    "\n",
    "    def doc_idx(self, i):\n",
    "        if i<0: i=self.totlen+i # count from end\n",
    "        docidx = np.searchsorted(self.cumlens, i+1)-1\n",
    "        cl = self.cumlens[docidx]\n",
    "        return docidx,i-cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = L(list(string.ascii_lowercase[a:b]) for a,b in ((0,3),(3,7),(7,8),(8,16),(16,24),(24,26)))\n",
    "\n",
    "b = Chunks(docs)\n",
    "test_eq([b[ o] for o in range(0,5)], ['a','b','c','d','e'])\n",
    "test_eq([b[-o] for o in range(1,6)], ['z','y','x','w','v'])\n",
    "test_eq(b[6:13], 'g,h,i,j,k,l,m'.split(','))\n",
    "test_eq(b[20:77], 'u,v,w,x,y,z'.split(','))\n",
    "test_eq(b[:5], 'a,b,c,d,e'.split(','))\n",
    "test_eq(b[:2], 'a,b'.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(26)\n",
    "docs = L(t[a:b] for a,b in ((0,3),(3,7),(7,8),(8,16),(16,24),(24,26)))\n",
    "b = Chunks(docs)\n",
    "test_eq([b[ o] for o in range(0,5)], range(0,5))\n",
    "test_eq([b[-o] for o in range(1,6)], [25,24,23,22,21])\n",
    "test_eq(b[6:13], torch.arange(6,13))\n",
    "test_eq(b[20:77], torch.arange(20,26))\n",
    "test_eq(b[:5], torch.arange(5))\n",
    "test_eq(b[:2], torch.arange(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = L(TensorBase(t[a:b]) for a,b in ((0,3),(3,7),(7,8),(8,16),(16,24),(24,26)))\n",
    "b = Chunks(docs)\n",
    "test_eq_type(b[:2], TensorBase(range(2)))\n",
    "test_eq_type(b[:5], TensorBase(range(5)))\n",
    "test_eq_type(b[9:13], TensorBase(range(9,13)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def one_param(m):\n",
    "    \"First parameter in `m`\"\n",
    "    return next(iter(m.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def apply(func, x, *args, **kwargs):\n",
    "    \"Apply `func` recursively to `x`, passing on args\"\n",
    "    if is_listy(x): return type(x)([apply(func, o, *args, **kwargs) for o in x])\n",
    "    if isinstance(x,dict):  return {k: apply(func, v, *args, **kwargs) for k,v in x.items()}\n",
    "    res = func(x, *args, **kwargs)\n",
    "    return res if x is None else retain_type(res, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_detach(b, cpu=True):\n",
    "    \"Recursively detach lists of tensors in `b `; put them on the CPU if `cpu=True`.\"\n",
    "    def _inner(x, cpu=True):\n",
    "        if not isinstance(x,Tensor): return x\n",
    "        x = x.detach()\n",
    "        return x.cpu() if cpu else x\n",
    "    return apply(_inner, b, cpu=cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_half(b):\n",
    "    \"Recursively map lists of tensors in `b ` to FP16.\"\n",
    "    return apply(lambda x: x.half() if torch.is_floating_point(x) else x, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_float(b):\n",
    "    \"Recursively map lists of int tensors in `b ` to float.\"\n",
    "    return apply(lambda x: x.float() if torch.is_floating_point(x) else x, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# None: True if available; True: error if not availabe; False: use CPU\n",
    "defaults.use_cuda = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def default_device(use_cuda=-1):\n",
    "    \"Return or set default device; `use_cuda`: None - CUDA if available; True - error if not availabe; False - CPU\"\n",
    "    if use_cuda != -1: defaults.use_cuda=use_cuda\n",
    "    use = defaults.use_cuda or (torch.cuda.is_available() and defaults.use_cuda is None)\n",
    "    assert torch.cuda.is_available() or not use\n",
    "    return torch.device(torch.cuda.current_device()) if use else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cuda\n",
    "_td = torch.device(torch.cuda.current_device())\n",
    "test_eq(default_device(None), _td)\n",
    "test_eq(default_device(True), _td)\n",
    "test_eq(default_device(False), torch.device('cpu'))\n",
    "default_device(None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_device(b, device=None):\n",
    "    \"Recursively put `b` on `device`.\"\n",
    "    if device is None: device=default_device()\n",
    "    def _inner(o): return o.to(device, non_blocking=True) if isinstance(o,Tensor) else o\n",
    "    return apply(_inner, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = to_device((3,(tensor(3),tensor(2))))\n",
    "t1,(t2,t3) = t\n",
    "test_eq_type(t,(3,(tensor(3).cuda(),tensor(2).cuda())))\n",
    "test_eq(t2.type(), \"torch.cuda.LongTensor\")\n",
    "test_eq(t3.type(), \"torch.cuda.LongTensor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_cpu(b):\n",
    "    \"Recursively map lists of tensors in `b ` to the cpu.\"\n",
    "    return to_device(b,'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = to_cpu(t3)\n",
    "test_eq(t3.type(), \"torch.LongTensor\")\n",
    "test_eq(t3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# def to_np(x):\n",
    "#     \"Convert a tensor to a numpy array.\"\n",
    "#     return apply(Self.detach().cpu().numpy(), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_np(x):\n",
    "    \"Convert a tensor to a numpy array.\"\n",
    "    return apply(lambda o: o.data.cpu().numpy(), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = to_np(t3)\n",
    "test_eq(type(t3), np.ndarray)\n",
    "test_eq(t3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def item_find(x, idx=0):\n",
    "    \"Recursively takes the `idx`-th element of `x`\"\n",
    "    if is_listy(x): return item_find(x[idx])\n",
    "    if isinstance(x,dict):\n",
    "        key = list(x.keys())[idx] if isinstance(idx, int) else idx\n",
    "        return item_find(x[key])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_device(b):\n",
    "    \"Recursively search the device of `b`.\"\n",
    "    return item_find(b).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = default_device()\n",
    "test_eq(find_device(t2), dev)\n",
    "test_eq(find_device([t2,t2]), dev)\n",
    "test_eq(find_device({'a':t2,'b':t2}), dev)\n",
    "test_eq(find_device({'a':[[t2],[t2]],'b':t2}), dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def find_bs(b):\n",
    "    \"Recursively search the batch size of `b`.\"\n",
    "    return item_find(b).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4,5)\n",
    "test_eq(find_bs(x), 4)\n",
    "test_eq(find_bs([x, x]), 4)\n",
    "test_eq(find_bs({'a':x,'b':x}), 4)\n",
    "test_eq(find_bs({'a':[[x],[x]],'b':x}), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def np_func(f):\n",
    "    \"Convert a function taking and returning numpy arrays to one taking and returning tensors\"\n",
    "    def _inner(*args, **kwargs):\n",
    "        nargs = [to_np(arg) if isinstance(arg,Tensor) else arg for arg in args]\n",
    "        return tensor(f(*nargs, **kwargs))\n",
    "    functools.update_wrapper(_inner, f)\n",
    "    return _inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This decorator is particularly useful for using numpy functions as fastai metrics, for instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "@np_func\n",
    "def f1(inp,targ): return f1_score(targ, inp)\n",
    "\n",
    "a1,a2 = array([0,1,1]),array([1,0,1])\n",
    "t = f1(tensor(a1),tensor(a2))\n",
    "test_eq(f1_score(a1,a2), t)\n",
    "assert isinstance(t,Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Module(nn.Module, metaclass=PrePostInitMeta):\n",
    "    \"Same as `nn.Module`, but no need for subclasses to call `super().__init__`\"\n",
    "    def __pre_init__(self, *args, **kwargs): super().__init__()\n",
    "    def __init__(self): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h3 id=\"Module\" class=\"doc_header\"><code>class</code> <code>Module</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h3>\n",
       "\n",
       "> <code>Module</code>() :: [`Module`](/torch.core.html#Module)\n",
       "\n",
       "Same as `nn.Module`, but no need for subclasses to call `super().__init__`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_doc(Module, title_level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0893], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class _T(Module):\n",
    "    def __init__(self): self.f = nn.Linear(1,1)\n",
    "    def forward(self,x): return self.f(x)\n",
    "\n",
    "t = _T()\n",
    "t(tensor([1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "\n",
    "def get_model(model):\n",
    "    \"Return the model maybe wrapped inside `model`.\"\n",
    "    return model.module if isinstance(model, (DistributedDataParallel, nn.DataParallel)) else model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def one_hot(x, c):\n",
    "    \"One-hot encode `x` with `c` classes.\"\n",
    "    res = torch.zeros(c, dtype=torch.uint8)\n",
    "    res[L(x)] = 1.\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(one_hot([1,4], 5), tensor(0,1,0,0,1).byte())\n",
    "test_eq(one_hot([], 5), tensor(0,0,0,0,0).byte())\n",
    "test_eq(one_hot(2, 5), tensor(0,0,1,0,0).byte())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def one_hot_decode(x, vocab=None):\n",
    "    return L(vocab[i] if vocab else i for i,x_ in enumerate(x) if x_==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(one_hot_decode(tensor(0,1,0,0,1)), [1,4])\n",
    "test_eq(one_hot_decode(tensor(0,0,0,0,0)), [   ])\n",
    "test_eq(one_hot_decode(tensor(0,0,1,0,0)), [2  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def params(m):\n",
    "    \"Return all parameters of `m`\"\n",
    "    return [p for p in m.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def trainable_params(m):\n",
    "    \"Return all trainable parameters of `m`\"\n",
    "    return [p for p in m.parameters() if p.requires_grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Linear(4,5)\n",
    "test_eq(trainable_params(m), [m.weight, m.bias])\n",
    "m.weight.requires_grad_(False)\n",
    "test_eq(trainable_params(m), [m.bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "bn_types = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def bn_bias_params(m, with_bias=True):\n",
    "    \"Return all bias and BatchNorm parameters\"\n",
    "    if isinstance(m, bn_types): return L(m.parameters())\n",
    "    res = L(m.children()).map(bn_bias_params, with_bias=with_bias).concat()\n",
    "    if with_bias and hasattr(m, 'bias'): res.append(m.bias)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(10,20), nn.BatchNorm1d(20), nn.Conv1d(3,4, 3))\n",
    "test_eq(bn_bias_params(model), [model[0].bias, model[1].weight, model[1].bias, model[2].bias])\n",
    "model = nn.ModuleList([nn.Linear(10,20), nn.Sequential(nn.BatchNorm1d(20), nn.Conv1d(3,4, 3))])\n",
    "test_eq(bn_bias_params(model), [model[0].bias, model[1][0].weight, model[1][0].bias, model[1][1].bias])\n",
    "model = nn.ModuleList([nn.Linear(10,20), nn.Sequential(nn.BatchNorm1d(20), nn.Conv1d(3,4, 3))])\n",
    "test_eq(bn_bias_params(model, with_bias=False), [model[1][0].weight, model[1][0].bias])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def batch_to_samples(b, max_n=10):\n",
    "    \"'Transposes' a batch to (at most `max_n`) samples\"\n",
    "    if isinstance(b, Tensor): return list(b[:max_n])\n",
    "    else:\n",
    "        res = L(b).map(partial(batch_to_samples,max_n=max_n))\n",
    "        return retain_types(res.zip(), [b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tensor([1,2,3])\n",
    "test_eq(batch_to_samples([t,t+1], max_n=2), ([1,2],[2,3]))\n",
    "test_eq(batch_to_samples(tensor([1,2,3]), 10), [1, 2, 3])\n",
    "test_eq(batch_to_samples([tensor([1,2,3]), tensor([4,5,6])], 10), [(1, 4), (2, 5), (3, 6)])\n",
    "test_eq(batch_to_samples([tensor([1,2,3]), tensor([4,5,6])], 2), [(1, 4), (2, 5)])\n",
    "test_eq(batch_to_samples([tensor([1,2,3]), [tensor([4,5,6]),tensor([7,8,9])]], 10), \n",
    "        [(1, (4, 7)), (2, (5, 8)), (3, (6, 9))])\n",
    "test_eq(batch_to_samples([tensor([1,2,3]), [tensor([4,5,6]),tensor([7,8,9])]], 2), [(1, (4, 7)), (2, (5, 8))])\n",
    "\n",
    "t = Tuple(tensor([1,2,3]),TensorBase([2,3,4]))\n",
    "test_eq_type(batch_to_samples(t)[0][1], TensorBase(2))\n",
    "test_eq(batch_to_samples(t).map(type), [Tuple]*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def interp_1d(x:Tensor, xp, fp):\n",
    "    \"Same as `np.interp`\"\n",
    "    slopes = (fp[1:]-fp[:-1])/(xp[1:]-xp[:-1])\n",
    "    incx = fp[:-1] - (slopes*xp[:-1])\n",
    "    locs = (x[:,None]>=xp[None,:]).long().sum(1)-1\n",
    "    return slopes[locs]*x + incx[locs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "brks = tensor(0,1,2,4,8,64).float()\n",
    "ys = tensor(range_of(brks)).float()\n",
    "ys /= ys[-1]\n",
    "pts = tensor(0.2,0.5,0.8,3,5,63)\n",
    "\n",
    "preds = pts.interp_1d(brks, ys)\n",
    "test_close(preds.numpy(), np.interp(pts.numpy(), brks.numpy(), ys.numpy()))\n",
    "\n",
    "plt.scatter(brks,ys)\n",
    "plt.scatter(pts,preds)\n",
    "plt.legend(['breaks','preds']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def make_cross_image(bw=True):\n",
    "    \"Create a tensor containing a cross image, either `bw` (True) or color\"\n",
    "    if bw:\n",
    "        im = torch.zeros(5,5)\n",
    "        im[2,:] = 1.\n",
    "        im[:,2] = 1.\n",
    "    else:\n",
    "        im = torch.zeros(3,5,5)\n",
    "        im[0,2,:] = 1.\n",
    "        im[1,:,2] = 1.\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPUAAAD4CAYAAAA0L6C7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAI40lEQVR4nO3dT4ichR3G8efpJqJgwUPmELKh60GkQaiSIQj2FDzEKtqjgj0JuVSIUBDtzUOvxYuXYEVBUQQ9iFhEUGsLVp34r6ZRCJJiUMgEkeqloj49zBxiu7vzzuR959355fuBhZ3dycyD7nff2dnlHScRgDp+0vcAAO0iaqAYogaKIWqgGKIGitnVxY3u2bMnGxsbXdz0Je/EiRN9T5jLwYMH+55Q0pkzZ3T+/Hlv9rlOot7Y2NBoNOripi959qb/H3csvg66MRwOt/wcD7+BYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiGkVt+4jtT2yftv1A16MALG5m1LbXJD0i6RZJByTdZftA18MALKbJkfqQpNNJPk3yraRnJN3R7SwAi2oS9T5Jn11w+ez0Yz9i+6jtke3ReDxuax+AOTWJerPTV/7fq+olOZ5kmGQ4GAwufhmAhTSJ+qyk/RdcXpf0eTdzAFysJlG/I+ka21fbvkzSnZJe6HYWgEXNPJl/ku9s3yvpZUlrkh5LcrLzZQAW0ugVOpK8JOmljrcAaAF/UQYUQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDFEDRRD1EAxRA0UQ9RAMUQNFEPUQDEzo7b9mO1ztj9axiAAF6fJkfpxSUc63gGgJTOjTvKGpC+XsAVAC/iZGiimtahtH7U9sj0aj8dt3SyAObUWdZLjSYZJhoPBoK2bBTAnHn4DxTT5ldbTkt6UdK3ts7bv6X4WgEXtmnWFJHctYwiAdvDwGyiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYpyk/Ru1279RAD+SxJt9nCM1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxcyM2vZ+26/ZPmX7pO1jyxgGYDEzz1Fme6+kvUnetf1TSSck/TrJP7f5N5yjDOjYwucoS/JFknen738t6ZSkfe3OA9CWXfNc2faGpBskvbXJ545KOtrKKgALa3yKYNtXSvqLpD8keX7GdXn4DXTsok4RbHu3pOckPTUraAD9avJEmSU9IenLJPc1ulGO1EDntjpSN4n6l5L+Kukfkn6Yfvj3SV7a5t8QNdCxhaNeBFED3eNld4BLBFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8UQNVAMUQPFEDVQDFEDxRA1UAxRA8XMdTbRpg4ePKjRaNTFTV/yJmeXWh1dnIQD0nA43PJzHKmBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiiBoohqiBYogaKIaogWKIGiiGqIFiZkZt+3Lbb9v+wPZJ2w8tYxiAxTQ5ndF/JB1O8o3t3ZL+ZvvPSf7e8TYAC5gZdSYnmfpmenH39I0TTwE7VKOfqW2v2X5f0jlJryR5q9tZABbVKOok3ye5XtK6pEO2r/vf69g+antkezQej9veCaChuZ79TvKVpNclHdnkc8eTDJMMB4NBS/MAzKvJs98D21dN379C0s2SPu56GIDFNHn2e6+kJ2yvafJN4NkkL3Y7C8Cimjz7/aGkG5awBUAL+IsyoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgmMZR216z/Z7tF7scBODizHOkPibpVFdDALSjUdS21yXdKunRbucAuFhNj9QPS7pf0g9bXcH2Udsj26PxeNzKOADzmxm17dsknUtyYrvrJTmeZJhkOBgMWhsIYD5NjtQ3Sbrd9hlJz0g6bPvJTlcBWNjMqJM8mGQ9yYakOyW9muTuzpcBWAi/pwaK2TXPlZO8Lun1TpYAaAVHaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGiiFqoBiiBoohaqAYogaKIWqgGKIGinGS9m/UHkv6V8s3u0fS+ZZvs0urtHeVtkqrtberrT9LsukZPjuJugu2R0mGfe9oapX2rtJWabX29rGVh99AMUQNFLNKUR/ve8CcVmnvKm2VVmvv0reuzM/UAJpZpSM1gAaIGihmJaK2fcT2J7ZP236g7z3bsf2Y7XO2P+p7yyy299t+zfYp2ydtH+t701ZsX277bdsfTLc+1PemJmyv2X7P9ovLus8dH7XtNUmPSLpF0gFJd9k+0O+qbT0u6UjfIxr6TtLvkvxc0o2SfruD/9v+R9LhJL+QdL2kI7Zv7HlTE8cknVrmHe74qCUdknQ6yadJvtXklTfv6HnTlpK8IenLvnc0keSLJO9O3/9aky++ff2u2lwmvple3D1929HP8tpel3SrpEeXeb+rEPU+SZ9dcPmsdugX3iqzvSHpBklv9btka9OHsu9LOifplSQ7duvUw5Lul/TDMu90FaL2Jh/b0d+hV43tKyU9J+m+JP/ue89Wknyf5HpJ65IO2b6u701bsX2bpHNJTiz7vlch6rOS9l9weV3S5z1tKcf2bk2CfirJ833vaSLJV5q8+upOfu7iJkm32z6jyY+Mh20/uYw7XoWo35F0je2rbV+myQvfv9DzphJsW9KfJJ1K8se+92zH9sD2VdP3r5B0s6SP+121tSQPJllPsqHJ1+yrSe5exn3v+KiTfCfpXkkva/JEzrNJTva7amu2n5b0pqRrbZ+1fU/fm7Zxk6TfaHIUeX/69qu+R21hr6TXbH+oyTf6V5Is7ddEq4Q/EwWK2fFHagDzIWqgGKIGiiFqoBiiBoohaqAYogaK+S/20vv5In3GxwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(make_cross_image(), cmap=\"Greys\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(make_cross_image(False).permute(1,2,0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_image(im, ax=None, figsize=None, title=None, ctx=None, **kwargs):\n",
    "    \"Show a PIL or PyTorch image on `ax`.\"\n",
    "    ax = ifnone(ax,ctx)\n",
    "    if ax is None: _,ax = plt.subplots(figsize=figsize)\n",
    "    # Handle pytorch axis order\n",
    "    if isinstance(im,Tensor):\n",
    "        im = to_cpu(im)\n",
    "        if im.shape[0]<5: im=im.permute(1,2,0)\n",
    "    elif not isinstance(im,np.ndarray): im=array(im)\n",
    "    # Handle 1-channel images\n",
    "    if im.shape[-1]==1: im=im[...,0]\n",
    "    ax.imshow(im, **kwargs)\n",
    "    if title is not None: ax.set_title(title)\n",
    "    ax.axis('off')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`show_image` can show b&w images..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAABZUlEQVR4nO3dSQrDMBAAwSjk/19WXpDxJQtKV10NxqjRQWBGa+99o+H+6w/ge8QOETtE7BCxQ8QOeVw8P+pcttZ6+zsPPJq+XAQ7O0TsELFDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TsELFDxtmln5gFeprT1mCatWpnh4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CHjD4enXRTusvSZnR0idojYIWKHiB0idojYIWKHiB0idojYIWKHiB0idojYIWKHiB0idojYIWKHiB0idojYIWKHiB0idojYIWKHiB0idojYIWKHiB0idojYIWKHiB0idojYIWKHiB0idojYIeufZnMys7NDxA4RO0TsELFDxA55Ao0DE/UW4fj8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = make_cross_image()\n",
    "ax = show_image(im, cmap=\"Greys\", figsize=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and color images with standard `c*h*w` dim order..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAABYUlEQVR4nO3dQQrCMBBAUSPe/8rjCYwbUyn/vW2hDHxmUSjJmpkHDc9/D8B1xA4RO0TsELFDxA557R6ute71XXZi2nXgnQfNzMeJbXaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih2zPLr3XwaV8Y7NDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TskP1l6VdNwc/sfhK12SFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdsmZciV5hs0PEDhE7ROwQsUPEDnkD1tcN9ECq4WMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "im2 = make_cross_image(False)\n",
    "ax = show_image(im2, figsize=(2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and color images with `h*w*c` dim order..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAAB7CAYAAABUx/9/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAABYUlEQVR4nO3dQQrCMBBAUSPe/8rjCYwbUyn/vW2hDHxmUSjJmpkHDc9/D8B1xA4RO0TsELFDxA557R6ute71XXZi2nXgnQfNzMeJbXaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih2zPLr3XwaV8Y7NDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TsELFDxA4RO0TskP1l6VdNwc/sfhK12SFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdInaI2CFih4gdsmZciV5hs0PEDhE7ROwQsUPEDnkD1tcN9ECq4WMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "im3 = im2.permute(1,2,0)\n",
    "ax = show_image(im3, figsize=(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHsAAACLCAYAAABBVeZmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAADqUlEQVR4nO3dO2sUYRiG4fuNiuksVMQIRmwEC7ERVNKIlkps0hi0srTwUCgYg1oIYhMC+gtELQQVFCFtkIDmByioKGoUT6iJB0jiazFThBBXlCSbz+e5YIqdmV129p7DtyTMRmZiGlqa/QZs/ji2EMcW4thCHFuIYwtxbCFFx46IfRExHBFjEfE6Iu5GREez39dCVWzsiDgK9AHngFXAWuAS0DnDuovn990tUJlZ3AQsA8aArt8sPw1cBy4DX4CDwFKqnWOknvqApfX6K4DbwCfgIzAItNTLjgOvgFHgEbCz2dv/r1Ope/w2oBW40WCdTqALOEAV+iSwFdgMJHAL6AFOAceAl8DK+rlbgYyIDcAhYEtmjkTEOmDRLG/LvCn1NL4ceJ+ZEw3WGcrMm5n5MzO/A93A2cx8m5nvgDPA/nrdcWA10J6Z45k5mNVhPUm1o2yMiCWZ+Swzn8zdZs2tUmN/AFb84Vr8YtrjNuD5lMfP63kAF4DHwEBEPI2IEwCZ+Rg4THVZeBsR1yKijUKVGnsI+AHsbbDO9D/njQDtUx6vreeRmaOZeSwz1wN7gKMRsbNediUzO+rnJnB+djZh/hV5zc7MzxHRC1yMiAlggOpUvAvYAXyb4WlXgZ6IeEAVrZdqAEdE7AYeAk+oBnSTwGR9zV4D3KPaub5T7gFS5mh8yqi7GxgGvgJvgDvAdqrT7uVp67YC/cDreuoHWutlR4Bn9eu8BE7V8zcB96lG4h+pRuxtzd7uf52i3igTUO4pyf6aYwtxbCGOLcSxhfzpe3ZRQ/WImPXXLPDbym8/BB/ZQhxbiGMLcWwhji3EsYU4thDHFuLYQhxbiGMLcWwhji3EsYU4thDHFuLYQhxbiGMLcWwhji3EsYU4thDHFuLYQhxbiGMLcWwhji3EsYU4thDHFuLYQhxbiGMLcWwhji3EsYU4thDHFuLYQhxbiGMLaXjv0rm4F2hpSvsMGt1r1Ue2EMcW4thCHFuIYwtxbCGOLcSxhTi2EMcW4thCHFuIYwtxbCGOLcSxhTi2EMcW4thCHFtIw384LO2Hwv1j6Y35yBbi2EIcW4hjC3FsIY4txLGFOLYQxxbi2EIcW4hjC3FsIY4txLGFOLYQxxbi2EIcW4hjC3FsIY4txLGFOLYQxxbi2EIcW4hjC3FsIY4txLGFOLYQxxbi2EIcW4hjC3FsIY4txLGFOLYQxxbi2EIcW0j8T/fmtMZ8ZAtxbCGOLcSxhTi2EMcW8gsg1f8mDj+bqgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = show_image(im, cmap=\"Greys\", figsize=(2,2))\n",
    "show_title(\"Cross\", ax);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_titled_image(o, **kwargs):\n",
    "    \"Call `show_image` destructuring `o` to `(img,title)`\"\n",
    "    show_image(o[0], title=str(o[1]), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def show_image_batch(b, show=show_titled_image, items=9, cols=3, figsize=None, **kwargs):\n",
    "    \"Display batch `b` in a grid of size `items` with `cols` width\"\n",
    "    rows = (items+cols-1) // cols\n",
    "    if figsize is None: figsize = (cols*3, rows*3)\n",
    "    fig,axs = plt.subplots(rows, cols, figsize=figsize)\n",
    "    for *o,ax in zip(*to_cpu(b), axs.flatten()): show(o, ax=ax, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image_batch(([im,im2,im3],['bw','chw','hwc']), items=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def requires_grad(m):\n",
    "    \"Check if the first parameter of `m` requires grad or not\"\n",
    "    ps = list(m.parameters())\n",
    "    return ps[0].requires_grad if len(ps)>0 else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = nn.Linear(4,5)\n",
    "assert requires_grad(tst)\n",
    "for p in tst.parameters(): p.requires_grad_(False)\n",
    "assert not requires_grad(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def init_default(m, func=nn.init.kaiming_normal_):\n",
    "    \"Initialize `m` weights with `func` and set `bias` to 0.\"\n",
    "    if func:\n",
    "        if hasattr(m, 'weight'): func(m.weight)\n",
    "        if hasattr(m, 'bias') and hasattr(m.bias, 'data'): m.bias.data.fill_(0.)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = nn.Linear(4,5)\n",
    "tst.weight.data.uniform_(-1,1)\n",
    "tst.bias.data.uniform_(-1,1)\n",
    "tst = init_default(tst, func = lambda x: x.data.fill_(1.))\n",
    "test_eq(tst.weight, torch.ones(5,4))\n",
    "test_eq(tst.bias, torch.zeros(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cond_init(m, func):\n",
    "    \"Apply `init_default` to `m` unless it's a batchnorm module\"\n",
    "    if (not isinstance(m, bn_types)) and requires_grad(m): init_default(m, func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = nn.Linear(4,5)\n",
    "tst.weight.data.uniform_(-1,1)\n",
    "tst.bias.data.uniform_(-1,1)\n",
    "cond_init(tst, func = lambda x: x.data.fill_(1.))\n",
    "test_eq(tst.weight, torch.ones(5,4))\n",
    "test_eq(tst.bias, torch.zeros(5))\n",
    "\n",
    "tst = nn.BatchNorm2d(5)\n",
    "init = [tst.weight.clone(), tst.bias.clone()]\n",
    "cond_init(tst, func = lambda x: x.data.fill_(1.))\n",
    "test_eq(tst.weight, init[0])\n",
    "test_eq(tst.bias, init[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def apply_leaf(m, f):\n",
    "    \"Apply `f` to children of `m`.\"\n",
    "    c = m.children()\n",
    "    if isinstance(m, nn.Module): f(m)\n",
    "    for l in c: apply_leaf(l,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = nn.Sequential(nn.Linear(4,5), nn.Sequential(nn.Linear(4,5), nn.Linear(4,5)))\n",
    "apply_leaf(tst, partial(init_default, func=lambda x: x.data.fill_(1.)))\n",
    "for l in [tst[0], *tst[1]]: test_eq(l.weight, torch.ones(5,4))\n",
    "for l in [tst[0], *tst[1]]: test_eq(l.bias,   torch.zeros(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def apply_init(m, func=nn.init.kaiming_normal_):\n",
    "    \"Initialize all non-batchnorm layers of `m` with `func`.\"\n",
    "    apply_leaf(m, partial(cond_init, func=func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = nn.Sequential(nn.Linear(4,5), nn.Sequential(nn.Linear(4,5), nn.BatchNorm1d(5)))\n",
    "init = [tst[1][1].weight.clone(), tst[1][1].bias.clone()]\n",
    "apply_init(tst, func=lambda x: x.data.fill_(1.))\n",
    "for l in [tst[0], tst[1][0]]: test_eq(l.weight, torch.ones(5,4))\n",
    "for l in [tst[0], tst[1][0]]: test_eq(l.bias,   torch.zeros(5))\n",
    "test_eq(tst[1][1].weight, init[0])\n",
    "test_eq(tst[1][1].bias,   init[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from multiprocessing import Process, Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "@delegates(concurrent.futures.ProcessPoolExecutor)\n",
    "class ProcessPoolExecutor(concurrent.futures.ProcessPoolExecutor):\n",
    "    def __init__(self, max_workers=None, on_exc=print, **kwargs):\n",
    "        self.not_parallel = max_workers==0\n",
    "        self.on_exc = on_exc\n",
    "        if self.not_parallel: max_workers=1\n",
    "        super().__init__(max_workers, **kwargs)\n",
    "\n",
    "    def map(self, f, items, *args, **kwargs):\n",
    "        g = partial(f, *args, **kwargs)\n",
    "        if self.not_parallel: return L(items).map(g)\n",
    "        try: return super().map(g, items)\n",
    "        except Exception as e: self.on_exc(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def parallel(f, items, *args, n_workers=defaults.cpus, **kwargs):\n",
    "    \"Applies `func` in parallel to `items`, using `n_workers`\"\n",
    "    with ProcessPoolExecutor(n_workers) as ex:\n",
    "        return L(progress_bar(ex.map(f,items, *args, **kwargs), total=len(items), leave=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_one(x, a=1): \n",
    "    time.sleep(random.random()/100)\n",
    "    return x+a\n",
    "\n",
    "inp,exp = range(100),range(1,101)\n",
    "test_eq(parallel(add_one, inp), exp)\n",
    "test_eq(parallel(add_one, inp, n_workers=1), exp)\n",
    "test_eq(parallel(add_one, inp, n_workers=0), exp)\n",
    "test_eq(parallel(add_one, inp, n_workers=1, a=2), range(2,102))\n",
    "test_eq(parallel(add_one, inp, n_workers=0, a=2), range(2,102))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def run_procs(f, f_done, args):\n",
    "    \"Call `f` for each item in `args` in parallel, yielding `f_done`\"\n",
    "    processes = L(args).map(Process, args=arg0, target=f)\n",
    "    for o in processes: o.start()\n",
    "    try: yield from f_done()\n",
    "    except Exception as e: print(e)\n",
    "    finally: processes.map(Self.join())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def parallel_gen(cls, items, n_workers=defaults.cpus, as_gen=False, **kwargs):\n",
    "    \"Instantiate `cls` in `n_workers` procs & call each on a subset of `items` in parallel.\"\n",
    "    batches = np.array_split(items, n_workers)\n",
    "    idx = np.cumsum(0 + L(batches).map(len))\n",
    "    queue = Queue()\n",
    "    def f(batch, start_idx):\n",
    "        for i,b in enumerate(cls(**kwargs)(batch)): queue.put((start_idx+i,b))\n",
    "    def done(): return (queue.get() for _ in progress_bar(items, leave=False))\n",
    "    yield from run_procs(f, done, L(batches,idx).zip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old code from parallel_gen that Sylvain wants to keep (Oct-1)\n",
    "    #res = []\n",
    "    #for _ in progress_bar(items, leave=False):\n",
    "    #    res.append(queue.get())\n",
    "    #res = (queue.get() for _ in progress_bar(items, leave=False))\n",
    "    #try: return res #if as_gen else [o[1] for o in sorted(res)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cls` is any class with `__call__`. It will be passed `args` and `kwargs` when initialized. Note that `n_workers` instances of `cls` are created, one in each process. `items` are then split in `n_workers` batches and one is sent to each `cls`. The function then returns a list of all the results, matching the order of `items` (if not `as_gen`) or a generator of tuples of item indices and results (if `as_gen`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SleepyBatchFunc:\n",
    "    def __init__(self): self.a=1\n",
    "    def __call__(self, batch):\n",
    "        for k in batch:\n",
    "            time.sleep(random.random()/20)\n",
    "            yield k+self.a\n",
    "\n",
    "x = np.linspace(0,0.99,100)\n",
    "res = L(parallel_gen(SleepyBatchFunc, x, n_workers=2))\n",
    "test_eq(res.sorted().itemgot(1), x+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_test.ipynb.\n",
      "Converted 01_core.ipynb.\n",
      "Converted 01a_utils.ipynb.\n",
      "Converted 01b_dispatch.ipynb.\n",
      "Converted 01c_torch_core.ipynb.\n",
      "Converted 02_script.ipynb.\n",
      "Converted 03_dataloader.ipynb.\n",
      "Converted 04_transform.ipynb.\n",
      "Converted 05_data_core.ipynb.\n",
      "Converted 06_data_transforms.ipynb.\n",
      "Converted 07_vision_core.ipynb.\n",
      "Converted 08_pets_tutorial.ipynb.\n",
      "Converted 09_vision_augment.ipynb.\n",
      "Converted 10_data_block.ipynb.\n",
      "Converted 11_layers.ipynb.\n",
      "Converted 11a_vision_models_xresnet.ipynb.\n",
      "Converted 12_optimizer.ipynb.\n",
      "Converted 13_learner.ipynb.\n",
      "Converted 14_callback_schedule.ipynb.\n",
      "Converted 14a_callback_data.ipynb.\n",
      "Converted 15_callback_hook.ipynb.\n",
      "Converted 16_callback_progress.ipynb.\n",
      "Converted 17_callback_tracker.ipynb.\n",
      "Converted 18_callback_fp16.ipynb.\n",
      "Converted 19_callback_mixup.ipynb.\n",
      "Converted 20_metrics.ipynb.\n",
      "Converted 21_vision_learner.ipynb.\n",
      "Converted 22_tutorial_imagenette.ipynb.\n",
      "Converted 23_tutorial_transfer_learning.ipynb.\n",
      "Converted 30_text_core.ipynb.\n",
      "Converted 31_text_data.ipynb.\n",
      "Converted 32_text_models_awdlstm.ipynb.\n",
      "Converted 33_text_models_core.ipynb.\n",
      "Converted 34_callback_rnn.ipynb.\n",
      "Converted 35_tutorial_wikitext.ipynb.\n",
      "Converted 36_text_models_qrnn.ipynb.\n",
      "Converted 37_text_learner.ipynb.\n",
      "Converted 38_tutorial_ulmfit.ipynb.\n",
      "Converted 40_tabular_core.ipynb.\n",
      "Converted 41_tabular_model.ipynb.\n",
      "Converted 42_tabular_rapids.ipynb.\n",
      "Converted 50_data_block_examples.ipynb.\n",
      "Converted 60_medical_imaging.ipynb.\n",
      "Converted 90_notebook_core.ipynb.\n",
      "Converted 91_notebook_export.ipynb.\n",
      "Converted 92_notebook_showdoc.ipynb.\n",
      "Converted 93_notebook_export2html.ipynb.\n",
      "Converted 94_notebook_test.ipynb.\n",
      "Converted 95_index.ipynb.\n",
      "Converted 96_data_external.ipynb.\n",
      "Converted 97_utils_test.ipynb.\n",
      "Converted notebook2jekyll.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from local.notebook.export import notebook2script\n",
    "notebook2script(all_fs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
