{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local.imports import *\n",
    "from local.test import *\n",
    "from local.core import *\n",
    "from local.layers import *\n",
    "from local.data.all import *\n",
    "from local.notebook.showdoc import show_doc\n",
    "from local.optimizer import *\n",
    "from local.learner import *\n",
    "from local.text.data import *\n",
    "from local.text.models.core import *\n",
    "from local.text.models.awdlstm import *\n",
    "from local.callback.rnn import *\n",
    "from local.callback.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration test on Wikitext-2\n",
    "\n",
    "> Training a Language Model on WT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.WIKITEXT_TINY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comes with all the wrticles concatenated. We split them to be able to shuffle at the beginning of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def istitle(line):\n",
    "    return len(re.findall(r'^ = [^=]* = $', line)) != 0\n",
    "\n",
    "def read_file(filename):\n",
    "    articles = L()\n",
    "    with open(filename, encoding='utf8') as f:\n",
    "        lines = f.readlines()\n",
    "    current_article = ''\n",
    "    for i,line in enumerate(lines):\n",
    "        current_article += line\n",
    "        if i < len(lines)-2 and lines[i+1] == ' \\n' and istitle(lines[i+2]):\n",
    "            articles.append(current_article.split(' '))\n",
    "            current_article = ''\n",
    "    articles.append(current_article.split(' '))\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we put our list of tokenized texts together in an `LM_Dataset`. It will return tuples of sequences of `seq_len`, with the second sequence between the first one shifted by one on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,sl = 104,72\n",
    "train = LM_Dataset(read_file(path/'train.txt'), bs=bs, seq_len=sl, shuffle=True)\n",
    "valid = LM_Dataset(read_file(path/'valid.txt'), bs=bs, seq_len=sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['', '\\n', '=', 'Geopyxis', 'carbonaria', '=', '\\n', '\\n', 'Geopyxis', 'carbonaria', 'is', 'a', 'species', 'of', 'fungus', 'in', 'the', 'genus', 'Geopyxis', ',', 'family', '<unk>', '.', 'First', 'described', 'to', 'science', 'in', '1805', ',', 'and', 'given', 'its', 'current', 'name', 'in', '1889', ',', 'the', 'species', 'is', 'commonly', 'known', 'as', 'the', 'charcoal', 'loving', 'elf', '@-@', 'cup', ',', 'dwarf', '<unk>', 'cup', ',', '<unk>', '<unk>', 'cup', ',', 'or', 'pixie', 'cup', '.', 'The', 'small', ',', '<unk>', '@-@', 'shaped', 'fruitbodies', 'of', 'the'], ['\\n', '=', 'Geopyxis', 'carbonaria', '=', '\\n', '\\n', 'Geopyxis', 'carbonaria', 'is', 'a', 'species', 'of', 'fungus', 'in', 'the', 'genus', 'Geopyxis', ',', 'family', '<unk>', '.', 'First', 'described', 'to', 'science', 'in', '1805', ',', 'and', 'given', 'its', 'current', 'name', 'in', '1889', ',', 'the', 'species', 'is', 'commonly', 'known', 'as', 'the', 'charcoal', 'loving', 'elf', '@-@', 'cup', ',', 'dwarf', '<unk>', 'cup', ',', '<unk>', '<unk>', 'cup', ',', 'or', 'pixie', 'cup', '.', 'The', 'small', ',', '<unk>', '@-@', 'shaped', 'fruitbodies', 'of', 'the', 'fungus'])\n"
     ]
    }
   ],
   "source": [
    "print(train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then wrap our `LM_Dataset`s in a `TfmdList` to apply the `Numericalize` transform. We can't use a `TfmdDS` because our elements are already tuples and `TfmdDS` is there to create such tuples from individual items. Since we already have tuples, we specify `as_item=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter([p for t in train.ds for p in t])\n",
    "vocab = make_vocab(count)\n",
    "train_ds = TfmdList(train, tfms=Numericalize(vocab), as_item=False, wrap_l=False)\n",
    "valid_ds = TfmdList(valid, tfms=Numericalize(vocab), as_item=False, wrap_l=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, we need to use a special sampler that will make sure we ask for the correct sequences to form a batch: in the first batch we don't want the sequences 0,1,2,3... (they are contiguous in the source obtained by concatenating all texts) but the sequences 0,`num_batches`,`2*num_batches`,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = TfmdDL(train_ds, bs=bs, sampler=LM_Sampler(train), tfms=Cuda(), num_workers=0)\n",
    "valid_dl = TfmdDL(valid_ds, bs=bs, sampler=LM_Sampler(valid), tfms=Cuda(), num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>\\n = Geopyxis carbonaria = \\n \\n Geopyxis carbonaria is a species of fungus in the genus Geopyxis , family &lt;unk&gt; . First described to science in 1805 , and given its current name in 1889 , the species is commonly known as the charcoal loving elf @-@ cup , dwarf &lt;unk&gt; cup , &lt;unk&gt; &lt;unk&gt; cup , or pixie cup . The small , &lt;unk&gt; @-@ shaped fruitbodies of the</td>\n",
       "      <td>\\n = Geopyxis carbonaria = \\n \\n Geopyxis carbonaria is a species of fungus in the genus Geopyxis , family &lt;unk&gt; . First described to science in 1805 , and given its current name in 1889 , the species is commonly known as the charcoal loving elf @-@ cup , dwarf &lt;unk&gt; cup , &lt;unk&gt; &lt;unk&gt; cup , or pixie cup . The small , &lt;unk&gt; @-@ shaped fruitbodies of the fungus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Creek is in a sandstone and shale mountain region , it has a relatively low capacity to neutralize added acid . This makes it especially vulnerable to increased &lt;unk&gt; from acid rain , which poses a threat to the long term health of the plants and animals in the creek . The total &lt;unk&gt; ( TA ) is a measure of the capacity of water to neutralize acid , with a larger</td>\n",
       "      <td>is in a sandstone and shale mountain region , it has a relatively low capacity to neutralize added acid . This makes it especially vulnerable to increased &lt;unk&gt; from acid rain , which poses a threat to the long term health of the plants and animals in the creek . The total &lt;unk&gt; ( TA ) is a measure of the capacity of water to neutralize acid , with a larger TA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>the homes of wealthy Jews . \\n Finally her soliciting paid off and in 1881 , Rosebery was offered a government position acceptable to him , that of Under Secretary at the Home Office with special responsibility for Scotland . He had sought the position feeling that Scotland was neglected by the Liberal Government who were more interested in Ireland . However , immediately upon assuming the job he began to demand</td>\n",
       "      <td>homes of wealthy Jews . \\n Finally her soliciting paid off and in 1881 , Rosebery was offered a government position acceptable to him , that of Under Secretary at the Home Office with special responsibility for Scotland . He had sought the position feeling that Scotland was neglected by the Liberal Government who were more interested in Ireland . However , immediately upon assuming the job he began to demand a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>and Sweden , and the Danish and Swedish governments threatened to terminate the SAS agreement . On 25 November 1966 , with 82 against 62 votes , the Norwegian Parliament voted to allow Braathens SAFE to fly to Bodø and Tromsø as an extension of the West Coast route from 1 April 1967 . \\n On 3 March 1967 , parliament decided to build four short take @-@ off and landing airports</td>\n",
       "      <td>Sweden , and the Danish and Swedish governments threatened to terminate the SAS agreement . On 25 November 1966 , with 82 against 62 votes , the Norwegian Parliament voted to allow Braathens SAFE to fly to Bodø and Tromsø as an extension of the West Coast route from 1 April 1967 . \\n On 3 March 1967 , parliament decided to build four short take @-@ off and landing airports along</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>the outfitting center for Colorado gold &lt;unk&gt; headed for &lt;unk&gt; Peak in 1859 , 268 steamboats arrived at Omaha between March and November . \\n With railroads becoming the dominant form of long @-@ range shipping and passenger travel in the early 1870s , &lt;unk&gt; like those in Omaha became obsolete . However , as late at 1949 the steamship Avalon was letting passengers in Omaha , before becoming one of the</td>\n",
       "      <td>outfitting center for Colorado gold &lt;unk&gt; headed for &lt;unk&gt; Peak in 1859 , 268 steamboats arrived at Omaha between March and November . \\n With railroads becoming the dominant form of long @-@ range shipping and passenger travel in the early 1870s , &lt;unk&gt; like those in Omaha became obsolete . However , as late at 1949 the steamship Avalon was letting passengers in Omaha , before becoming one of the famous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>of February 2016 , Vistara has a share of 2 % in the domestic carrier market . \\n \\n = = Corporate affairs = = \\n \\n In March 2015 , Vistara shifted to its new office at the One Horizon Center tower in Sector 43 , &lt;unk&gt; , a satellite city of Delhi . Vistara chose &lt;unk&gt; &lt;unk&gt; Yeoh as the chief executive officer ( CEO ) and &lt;unk&gt; Ming &lt;unk&gt;</td>\n",
       "      <td>February 2016 , Vistara has a share of 2 % in the domestic carrier market . \\n \\n = = Corporate affairs = = \\n \\n In March 2015 , Vistara shifted to its new office at the One Horizon Center tower in Sector 43 , &lt;unk&gt; , a satellite city of Delhi . Vistara chose &lt;unk&gt; &lt;unk&gt; Yeoh as the chief executive officer ( CEO ) and &lt;unk&gt; Ming &lt;unk&gt; as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>the &lt;unk&gt; of the Indian National Congress and is still a major newspaper of Hindi northern India . \\n \\n = = = Art = = = \\n \\n Varanasi is a major centre of arts and designs . It is a producer of &lt;unk&gt; and &lt;unk&gt; with gold and silver thread work , carpet weaving , wooden toys , &lt;unk&gt; made of glass , ivory work , perfumes , artistic brass</td>\n",
       "      <td>&lt;unk&gt; of the Indian National Congress and is still a major newspaper of Hindi northern India . \\n \\n = = = Art = = = \\n \\n Varanasi is a major centre of arts and designs . It is a producer of &lt;unk&gt; and &lt;unk&gt; with gold and silver thread work , carpet weaving , wooden toys , &lt;unk&gt; made of glass , ivory work , perfumes , artistic brass and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>of war under your charge to the State authorities , to be held subject to the action of the convention to be held on the 4th of March next . \\n Perhaps because Abraham Lincoln had not yet been inaugurated as President , Captain Totten received no instructions from his superiors and was forced to withdraw his troops . He agreed to surrender the arsenal as long as the governor agreed to</td>\n",
       "      <td>war under your charge to the State authorities , to be held subject to the action of the convention to be held on the 4th of March next . \\n Perhaps because Abraham Lincoln had not yet been inaugurated as President , Captain Totten received no instructions from his superiors and was forced to withdraw his troops . He agreed to surrender the arsenal as long as the governor agreed to three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>the home . The decision to use a stay @-@ at @-@ home dad arrangement is most commonly due to economic reasons . At the same time , women are progressing into higher @-@ paying jobs . There are now financial ramifications in deciding whether the mother or father should become the stay @-@ at @-@ home parent . In cases where the woman is the higher @-@ paid parent , it</td>\n",
       "      <td>home . The decision to use a stay @-@ at @-@ home dad arrangement is most commonly due to economic reasons . At the same time , women are progressing into higher @-@ paying jobs . There are now financial ramifications in deciding whether the mother or father should become the stay @-@ at @-@ home parent . In cases where the woman is the higher @-@ paid parent , it makes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Allosaurus itself or at least the species A. fragillis , is technically a nomen dubium ( \" dubious name \" , based on a specimen too incomplete to compare to other specimens or to classify ) . In an attempt to fix this situation , Gregory S. Paul and Kenneth Carpenter ( 2010 ) submitted a petition to the &lt;unk&gt; to have the name A. fragillis officially transferred to the more complete</td>\n",
       "      <td>itself or at least the species A. fragillis , is technically a nomen dubium ( \" dubious name \" , based on a specimen too incomplete to compare to other specimens or to classify ) . In an attempt to fix this situation , Gregory S. Paul and Kenneth Carpenter ( 2010 ) submitted a petition to the &lt;unk&gt; to have the name A. fragillis officially transferred to the more complete specimen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbch = DataBunch(train_dl, valid_dl)\n",
    "dbch.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = awd_lstm_lm_config.copy()\n",
    "config.update({'input_p': 0.6, 'output_p': 0.4, 'weight_p': 0.5, 'embed_p': 0.1, 'hidden_p': 0.2})\n",
    "model = get_language_model(AWD_LSTM, len(vocab), config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_func = partial(Adam, wd=0.1, eps=1e-7)\n",
    "cb_funcs = [partial(MixedPrecision, clip=0.1), partial(RNNTrainer, alpha=3, beta=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(model, dbch, loss_func=CrossEntropyLossFlat(), opt_func=opt_func, cb_funcs=cb_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>5.618763</td>\n",
       "      <td>5.228721</td>\n",
       "      <td>02:34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "%prun learn.fit_one_cycle(1, 5e-3, moms=(0.8,0.7,0.8), div=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
