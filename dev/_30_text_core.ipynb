{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from local.imports import *\n",
    "from local.test import *\n",
    "from local.core import *\n",
    "from local.data.core import *\n",
    "from local.data.external import *\n",
    "from local.notebook.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp text.core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text core\n",
    "\n",
    "> Basic function to preprocess text before assembling it in a `DataBunch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from multiprocessing import Process, Queue\n",
    "import spacy,html\n",
    "from spacy.symbols import ORTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are rules applied to texts before or after it's tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#special tokens\n",
    "UNK, PAD, BOS, EOS, FLD, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxfld xxrep xxwrep xxup xxmaj\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_spec = re.compile(r'([/#\\\\])')\n",
    "\n",
    "def spec_add_spaces(t):\n",
    "    \"Add spaces around / and #\"\n",
    "    return _re_spec.sub(r' \\1 ', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(spec_add_spaces('#fastai'), ' # fastai')\n",
    "test_eq(spec_add_spaces('/fastai'), ' / fastai')\n",
    "test_eq(spec_add_spaces('\\\\fastai'), ' \\\\ fastai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_space = re.compile(' {2,}')\n",
    "\n",
    "def rm_useless_spaces(t):\n",
    "    \"Remove multiple spaces\"\n",
    "    return _re_space.sub(' ', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(rm_useless_spaces('a  b   c'), 'a b c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_rep = re.compile(r'(\\S)(\\1{3,})')\n",
    "\n",
    "def replace_rep(t):\n",
    "    \"Replace repetitions at the character level: cccc -> TK_REP 4 c\"\n",
    "    def _replace_rep(m):\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_REP} {len(cc)+1} {c} '\n",
    "    return _re_rep.sub(_replace_rep, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It starts replacing at 4 repetitions of the same character or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(replace_rep('aaa'), 'aaa')\n",
    "test_eq(replace_rep('aaaa'), f' {TK_REP} 4 a ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_wrep = re.compile(r'(?:\\s|^)(\\w+)\\s+((?:\\1\\s+){2,})\\1(\\s|\\W|$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "\n",
    "Matches any word repeated at least four times with spaces between them\n",
    "```\n",
    "(?:\\s|^)          Non-catching group with either a whitespace character or the beginning of text\n",
    "(\\w+)             Catching group of any alphanumeric character\n",
    "\\s+               One or more whitespace\n",
    "((?:\\1\\s+){2,})   Catching group of a repetition of two or more times \\1 followed by one or more whitespace\n",
    "\\1                Occurence of \\1\n",
    "(\\s|\\W|$)         Catching group of last whitespace, non alphanumeric character or end of text\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_wrep(t):\n",
    "    \"Replace word repetitions: word word word word -> TK_WREP 4 word\"\n",
    "    def _replace_wrep(m):\n",
    "        c,cc,e = m.groups()\n",
    "        return f' {TK_WREP} {len(cc.split())+2} {c} {e}'\n",
    "    return _re_wrep.sub(_replace_wrep, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It starts replacing at 4 repetitions of the same word or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(replace_wrep('ah ah'), 'ah ah')\n",
    "test_eq(replace_wrep('ah ah ah ah'), f' {TK_WREP} 4 ah ')\n",
    "test_eq(replace_wrep('ah ah   ah  ah'), f' {TK_WREP} 4 ah ')\n",
    "test_eq(replace_wrep('ah ah ah ah '), f' {TK_WREP} 4 ah  ')\n",
    "test_eq(replace_wrep('ah ah ah ah.'), f' {TK_WREP} 4 ah .')\n",
    "test_eq(replace_wrep('ah ah ah ahi'), f'ah ah ah ahi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fix_html(x):\n",
    "    \"Various messy things we've seen in documents\"\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace('nbsp;', ' ').replace(\n",
    "        '#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace('<br />', \"\\n\").replace(\n",
    "        '\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(' @-@ ','-')\n",
    "    return html.unescape(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(fix_html('#39;bli#146;'), \"'bli'\")\n",
    "test_eq(fix_html('Sarah amp; Duck'), 'Sarah & Duck')\n",
    "test_eq(fix_html('a nbsp; #36;'), 'a   $')\n",
    "test_eq(fix_html('\\\\\" <unk>'), f'\" {UNK}')\n",
    "test_eq(fix_html('quot;  @.@  @-@ '), \"' .-\")\n",
    "test_eq(fix_html('<br />text\\\\n'), '\\ntext\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_all_caps = re.compile(r'(\\s|^)([A-Z]+[^a-z\\s]*)(?=(\\s|$))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "\n",
    "Catches any word in all caps, even with ' or - inside\n",
    "```\n",
    "(\\s|^)        Catching group with either a whitespace or the beginning of text\n",
    "([A-Z]+       Catching group with one capitalized letter or more...\n",
    "[^a-z\\s]*)    ...followed by anything that's non lowercase or whitespace\n",
    "(?=(\\s|$))    Look ahead for a space of end of text\n",
    "```\n",
    "The look ahead is there to not move the pointer ahead of the next space in case we have consecutive words in all caps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_all_caps(t):\n",
    "    \"Replace tokens in ALL CAPS by their lower version and add `TK_UP` before.\"\n",
    "    def _replace_all_caps(m):\n",
    "        tok = f'{TK_UP} ' if len(m.groups()[1]) > 1 else ''\n",
    "        return f\"{m.groups()[0]}{tok}{m.groups()[1].lower()}\"\n",
    "    return _re_all_caps.sub(_replace_all_caps, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(replace_all_caps(\"I'M SHOUTING\"), f\"{TK_UP} i'm {TK_UP} shouting\")\n",
    "test_eq(replace_all_caps(\"I'm speaking normally\"), \"I'm speaking normally\")\n",
    "test_eq(replace_all_caps(\"I am speaking normally\"), \"i am speaking normally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_maj = re.compile(r'(\\s|^)([A-Z][^A-Z\\s]*)(?=(\\s|$))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hide\n",
    "\n",
    "Catches any capitalized word\n",
    "```\n",
    "(\\s|^)       Catching group with either a whitespace or the beginning of text\n",
    "([A-Z]       Catching group with exactly one capitalized letter...\n",
    "[^A-Z\\s]*)   ...followed by anything that's not uppercase or whitespace\n",
    "(?=(\\s|$))   Look ahead for a space of end of text\n",
    "```\n",
    "The look ahead is there to not move the pointer ahead of the next space in case we have consecutive words in all caps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_maj(t):\n",
    "    \"Replace tokens in ALL CAPS by their lower version and add `TK_UP` before.\"\n",
    "    def _replace_maj(m):\n",
    "        tok = f'{TK_MAJ} ' if len(m.groups()[1]) > 1 else ''\n",
    "        return f\"{m.groups()[0]}{tok}{m.groups()[1].lower()}\"\n",
    "    return _re_maj.sub(_replace_maj, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(replace_maj(\"Jeremy Howard\"), f'{TK_MAJ} jeremy {TK_MAJ} howard')\n",
    "test_eq(replace_maj(\"I don't think there is any maj here\"), (\"i don't think there is any maj here\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lowercase(t):\n",
    "    \"Converts `t` to lowercase\"\n",
    "    return t.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaults.text_proc_rules = [fix_html, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces,\n",
    "                            replace_all_caps, replace_maj, lowercase]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenizer is a class that must implement a `pipe` method. This `pipe` method receives a generator of texts and must return a generator with their tokenized versions. Here is the most basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTokenizer():\n",
    "    \"Basic tokenizer that just splits on spaces\"\n",
    "    def __init__(self, **kwargs): pass\n",
    "    def pipe(self, items): \n",
    "        for t in items: yield t.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = BaseTokenizer()\n",
    "for t in tok.pipe([\"This is a text\"]): test_eq(t, [\"This\", \"is\", \"a\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rules(items, rules):\n",
    "    \"Returns a generator that apply `rules`  to `items`\"\n",
    "    for o in items: yield compose(*rules)(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in apply_rules([\"This is a text\"], [replace_maj]): test_eq(t, f\"{TK_MAJ} this is a text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize1(text, tok_func=SpacyTokenizer, rules=None, **tok_kwargs):\n",
    "    \"Tokenize one `text` with an instance of `tok_func` and some `rules`\"\n",
    "    rules = L(ifnone(rules, defaults.text_proc_rules))\n",
    "    tokenizer = tok_func(**tok_kwargs)\n",
    "    for tok in tokenizer.pipe(apply_rules([text], rules)): return tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(tokenize1(\"This is a text\"), [TK_MAJ, 'this', 'is', 'a', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_items(items, tok_func, rules, output_func, output_queue, data_queue=None, **tok_kwargs):\n",
    "    'Tokenize `items` with an instance of `tok_func` and some `rules`'\n",
    "    tokenizer = tok_func(**tok_kwargs)\n",
    "    if data_queue: counts = Counter()\n",
    "    for o,tok in zip(items, tokenizer.pipe(apply_rules(items, rules))):\n",
    "        output_queue.put(output_func(o, tok))\n",
    "        if data_queue: counts.update(Counter(tok))\n",
    "    if data_queue: data_queue.put(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function that will be called during one of the processes handling tokenization. It will create an instance of a tokenizer with `tok_func` and `tok_kwargs`, then iterate through the `items`, apply them `rules`, tokenize them, then apply `output_func` to the original item and the tokens and put the result in `output_queue`.\n",
    "\n",
    "If a `data_queue` is passed, we count the different tokens and return the Counter in it at the end, to save the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_process(items, tok_func, rules, output_func):#Just for testing\n",
    "    output_queue = Queue(1)\n",
    "    process = Process(target=tok_items,\n",
    "                      args=(items, tok_func, rules, output_func, output_queue))\n",
    "    process.start()\n",
    "    for _ in range(2): print(output_queue.get())\n",
    "    process.join()\n",
    "\n",
    "texts = [\"this is a text\", \"this is another text\"]\n",
    "rules = [lambda i: texts[i]]\n",
    "output_func = lambda i,t: t\n",
    "test_stdout(lambda: launch_process([0,1], BaseTokenizer, rules, output_func),\n",
    "           \"\"\"['this', 'is', 'a', 'text']\n",
    "['this', 'is', 'another', 'text']\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize texts in files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to create the same directory structure as in a given folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folders(path, output_dir, include=None):\n",
    "    output_dir = Path(output_dir)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    for i,(p,d,f) in enumerate(os.walk(path)): # returns (dirpath, dirnames, filenames)\n",
    "        if include is not None and i==0: d[:] = [o for o in d if o in include]\n",
    "        else:                            d[:] = [o for o in d if not o.startswith('.')]\n",
    "        for x in d: os.makedirs(output_dir/(Path(p)/Path(x)).relative_to(path), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing function for texts in filenames. Tokenized texts will be saved in a similar fashion in a directory suffixed with `_tok` in the parent folder of `path` (override with `output_dir`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text(fname):\n",
    "    with open(fname, 'r') as f: return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP = 'â–'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = path/'labels.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname.suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_folder(path, extensions=['.txt'], include=None, output_dir=None, n_workers=4,\n",
    "               pre_rules=None, post_rules=None, tok_func=SpacyTokenizer, **tok_kwargs):\n",
    "    path = Path(path)\n",
    "    fnames = get_files(path, extensions=extensions, recurse=True, include=include)\n",
    "    output_dir = Path(ifnone(output_dir, path.parent/f'{path.name}_tok'))\n",
    "    create_folders(path, output_dir, include=include)\n",
    "    pre_rules = [read_text] + listify(ifnone(pre_rules, default_pre_rules.copy()))\n",
    "    post_rules = listify(ifnone(post_rules, default_post_rules.copy()))\n",
    "    \n",
    "    output_queue,data_queue = Queue(maxsize=n_workers),Queue(maxsize=n_workers)\n",
    "    def _output(o, tok):\n",
    "        out = output_dir/o.relative_to(path)\n",
    "        with open(out, 'w') as f: f.write(SEP.join(tok))\n",
    "        with open(out.parent/f'{out.stem}.len', 'w') as f: f.write(str(len(tok)))\n",
    "        return 1\n",
    "            \n",
    "    processes = [Process(target=tok_items,\n",
    "                         args=(batch, tok_func, pre_rules, post_rules, _output, output_queue),\n",
    "                         kwargs={'data_queue': data_queue, **tok_kwargs})\n",
    "                 for i,batch in enumerate(np.array_split(fnames, n_workers))]\n",
    "    \n",
    "    for p in processes: p.start()\n",
    "    counter = Counter()\n",
    "    for _ in progress_bar(fnames, leave=False): _ = output_queue.get()\n",
    "    for _ in processes: counter.update(data_queue.get())\n",
    "    for p in processes: p.join()\n",
    "    pickle.dump(counter, open(output_dir/'counter.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "fnames = get_files(path, extensions=['.txt'], recurse=True, include=['train', 'test', 'unsup'])\n",
    "tok_path = path.parent/'imdb_tok'\n",
    "assert tok_path.exists()\n",
    "#Take one file randomly\n",
    "idx = random.randint(0, len(fnames)-1)\n",
    "#Check we have the corresponding tokenized version...\n",
    "tok_fname = tok_path/(fnames[idx].relative_to(path))\n",
    "assert tok_fname.exists()\n",
    "text = read_text(fnames[idx])\n",
    "tok = tokenize1(text)\n",
    "assert SEP.join(tok) == read_text(tok_fname)\n",
    "len_fname = tok_fname.parent/f'{tok_fname.stem}.len'\n",
    "assert len(tok) == int(read_text(len_fname))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When text is in a dataframe, we need to merge the text columns, and maybe mark_fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_texts(idx, df, mark_fields=False):\n",
    "    return ' '.join([(f'{FLD} {i} ' if mark_fields else '') + t for i,t in enumerate(df.iloc[int(idx)].values)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing function for texts in a dataframe. Tokenized texts will be put in a similar dataframe with just one column of texts and the other columns the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_df(df, text_cols, n_workers=4, pre_rules=None, post_rules=None, mark_fields=None, \n",
    "           tok_func=SpacyTokenizer, **tok_kwargs):\n",
    "    text_cols = listify(text_cols)\n",
    "    mark_fields = ifnone(mark_fields, len(listify(text_cols)) > 1)\n",
    "    pre_rules = listify(ifnone(pre_rules, default_pre_rules.copy()))\n",
    "    pre_rules = [partial(join_texts, df=df[text_cols], mark_fields=mark_fields)] + pre_rules\n",
    "    post_rules = listify(ifnone(post_rules, default_post_rules.copy()))\n",
    "    \n",
    "    output_queue,data_queue = Queue(maxsize=n_workers),Queue(maxsize=n_workers)\n",
    "    def _output(o, tok): return (o,tok)\n",
    "            \n",
    "    processes = [Process(target=tok_items,\n",
    "                         args=(batch, tok_func, pre_rules, post_rules, _output, output_queue),\n",
    "                         kwargs={'data_queue': data_queue, **tok_kwargs})\n",
    "                 for i,batch in enumerate(np.array_split(range(len(df)), n_workers))]\n",
    "    \n",
    "    for p in processes: p.start()\n",
    "    lengths,outputs,counter = np.zeros(len(df)),np.zeros(len(df), dtype=np.object),Counter()\n",
    "    for _ in progress_bar(range(len(df)), leave=False): \n",
    "        i,tok = output_queue.get()\n",
    "        lengths[i],outputs[i] = len(tok),SEP.join(tok)\n",
    "    for _ in processes: counter.update(data_queue.get())\n",
    "    for p in processes: p.join()\n",
    "    \n",
    "    other_cols = [c for c in df.columns if c not in text_cols]\n",
    "    res = df[other_cols].copy()\n",
    "    res['text'],res['text_lengths'] = outputs,lengths\n",
    "    return res, counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "df = pd.read_csv(path/'texts.csv')\n",
    "out,cnt = tok_df(df, text_cols='text')\n",
    "test_eq(set(out.columns),set(list(df.columns)+['text_lengths']))\n",
    "idx = random.randint(0, len(df)-1)\n",
    "text = df['text'][idx]\n",
    "tok = tokenize1(text)\n",
    "test_eq(SEP.join(tok), out['text'][idx])\n",
    "test_eq(len(tok), out['text_lengths'][idx])\n",
    "#With two fields, mark fields become true by default\n",
    "df['text1'] = df['text']\n",
    "out,cnt = tok_df(df, text_cols=['text', 'text1'])\n",
    "idx = random.randint(0, len(df)-1)\n",
    "text = f\"{FLD} 0 {df['text'][idx]} {FLD} 1 {df['text1'][idx]}\"\n",
    "tok = tokenize1(text)\n",
    "test_eq(SEP.join(tok), out['text'][idx])\n",
    "test_eq(len(tok), out['text_lengths'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_csv(fname, text_cols, outname=None, n_workers=4, pre_rules=None, post_rules=None, \n",
    "            mark_fields=None, tok_func=SpacyTokenizer, header='infer', chunksize=None, **tok_kwargs):\n",
    "    df = pd.read_csv(fname, header=header, chunksize=chunksize)\n",
    "    outname = Path(ifnone(outname, fname.parent/f'{fname.stem}_tok.csv'))\n",
    "    kwargs = dict(n_workers=n_workers, pre_rules=pre_rules, post_rules=post_rules, \n",
    "                  mark_fields=mark_fields, tok_func=tok_func, **tok_kwargs)\n",
    "    if chunksize is None:\n",
    "        out,cnt = tok_df(df, text_cols, **kwargs)\n",
    "        out.to_csv(outname, header=header, index=False)\n",
    "    else:\n",
    "        cnt = Counter()\n",
    "        for i,dfp in enumerate(df):\n",
    "            out,c = tok_df(dfp, text_cols, **kwargs)\n",
    "            out.to_csv(outname, header=header if i==0 else None, index=False, mode='w' if i==0 else 'a')\n",
    "            cnt.update(c)\n",
    "    pickle.dump(cnt, open(outname.parent/'counter.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "tok_csv(path/'texts.csv', 'text')\n",
    "assert (path/'texts_tok.csv').exists()\n",
    "df = pd.read_csv(path/'texts.csv')\n",
    "df_tok = pd.read_csv(path/'texts_tok.csv')\n",
    "idx = random.randint(0, len(df)-1)\n",
    "text = df['text'][idx]\n",
    "tok = tokenize1(text)\n",
    "test_eq(SEP.join(tok), df_tok['text'][idx])\n",
    "test_eq(len(tok), df_tok['text_lengths'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "path = untar_data(URLs.IMDB_SAMPLE)\n",
    "tok_csv(path/'texts.csv', 'text', chunksize=500)\n",
    "assert (path/'texts_tok.csv').exists()\n",
    "df = pd.read_csv(path/'texts.csv')\n",
    "df_tok = pd.read_csv(path/'texts_tok.csv')\n",
    "test_eq(len(df_tok), len(df))\n",
    "idx = random.randint(0, len(df)-1)\n",
    "text = df['text'][idx]\n",
    "tok = tokenize1(text)\n",
    "test_eq(SEP.join(tok), df_tok['text'][idx])\n",
    "test_eq(len(tok), df_tok['text_lengths'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpacyTokenizer():\n",
    "    \"Spacy tokenizer for `lang`\"\n",
    "    def __init__(self, lang='en', special_toks=None, batch_size=5000):\n",
    "        special_toks = ifnone(special_toks, default_spec_tok)\n",
    "        self.nlp = spacy.blank(lang, disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "        for w in default_spec_tok: self.nlp.tokenizer.add_special_case(w, [{ORTH: w}])\n",
    "        self.batch_size=batch_size\n",
    "    \n",
    "    def pipe(self, items):\n",
    "        for doc in self.nlp.pipe(items, batch_size=self.batch_size):\n",
    "            yield [d.text for d in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
