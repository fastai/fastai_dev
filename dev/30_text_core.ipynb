{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from local.imports import *\n",
    "from local.test import *\n",
    "from local.core import *\n",
    "from local.data.core import *\n",
    "from local.data.external import *\n",
    "from local.notebook.showdoc import show_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp text.core\n",
    "#default_cls_lvl 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text core\n",
    "\n",
    "> Basic function to preprocess text before assembling it in a `DataBunch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import concurrent.futures\n",
    "from concurrent.futures import as_completed\n",
    "from multiprocessing import Process, Queue\n",
    "import spacy,html\n",
    "from spacy.symbols import ORTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class ProcessPoolExecutor(concurrent.futures.ProcessPoolExecutor):\n",
    "    def __init__(self, max_workers=None, mp_context=None, initializer=None, initargs=()):\n",
    "        self.no_workers = max_workers==0\n",
    "        if self.no_workers: max_workers=1\n",
    "        super().__init__(max_workers, mp_context, initializer=initializer, initargs=initializer)\n",
    "\n",
    "    def map(self, f, items):\n",
    "        return [f(o) for o in items] if self.no_workers else super().map(f, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def parallel(func, items, n_workers=defaults.cpus):\n",
    "    \"Applies `func` in parallel to `items`, using `n_workers`\"\n",
    "    with ProcessPoolExecutor(max_workers=n_workers) as ex:\n",
    "        return [x for x in progress_bar(ex.map(func,items), total=len(items), leave=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_one(x): \n",
    "    time.sleep(random.random()/100)\n",
    "    return x+1\n",
    "\n",
    "test_eq(parallel(add_one, range(100)), range(1,101))\n",
    "test_eq(parallel(add_one, range(100), n_workers=1), range(1,101))\n",
    "test_eq(parallel(add_one, range(100), n_workers=0), range(1,101))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def parallel_gen(cls, items, n_workers=defaults.cpus, as_gen=False, **kwargs):\n",
    "    \"Instantiate `cls` in `n_workers` procs & call each on a subset of `items` in parallel.\"\n",
    "    queue = Queue()\n",
    "    batches = np.array_split(items, n_workers)\n",
    "    idx = np.cumsum(0 + L(batches).mapped(len))\n",
    "    def _f(batch, start_idx):\n",
    "        f = cls(**kwargs)\n",
    "        for i,b in enumerate(f(batch)): queue.put((start_idx+i,b))\n",
    "    processes = [Process(target=_f, args=o) for o in zip(batches,idx)]\n",
    "    for p in processes: p.start()\n",
    "    res = (queue.get() for _ in progress_bar(items, leave=False))\n",
    "    try: return res if as_gen else [o[1] for o in sorted(res)]\n",
    "    finally:\n",
    "        for p in processes: p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cls` is any class with `__call__`. It will be passed `args` and `kwargs` when initialized. Note that `n_workers` instances of `cls` are created, one in each process. `items` are then split in `n_workers` batches and one is sent to each `cls`. The function then returns a list of all the results, matching the order of `items` (if not `as_gen`) or a generator of tuples of item indices and results (if `as_gen`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SleepyBatchFunc:\n",
    "    def __init__(self): self.a=1\n",
    "    def __call__(self, batch):\n",
    "        for k in batch:\n",
    "            time.sleep(random.random()/10)\n",
    "            yield k+self.a\n",
    "\n",
    "x = np.linspace(0,0.99,100)\n",
    "res = parallel_gen(SleepyBatchFunc, x, n_workers=2)\n",
    "test_eq(res, x+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are rules applied to texts before or after it's tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#special tokens\n",
    "UNK, PAD, BOS, EOS, FLD, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxfld xxrep xxwrep xxup xxmaj\".split()\n",
    "_all_ = ['UNK', 'PAD', 'BOS', 'EOS', 'FLD', 'TK_REP', 'TK_WREP', 'TK_UP', 'TK_MAJ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_spec = re.compile(r'([/#\\\\])')\n",
    "\n",
    "def spec_add_spaces(t):\n",
    "    \"Add spaces around / and #\"\n",
    "    return _re_spec.sub(r' \\1 ', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(spec_add_spaces('#fastai'), ' # fastai')\n",
    "test_eq(spec_add_spaces('/fastai'), ' / fastai')\n",
    "test_eq(spec_add_spaces('\\\\fastai'), ' \\\\ fastai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_space = re.compile(' {2,}')\n",
    "\n",
    "def rm_useless_spaces(t):\n",
    "    \"Remove multiple spaces\"\n",
    "    return _re_space.sub(' ', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(rm_useless_spaces('a  b   c'), 'a b c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_rep = re.compile(r'(\\S)(\\1{2,})')\n",
    "\n",
    "def replace_rep(t):\n",
    "    \"Replace repetitions at the character level: cccc -> TK_REP 4 c\"\n",
    "    def _replace_rep(m):\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_REP} {len(cc)+1} {c} '\n",
    "    return _re_rep.sub(_replace_rep, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It starts replacing at 3 repetitions of the same character or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(replace_rep('aa'), 'aa')\n",
    "test_eq(replace_rep('aaaa'), f' {TK_REP} 4 a ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_wrep = re.compile(r'(?:\\s|^)(\\w+)\\s+((?:\\1\\s+)+)\\1(\\s|\\W|$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\"\"\"\n",
    "Matches any word repeated at least four times with spaces between them\n",
    "(?:\\s|^)       Non-catching group with either a whitespace character or the beginning of text\n",
    "(\\w+)          Catching group of any alphanumeric character\n",
    "\\s+            One or more whitespace\n",
    "((?:\\1\\s+)+)   Catching group of a repetition of one or more times \\1 followed by one or more whitespace\n",
    "\\1             Occurence of \\1\n",
    "(\\s|\\W|$)      Catching group of last whitespace, non alphanumeric character or end of text\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_wrep(t):\n",
    "    \"Replace word repetitions: word word word word -> TK_WREP 4 word\"\n",
    "    def _replace_wrep(m):\n",
    "        c,cc,e = m.groups()\n",
    "        return f' {TK_WREP} {len(cc.split())+2} {c} {e}'\n",
    "    return _re_wrep.sub(_replace_wrep, t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It starts replacing at 3 repetitions of the same word or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(replace_wrep('ah ah'), 'ah ah')\n",
    "test_eq(replace_wrep('ah ah ah'), f' {TK_WREP} 3 ah ')\n",
    "test_eq(replace_wrep('ah ah   ah  ah'), f' {TK_WREP} 4 ah ')\n",
    "test_eq(replace_wrep('ah ah ah ah '), f' {TK_WREP} 4 ah  ')\n",
    "test_eq(replace_wrep('ah ah ah ah.'), f' {TK_WREP} 4 ah .')\n",
    "test_eq(replace_wrep('ah ah ahi'), f'ah ah ahi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def fix_html(x):\n",
    "    \"Various messy things we've seen in documents\"\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace('nbsp;', ' ').replace(\n",
    "        '#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace('<br />', \"\\n\").replace(\n",
    "        '\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(' @-@ ','-').replace('...',' …')\n",
    "    return html.unescape(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(fix_html('#39;bli#146;'), \"'bli'\")\n",
    "test_eq(fix_html('Sarah amp; Duck...'), 'Sarah & Duck …')\n",
    "test_eq(fix_html('a nbsp; #36;'), 'a   $')\n",
    "test_eq(fix_html('\\\\\" <unk>'), f'\" {UNK}')\n",
    "test_eq(fix_html('quot;  @.@  @-@ '), \"' .-\")\n",
    "test_eq(fix_html('<br />text\\\\n'), '\\ntext\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_all_caps = re.compile(r'(\\s|^)([A-Z]+[^a-z\\s]*)(?=(\\s|$))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\"\"\"\n",
    "Catches any word in all caps, even with ' or - inside\n",
    "(\\s|^)        Catching group with either a whitespace or the beginning of text\n",
    "([A-Z]+       Catching group with one capitalized letter or more...\n",
    "[^a-z\\s]*)    ...followed by anything that's non lowercase or whitespace\n",
    "(?=(\\s|$))    Look ahead for a space of end of text\n",
    "The look ahead is there to not move the pointer ahead of the next space in case we have consecutive words in all caps.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_all_caps(t):\n",
    "    \"Replace tokens in ALL CAPS by their lower version and add `TK_UP` before.\"\n",
    "    def _replace_all_caps(m):\n",
    "        tok = f'{TK_UP} ' if len(m.groups()[1]) > 1 else ''\n",
    "        return f\"{m.groups()[0]}{tok}{m.groups()[1].lower()}\"\n",
    "    return _re_all_caps.sub(_replace_all_caps, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(replace_all_caps(\"I'M SHOUTING\"), f\"{TK_UP} i'm {TK_UP} shouting\")\n",
    "test_eq(replace_all_caps(\"I'm speaking normally\"), \"I'm speaking normally\")\n",
    "test_eq(replace_all_caps(\"I am speaking normally\"), \"i am speaking normally\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_re_maj = re.compile(r'(\\s|^)([A-Z][^A-Z\\s]*)(?=(\\s|$))')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "\"\"\"\n",
    "Catches any capitalized word\n",
    "(\\s|^)       Catching group with either a whitespace or the beginning of text\n",
    "([A-Z]       Catching group with exactly one capitalized letter...\n",
    "[^A-Z\\s]*)   ...followed by anything that's not uppercase or whitespace\n",
    "(?=(\\s|$))   Look ahead for a space of end of text\n",
    "The look ahead is there to not move the pointer ahead of the next space in case we have consecutive words in all caps.\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_maj(t):\n",
    "    \"Replace tokens in ALL CAPS by their lower version and add `TK_UP` before.\"\n",
    "    def _replace_maj(m):\n",
    "        tok = f'{TK_MAJ} ' if len(m.groups()[1]) > 1 else ''\n",
    "        return f\"{m.groups()[0]}{tok}{m.groups()[1].lower()}\"\n",
    "    return _re_maj.sub(_replace_maj, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(replace_maj(\"Jeremy Howard\"), f'{TK_MAJ} jeremy {TK_MAJ} howard')\n",
    "test_eq(replace_maj(\"I don't think there is any maj here\"), (\"i don't think there is any maj here\"),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lowercase(t, add_bos=True, add_eos=False):\n",
    "    \"Converts `t` to lowercase\"\n",
    "    return (f'{BOS} ' if add_bos else '') + t.lower().strip() + (f' {EOS}' if add_eos else '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_space(t):\n",
    "    \"Replace embedded spaces in a token with unicode line char to allow for split/join\"\n",
    "    return t.replace(' ', '▁')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "defaults.text_spec_tok = [UNK, PAD, BOS, EOS, FLD, TK_REP, TK_WREP, TK_UP, TK_MAJ]\n",
    "defaults.text_proc_rules = [fix_html, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces,\n",
    "                            replace_all_caps, replace_maj, lowercase]\n",
    "defaults.text_postproc_rules = [replace_space]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenizer is a class that must implement a `pipe` method. This `pipe` method receives a generator of texts and must return a generator with their tokenized versions. Here is the most basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class BaseTokenizer():\n",
    "    \"Basic tokenizer that just splits on spaces\"\n",
    "    def __init__(self, split_char=' ', **kwargs): self.split_char=split_char\n",
    "    def pipe(self, items): return (t.split(self.split_char) for t in items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = BaseTokenizer()\n",
    "for t in tok.pipe([\"This is a text\"]): test_eq(t, [\"This\", \"is\", \"a\", \"text\"])\n",
    "tok = BaseTokenizer('x')\n",
    "for t in tok.pipe([\"This is a text\"]): test_eq(t, [\"This is a te\", \"t\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SpacyTokenizer():\n",
    "    \"Spacy tokenizer for `lang`\"\n",
    "    def __init__(self, lang='en', special_toks=None, batch_size=5000):\n",
    "        special_toks = ifnone(special_toks, defaults.text_spec_tok)\n",
    "        self.nlp = spacy.blank(lang, disable=[\"parser\", \"tagger\", \"ner\"])\n",
    "        for w in special_toks: self.nlp.tokenizer.add_special_case(w, [{ORTH: w}])\n",
    "        self.batch_size=batch_size\n",
    "    \n",
    "    def pipe(self, items):\n",
    "        for doc in self.nlp.pipe(items, batch_size=self.batch_size):\n",
    "            yield [d.text for d in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = SpacyTokenizer()\n",
    "for t in tok.pipe([\"This isn't the easiest text.\"]): \n",
    "    test_eq(t, [\"This\", \"is\", \"n't\", \"the\", \"easiest\", \"text\", \".\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def apply_rules(items, rules):\n",
    "    \"Returns a generator that apply `rules`  to `items`\"\n",
    "    return map(compose(*rules), items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in apply_rules([\"This is a text\"], [replace_maj]): test_eq(t, f\"{TK_MAJ} this is a text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokenizeBatch:\n",
    "    \"A wrapper around `tok_func` to apply `rules` and tokenize in parallel\"\n",
    "    def __init__(self, tok_func=SpacyTokenizer, rules=None, post_rules=None, **tok_kwargs ):\n",
    "        self.rules = L(ifnone(rules, defaults.text_proc_rules))\n",
    "        self.post_f = compose(*L(ifnone(post_rules, defaults.text_postproc_rules)))\n",
    "        self.tok = tok_func(**tok_kwargs)\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        for o in self.tok.pipe(apply_rules(batch, self.rules)): yield L(o).mapped(self.post_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = TokenizeBatch()\n",
    "test_eq(f([\"This isn't a problem\"]), [[BOS, TK_MAJ, 'this', 'is', \"n't\", 'a', 'problem']])\n",
    "f = TokenizeBatch(BaseTokenizer, rules=[], split_char=\"'\")\n",
    "test_eq(f([\"This isn't a problem\"]), [['This▁isn', 't▁a▁problem']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main function that will be called during one of the processes handling tokenization. It will create an instance of a tokenizer with `tok_func` and `tok_kwargs` at init, then iterate through the `batch` of texts, apply them `rules` and tokenize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"this is a text\", \"this is another text\"]\n",
    "tok = TokenizeBatch(BaseTokenizer, texts.__getitem__)\n",
    "test_eq([t for t in tok([0,1])],[['this', 'is', 'a', 'text'], ['this', 'is', 'another', 'text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize1(text, tok_func=SpacyTokenizer, rules=None, post_rules=None, **tok_kwargs):\n",
    "    \"Tokenize one `text` with an instance of `tok_func` and some `rules`\"\n",
    "    return next(iter(TokenizeBatch(tok_func, rules, post_rules, **tok_kwargs)([text])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(tokenize1(\"This isn't a problem\"),\n",
    "        [BOS, TK_MAJ, 'this', 'is', \"n't\", 'a', 'problem'])\n",
    "test_eq(tokenize1(\"This isn't a problem\", BaseTokenizer, rules=[], split_char=\"'\"),\n",
    "        ['This▁isn', 't▁a▁problem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def parallel_tokenize(items, tok_func, rules, as_gen=False, n_workers=defaults.cpus, **tok_kwargs):\n",
    "    \"Calls a potential setup on `tok_func` before launching `TokenizeBatch` in parallel\"\n",
    "    if hasattr(tok_func, 'setup'): tok_kwargs = tok_func(**tok_kwargs).setup(items, rules)\n",
    "    return parallel_gen(TokenizeBatch, items, as_gen=as_gen, tok_func=tok_func,\n",
    "                        rules=rules, n_workers=n_workers, **tok_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize texts in files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing function for texts in filenames. Tokenized texts will be saved in a similar fashion in a directory suffixed with `_tok` in the parent folder of `path` (override with `output_dir`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def read(self:Path):\n",
    "    \"Read the content of `fname`\"\n",
    "    with self.open() as f: return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@patch\n",
    "def write(self:Path, txt):\n",
    "    \"Write `txt` to `self`, creating directories as needed\"\n",
    "    self.parent.mkdir(parents=True,exist_ok=True)\n",
    "    with self.open('w') as f: f.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize_folder(path, extensions=None, include=None, output_dir=None, n_workers=defaults.cpus,\n",
    "                    rules=None, tok_func=SpacyTokenizer, **tok_kwargs):\n",
    "    \"Tokenize text files in `path` in parallel using `n_workers`\"\n",
    "    path,extensions = Path(path),ifnone(extensions, ['.txt'])\n",
    "    fnames = get_files(path, extensions=extensions, recurse=True, include=include)\n",
    "    output_dir = Path(ifnone(output_dir, path.parent/f'{path.name}_tok'))\n",
    "    rules = Path.read + L(ifnone(rules, defaults.text_proc_rules.copy()))\n",
    "    \n",
    "    counter = Counter()\n",
    "    for i,tok in parallel_tokenize(fnames, tok_func, rules, as_gen=True, n_workers=n_workers, **tok_kwargs):\n",
    "        out = output_dir/fnames[i].relative_to(path)\n",
    "        out.write(' '.join(tok))\n",
    "        out.with_suffix('.len').write(str(len(tok)))\n",
    "        counter.update(tok)\n",
    "\n",
    "    pickle.dump(counter, open(output_dir/'counter.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be in `output_dir` (defaults to a folder in the same parent directory as `path`, with `_tok` added to `path.name`) with the same structure as in `path`. Tokenized texts for a given file will be in the file having the same name in `output_dir`. Additionally, a file with a .len suffix contains the number of tokens and the count of all words is stored in `output_dir/counter.pkl`.\n",
    "\n",
    "`extensions` will default to `['.txt']` and all text files in `path` are treated unless you specify a list of folders in `include`. `tok_func` is instantiated in each process with `tok_kwargs`, and `rules` (that defaults to `defaults.text_proc_rules`) are applied to each text before going in the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: test include option\n",
    "path = Path('tmp')\n",
    "os.makedirs(path, exist_ok=True)\n",
    "for d in ['a', 'b', 'c']: \n",
    "    os.makedirs(path/d, exist_ok=True)\n",
    "    for i in range(5):\n",
    "        with open(path/d/f'text{i}.txt', 'w') as f: f.write(f\"This is an example of text {d} {i}\")\n",
    "tokenize_folder(path)\n",
    "outp = Path('tmp_tok')\n",
    "assert outp.is_dir()\n",
    "for d in ['a', 'b', 'c']: \n",
    "    p = outp/d\n",
    "    assert p.is_dir()\n",
    "    for i in range(5):\n",
    "        assert (p/f'text{i}.txt').is_file()\n",
    "        assert (p/f'text{i}.len').is_file()\n",
    "        test_eq((p/f'text{i}.txt').read(), ' '.join([\n",
    "            BOS, TK_MAJ, 'this', 'is', 'an', 'example', 'of', 'text', d, str(i) ]))\n",
    "        test_eq((p/f'text{i}.len').read(), '10')\n",
    "\n",
    "shutil.rmtree(path)\n",
    "shutil.rmtree(outp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize texts in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _join_texts(df, mark_fields=False):\n",
    "    \"Join texts in row `idx` of `df`, marking each field with `FLD` if `mark_fields=True`\"\n",
    "    text_col = (f'{FLD} {1} ' if mark_fields else '' ) + df.iloc[:,0].astype(str)\n",
    "    for i in range(1,len(df.columns)):\n",
    "        text_col += (f' {FLD} {i+1} ' if mark_fields else ' ') + df.iloc[:,i].astype(str)\n",
    "    return text_col.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def tokenize_df(df, text_cols, n_workers=defaults.cpus, rules=None, mark_fields=None, \n",
    "                tok_func=SpacyTokenizer, **tok_kwargs):\n",
    "    \"Tokenize texts in `df[text_cols]` in parallel using `n_workers`\"\n",
    "    text_cols = L(text_cols)\n",
    "    mark_fields = ifnone(mark_fields, len(text_cols) > 1)\n",
    "    rules = L(ifnone(rules, defaults.text_proc_rules.copy()))\n",
    "    texts = _join_texts(df[text_cols], mark_fields=mark_fields)\n",
    "    lengths,outputs,counter = [],[],Counter()\n",
    "    \n",
    "    for tok in parallel_tokenize(texts, tok_func, rules, n_workers=n_workers, **tok_kwargs):\n",
    "        lengths.append(len(tok))\n",
    "        outputs.append(' '.join(tok))\n",
    "        counter.update(tok)\n",
    "        \n",
    "    other_cols = [c for c in df.columns if c not in text_cols]\n",
    "    res = df[other_cols].copy()\n",
    "    res['text'],res['text_lengths'] = outputs,lengths\n",
    "    return res,counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns a new dataframe with the same non-text columns, a colum named text that contains the tokenized texts and a column named text_lengths that contains their respective length. It also returns a counter of all words see to quickly build a vocabulary afterward.\n",
    "\n",
    "`tok_func` is instantiated in each process with `tok_kwargs`, and `rules` (that defaults to `defaults.text_proc_rules`) are applied to each text before going in the tokenizer. If `mark_fields` isn't specified, it defaults to `False` when there is a single text column, `True` when there are several. In that case, the texts in each of those columns are joined with `FLD` markes followed by the number of the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [f\"This is an example of text {i}\" for i in range(10)]\n",
    "df = pd.DataFrame({'text': texts, 'label': list(range(10))}, columns=['text', 'label'])\n",
    "out,cnt = tokenize_df(df, text_cols='text')\n",
    "test_eq(list(out.columns), ['label', 'text', 'text_lengths'])\n",
    "test_eq(out['label'].values, df['label'].values)\n",
    "for i in range(len(df)):\n",
    "    test_eq(out['text'][i], ' '.join([\n",
    "        BOS, TK_MAJ, 'this', 'is', 'an', 'example', 'of', 'text', str(i) ]))\n",
    "    test_eq(out['text_lengths'][i], 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "#With two columns of text, mark_fields defaults to True\n",
    "df['text1'] = df['text'].values\n",
    "out,cnt = tokenize_df(df, text_cols=['text', 'text1'])\n",
    "test_eq(list(out.columns), ['label', 'text', 'text_lengths'])\n",
    "test_eq(out['label'].values, df['label'].values)\n",
    "for i in range(len(df)):\n",
    "    test_eq(out['text'][i], ' '.join([\n",
    "        BOS, FLD, '1', TK_MAJ, 'this', 'is', 'an', 'example', 'of', 'text', str(i),\n",
    "             FLD, '2', TK_MAJ, 'this', 'is', 'an', 'example', 'of', 'text', str(i)\n",
    "    ]))\n",
    "    test_eq(out['text_lengths'][i], 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#TODO: test + rework\n",
    "def tokenize_csv(fname, text_cols, outname=None, n_workers=4, rules=None, mark_fields=None, \n",
    "                 tok_func=SpacyTokenizer, header='infer', chunksize=None, **tok_kwargs):\n",
    "    \"Tokenize texts in the `text_cols` of the csv `fname` in parallel using `n_workers`\"\n",
    "    df = pd.read_csv(fname, header=header, chunksize=chunksize)\n",
    "    outname = Path(ifnone(outname, fname.parent/f'{fname.stem}_tok.csv'))\n",
    "    kwargs = dict(n_workers=n_workers, pre_rules=pre_rules, post_rules=post_rules, \n",
    "                  mark_fields=mark_fields, tok_func=tok_func, **tok_kwargs)\n",
    "    if chunksize is None:\n",
    "        out,cnt = tok_df(df, text_cols, **kwargs)\n",
    "        out.to_csv(outname, header=header, index=False)\n",
    "    else:\n",
    "        cnt = Counter()\n",
    "        for i,dfp in enumerate(df):\n",
    "            out,c = tok_df(dfp, text_cols, **kwargs)\n",
    "            out.to_csv(outname, header=header if i==0 else None, index=False, mode='w' if i==0 else 'a')\n",
    "            cnt.update(c)\n",
    "    pickle.dump(cnt, open(outname.parent/'counter.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be written in a new csv file in `outname` (defaults to the same as `fname` with the suffix `_tok.csv`) and will have the same header as the original file, the same non-text columns, a text and a text_lengths column as described in `tokenize_df`.\n",
    "\n",
    "`tok_func` is instantiated in each process with `tok_kwargs`, and `rules` (that defaults to `defaults.text_proc_rules`) are applied to each text before going in the tokenizer. If `mark_fields` isn't specified, it defaults to `False` when there is a single text column, `True` when there are several. In that case, the texts in each of those columns are joined with `FLD` markes followed by the number of the field.\n",
    "\n",
    "The csv file is opened with `header` and optionally with blocks of `chunksize` at a time. If this argument is passed, each chunk is processed independtly and saved in the output file to save memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_langs = [\"bg\", \"cs\", \"da\", \"de\", \"el\", \"en\", \"es\", \"et\", \"fi\", \"fr\", \"ga\", \"hr\", \"hu\",\n",
    "            \"it\",\"lt\",\"lv\",\"mt\",\"nl\",\"pl\",\"pt\",\"ro\",\"sk\",\"sl\",\"sv\"] # all European langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SentencePieceTokenizer():#TODO: pass the special tokens symbol to sp\n",
    "    \"Spacy tokenizer for `lang`\"\n",
    "    def __init__(self, lang='en', special_toks=None, sp_model=None, vocab_sz=None, max_vocab_sz=30000, \n",
    "                 model_type='unigram', char_coverage=None, cache_dir='tmp'):\n",
    "        try: from sentencepiece import SentencePieceTrainer,SentencePieceProcessor\n",
    "        except ImportError:\n",
    "            raise Exception('sentencepiece module is missing: run `pip install sentencepiece`')\n",
    "        self.sp_model,self.cache_dir = sp_model,Path(cache_dir)\n",
    "        self.vocab_sz,self.max_vocab_sz,self.model_type = vocab_sz,max_vocab_sz,model_type\n",
    "        self.char_coverage = ifnone(char_coverage, 0.99999 if lang in eu_langs else 0.9998)\n",
    "        self.special_toks = ifnone(special_toks, defaults.text_spec_tok)\n",
    "        if sp_model is None: self.tok = None\n",
    "        else:\n",
    "            self.tok = SentencePieceProcessor()\n",
    "            self.tok.Load(str(sp_model))\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "    \n",
    "    def _get_vocab_sz(self, raw_text_path):\n",
    "        cnt = Counter()\n",
    "        with open(raw_text_path, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                cnt.update(line.split())\n",
    "                if len(cnt)//4 > self.max_vocab_sz: return self.max_vocab_sz\n",
    "        res = len(cnt)//4\n",
    "        while res%8 != 0: res+=1\n",
    "        return res\n",
    "    \n",
    "    def train(self, raw_text_path):\n",
    "        \"Train a sentencepiece tokenizer on `texts` and save it in `path/tmp_dir`\"\n",
    "        from sentencepiece import SentencePieceTrainer\n",
    "        vocab_sz = self._get_vocab_sz(raw_text_path) if self.vocab_sz is None else self.vocab_sz\n",
    "        spec_tokens = ['\\u2581'+s for s in self.special_toks]\n",
    "        SentencePieceTrainer.Train(\" \".join([\n",
    "            f\"--input={raw_text_path} --vocab_size={vocab_sz} --model_prefix={self.cache_dir/'spm'}\",\n",
    "            f\"--character_coverage={self.char_coverage} --model_type={self.model_type}\",\n",
    "            f\"--unk_id={len(spec_tokens)} --pad_id=-1 --bos_id=-1 --eos_id=-1\",\n",
    "            f\"--user_defined_symbols={','.join(spec_tokens)}\"]))\n",
    "        raw_text_path.unlink()\n",
    "        return self.cache_dir/'spm.model'\n",
    "    \n",
    "    def setup(self, items, rules):\n",
    "        if self.tok is not None: return {'sp_model': self.sp_model}\n",
    "        raw_text_path = self.cache_dir/'texts.out'\n",
    "        with open(raw_text_path, 'w') as f:\n",
    "            for t in progress_bar(apply_rules(items, rules), total=len(items), leave=False): \n",
    "                f.write(f'{t}\\n')\n",
    "        return {'sp_model': self.train(raw_text_path)}\n",
    "        \n",
    "    def pipe(self, items):\n",
    "        for t in items: yield self.tok.EncodeAsPieces(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [f\"This is an example of text {i}\" for i in range(10)]\n",
    "df = pd.DataFrame({'text': texts, 'label': list(range(10))}, columns=['text', 'label'])\n",
    "out,cnt = tokenize_df(df, text_cols='text', tok_func=SentencePieceTokenizer, vocab_sz=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_lengths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>▁xxbos ▁xxmaj ▁ t h i s ▁ i s ▁ a n ▁ e x a m p l e ▁ o f ▁ t e x t ▁ 0</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>▁xxbos ▁xxmaj ▁ t h i s ▁ i s ▁ a n ▁ e x a m p l e ▁ o f ▁ t e x t ▁ 1</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>▁xxbos ▁xxmaj ▁ t h i s ▁ i s ▁ a n ▁ e x a m p l e ▁ o f ▁ t e x t ▁ 2</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>▁xxbos ▁xxmaj ▁ t h i s ▁ i s ▁ a n ▁ e x a m p l e ▁ o f ▁ t e x t ▁ 3</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>▁xxbos ▁xxmaj ▁ t h i s ▁ i s ▁ a n ▁ e x a m p l e ▁ o f ▁ t e x t ▁ 4</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>▁xxbos ▁xxmaj ▁ t h i s ▁ i s ▁ a n ▁ e x a m p l e ▁ o f ▁ t e x t ▁ 5</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>▁xxbos ▁xxmaj ▁ t h i s ▁ i s ▁ a n ▁ e x a m p l e ▁ o f ▁ t e x t ▁ 6</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>▁xxbos ▁xxmaj ▁ t h i s ▁ i s ▁ a n ▁ e x a m p l e ▁ o f ▁ t e x t ▁ 7</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>▁xxbos ▁xxmaj ▁ t h i s ▁ i s ▁ a n ▁ e x a m p l e ▁ o f ▁ t e x t ▁ 8</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>▁xxbos ▁xxmaj ▁ t h i s ▁ i s ▁ a n ▁ e x a m p l e ▁ o f ▁ t e x t ▁ 9</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  \\\n",
       "0      0   \n",
       "1      1   \n",
       "2      2   \n",
       "3      3   \n",
       "4      4   \n",
       "5      5   \n",
       "6      6   \n",
       "7      7   \n",
       "8      8   \n",
       "9      9   \n",
       "\n",
       "                                                                      text  \\\n",
       "0  ▁xxbos ▁xxmaj ▁ t h i s ▁ i s ▁ a n ▁ e x a m p l e ▁ o f ▁ t e x t ▁ 0   \n",
       "1  ▁xxbos ▁xxmaj ▁ t h i s ▁ i s ▁ a n ▁ e x a m p l e ▁ o f ▁ t e x t ▁ 1   \n",
       "2  ▁xxbos ▁xxmaj ▁ t h i s ▁ i s ▁ a n ▁ e x a m p l e ▁ o f ▁ t e x t ▁ 2   \n",
       "3  ▁xxbos ▁xxmaj ▁ t h i s ▁ i s ▁ a n ▁ e x a m p l e ▁ o f ▁ t e x t ▁ 3   \n",
       "4  ▁xxbos ▁xxmaj ▁ t h i s ▁ i s ▁ a n ▁ e x a m p l e ▁ o f ▁ t e x t ▁ 4   \n",
       "5  ▁xxbos ▁xxmaj ▁ t h i s ▁ i s ▁ a n ▁ e x a m p l e ▁ o f ▁ t e x t ▁ 5   \n",
       "6  ▁xxbos ▁xxmaj ▁ t h i s ▁ i s ▁ a n ▁ e x a m p l e ▁ o f ▁ t e x t ▁ 6   \n",
       "7  ▁xxbos ▁xxmaj ▁ t h i s ▁ i s ▁ a n ▁ e x a m p l e ▁ o f ▁ t e x t ▁ 7   \n",
       "8  ▁xxbos ▁xxmaj ▁ t h i s ▁ i s ▁ a n ▁ e x a m p l e ▁ o f ▁ t e x t ▁ 8   \n",
       "9  ▁xxbos ▁xxmaj ▁ t h i s ▁ i s ▁ a n ▁ e x a m p l e ▁ o f ▁ t e x t ▁ 9   \n",
       "\n",
       "   text_lengths  \n",
       "0            31  \n",
       "1            31  \n",
       "2            31  \n",
       "3            31  \n",
       "4            31  \n",
       "5            31  \n",
       "6            31  \n",
       "7            31  \n",
       "8            31  \n",
       "9            31  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_test.ipynb.\n",
      "Converted 01_core.ipynb.\n",
      "Converted 01a_script.ipynb.\n",
      "Converted 02_transforms.ipynb.\n",
      "Converted 03_pipeline.ipynb.\n",
      "Converted 04_data_external.ipynb.\n",
      "Converted 05_data_core.ipynb.\n",
      "Converted 06_data_source.ipynb.\n",
      "Converted 07_vision_core.ipynb.\n",
      "Converted 08_pets_tutorial.ipynb.\n",
      "Converted 09_vision_augment.ipynb.\n",
      "Converted 09a_rect_augment.ipynb.\n",
      "Converted 10_data_block.ipynb.\n",
      "Converted 11_layers.ipynb.\n",
      "Converted 12_optimizer.ipynb.\n",
      "Converted 13_learner.ipynb.\n",
      "Converted 14_callback_schedule.ipynb.\n",
      "Converted 15_callback_hook.ipynb.\n",
      "Converted 16_callback_progress.ipynb.\n",
      "Converted 17_callback_tracker.ipynb.\n",
      "Converted 18_callback_fp16.ipynb.\n",
      "Converted 19_callback_mixup.ipynb.\n",
      "Converted 20_metrics.ipynb.\n",
      "Converted 21_tutorial_imagenette.ipynb.\n",
      "Converted 30_text_core.ipynb.\n",
      "Converted 40_tabular_core.ipynb.\n",
      "Converted 60_vision_models_xresnet.ipynb.\n",
      "Converted 90_notebook_core.ipynb.\n",
      "Converted 91_notebook_export.ipynb.\n",
      "Converted 92_notebook_showdoc.ipynb.\n",
      "Converted 93_notebook_export2html.ipynb.\n",
      "Converted 94_index.ipynb.\n",
      "Converted 95_synth_learner.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from local.notebook.export import notebook2script\n",
    "notebook2script(all_fs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
