{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.docs import *\n",
    "from fastai.text import *\n",
    "torch.backends.cudnn.benchmark=True\n",
    "import json\n",
    "\n",
    "import fastText as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeVise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this class, we will implement the [DeVise paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41473.pdf). What makes this paper specially interesting is that it combines image classification and text embeddings. The technique presented by the authors leverages word embeddings to assign several possible tags to each image. By doing this, the model fares considerably well (achieving up to 18% hit rates) in never seen before categories (zero-shot learning). But how can the model classify objects it has never seen before? That is the power of word embeddings.\n",
    "\n",
    "Basically, the model will use the 'closeness' of several words it knows through the embeddings to classify a new image. Perhaps this is easiest explained through a human example. When we are teaching a toddler what a motorcycle is, for example, we might say \"Well, it is a bicycle but it goes faster\". That is we relate it to what *he/she already knows*. In the same way, if the model sees a trout, it might say \"Well, I know it is very similar to a trench and I know what a trench is so I will say it is either a trench or something very similar, like a sea bass or a trout\". In 2D these relationships would look like this:\n",
    "\n",
    "![clusters](imgs/clusters.png)\n",
    "Frome et al., 2013\n",
    "\n",
    "Please consider that while you may say \"Obviously, a goldfish has to do more with a shark than with an iguana because they are both aquatic\" you are comparing these across one dimension, namely natural habitat, while if you compared them by size the results would be different. These infinite dimensions across which you can compare two words are resumed into a finite number of categories which is what we call embeddings. In this image, we are arbitrarily choosing one dimension to make the point since it is intuitive to us human beings.\n",
    "\n",
    "To create this network the authors combined a computer vision architecture with the embeddings data to create a hybrid model that we can see in the following picture:\n",
    "\n",
    "![devise_arch](imgs/devise_arch.png)\n",
    "Frome et al., 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('../data/imagenet/')\n",
    "TMP_PATH = Path('../data/imagenet/tmp')\n",
    "TRANS_PATH = Path('../data/translate/')\n",
    "PATH_TRN = PATH/'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open(PATH/fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to load our word vectors. We'll see that each word has a normalized number between [-1, 1] for each of the 300 embeddings. This is effectively a 300 dimension representation of the meaning of each word. As an example, let's see the embedding for 'king'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_vecs = ft.load_model(str((TRANS_PATH/'wiki.en.bin')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.03258984, -0.1816415 , -0.2904873 , -0.10505921, -0.16711563,\n",
       "       -0.07748009, -0.56610477, -0.08621992, -0.00216415,  0.15366264,\n",
       "        0.12189342, -0.14721851,  0.01511091,  0.07209041, -0.02156133,\n",
       "       -0.20612358, -0.02104198, -0.01999179, -0.1550624 ,  0.00802041,\n",
       "       -0.22745976,  0.33518317, -0.10629012, -0.5031787 , -0.15820101,\n",
       "        0.2782948 ,  0.0575151 , -0.32697093,  0.04766395,  0.01075639,\n",
       "        0.13971788, -0.1244529 , -0.1898885 ,  0.3296893 , -0.32513466,\n",
       "        0.10957629,  0.21961868, -0.47214925,  0.03422011, -0.22069897,\n",
       "        0.0217707 ,  0.08320427, -0.04775527, -0.48873124,  0.05207052,\n",
       "       -0.15001085, -0.1920319 ,  0.06177191,  0.15535438, -0.05597998,\n",
       "        0.11070774,  0.39161414, -0.17715958,  0.05448888,  0.25898436,\n",
       "       -0.13954401,  0.4271997 , -0.07272825, -0.47139668,  0.04992674,\n",
       "        0.2952612 , -0.05318918,  0.03451393, -0.10583404, -0.3013718 ,\n",
       "        0.16371812,  0.07540949,  0.21017765, -0.11459109,  0.10975838,\n",
       "        0.04923463,  0.17687643,  0.45658308, -0.5976241 , -0.0039    ,\n",
       "        0.0886564 ,  0.5310349 ,  0.15299918, -0.16729951,  0.13121451,\n",
       "       -0.05546572, -0.03581958, -0.3453494 ,  0.09128479,  0.03323454,\n",
       "        0.45210728, -0.16893888,  0.21139206,  0.24152668,  0.5101379 ,\n",
       "       -0.01474016, -0.47179154,  0.2234959 , -0.34667644,  0.12126154,\n",
       "        0.2372651 , -0.08424412,  0.04554816, -0.07697721,  0.04279696,\n",
       "       -0.13887408,  0.29286367, -0.28864288,  0.534462  ,  0.02676576,\n",
       "       -0.04118502,  0.40155724,  0.38333884,  0.01934905,  0.02088826,\n",
       "        0.02142423, -0.11957626, -0.44997135,  0.13684812, -0.1218469 ,\n",
       "       -0.0050911 ,  0.6034169 ,  0.6588873 , -0.16250865,  0.46392858,\n",
       "        0.19732077,  0.19345535, -0.07764955,  0.17386931,  0.07278897,\n",
       "        0.04364506, -0.0124623 ,  0.43920207,  0.0318237 ,  0.34926617,\n",
       "       -0.13155296,  0.41264588,  0.13480337,  0.03162143,  0.17820732,\n",
       "        0.20899288, -0.03223703, -0.3779873 ,  0.23646364,  0.10512199,\n",
       "       -0.00482618,  0.33616552,  0.43213743,  0.28264466,  0.01724773,\n",
       "        0.35154724,  0.2850445 , -0.4146821 , -0.208589  ,  0.08934677,\n",
       "       -0.08568424, -0.39820084, -0.61610675,  0.573997  , -0.34190702,\n",
       "        0.03569467,  0.08309295,  0.0275823 ,  0.30767068, -0.14426386,\n",
       "       -0.23718187,  0.19269057,  0.12444276,  0.20298469, -0.08635711,\n",
       "       -0.30211648,  0.06119148,  0.08864971,  0.60565126,  0.2309215 ,\n",
       "       -0.16017546, -0.44801518, -0.14103134,  0.08388583,  0.08604109,\n",
       "        0.17386845, -0.11658656,  0.15750763, -0.25178352,  0.12577309,\n",
       "        0.28713042, -0.0018292 ,  0.05259135, -0.04950036, -0.03081701,\n",
       "        0.13132755, -0.00866911,  0.00691195,  0.30406228,  0.1815319 ,\n",
       "       -0.05478504, -0.39295363,  0.29229477,  0.2720397 ,  0.01184659,\n",
       "        0.02325401,  0.02535062, -0.2110338 , -0.4548862 ,  0.10003702,\n",
       "        0.2665903 , -0.12584901, -0.03635841, -0.13040446, -0.10384749,\n",
       "       -0.35109165, -0.04137954,  0.20201625,  0.0872402 , -0.2208775 ,\n",
       "        0.2537538 ,  0.08034121,  0.00219653, -0.14620847, -0.16163576,\n",
       "        0.12694114, -0.01651168, -0.11299416, -0.06234654,  0.1573906 ,\n",
       "       -0.2058823 , -0.09686708, -0.22730932, -0.10299324, -0.02207511,\n",
       "        0.17049968, -0.41713923,  0.13382415, -0.09988096, -0.35682505,\n",
       "        0.49678072, -0.00604047, -0.09916737,  0.2835531 ,  0.27950612,\n",
       "        0.09212741,  0.12555405,  0.12955493,  0.05187815, -0.14201908,\n",
       "       -0.18415913, -0.4802362 , -0.02423341,  0.10908429, -0.04116749,\n",
       "       -0.20894691, -0.3023491 ,  0.47611585, -0.22304739, -0.41870922,\n",
       "       -0.03084051,  0.0298071 ,  0.21836448, -0.04544206, -0.242218  ,\n",
       "        0.07350282, -0.16437797, -0.05720804,  0.31027743,  0.26953536,\n",
       "        0.20620646,  0.04835131,  0.10145597, -0.2655033 ,  0.00589055,\n",
       "       -0.02689921,  0.05518554,  0.20959806, -0.21835086,  0.12024691,\n",
       "       -0.44547957,  0.0532173 , -0.23166235,  0.03323144,  0.13660906,\n",
       "       -0.3905801 ,  0.18340175,  0.01625851, -0.19765157,  0.14756788,\n",
       "       -0.06412784,  0.3466107 ,  0.31600937,  0.1333442 , -0.53255415,\n",
       "        0.26907745,  0.27234444, -0.1101026 , -0.11572064, -0.42586207,\n",
       "        0.21508802, -0.2338279 ,  0.07460733,  0.30356342,  0.09549583,\n",
       "       -0.30532312, -0.28579986,  0.27764246,  0.04027823, -0.09576365],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_vecs.get_word_vector('king')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see how correlated two words are in how close these numbers are for each embedding. For example we would stipulate that 'jeremy' and 'Jeremy' are more related than 'banana' and 'Jeremy'. Let's see if our embeddings think alike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.60866078],\n",
       "       [0.60866078, 1.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(ft_vecs.get_word_vector('jeremy'), ft_vecs.get_word_vector('Jeremy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.14482342],\n",
       "       [0.14482342, 1.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(ft_vecs.get_word_vector('banana'), ft_vecs.get_word_vector('Jeremy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map imagenet classes to word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will get all the words in the dictionary and sort them by their frequency (how often do they appear in the aforementioned datasets). We will then count how many words do we have in our dictionary as another step in the 'discovery phase' of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_words = ft_vecs.get_words(include_freq=True)\n",
    "ft_word_dict = {k:v for k,v in zip(*ft_words)}\n",
    "ft_words = sorted(ft_word_dict.keys(), key=lambda x: ft_word_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download the names of our 1000 imagenet classes so that we can assign each class in our imagenet dataset to a 300-long embedding (for that we need the actual word-id for each class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_FN = 'imagenet_class_index.json'\n",
    "download_url(f'http://files.fast.ai/models/{CLASSES_FN}', TMP_PATH/CLASSES_FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also download all the nouns in English from WORDNET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_FN = 'classids.txt'\n",
    "download_url(f'http://files.fast.ai/data/{WORDS_FN}', PATH/WORDS_FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will build a dictionary that maps our classes to the word-id for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict = json.load((TMP_PATH/CLASSES_FN).open())\n",
    "classids_1k = dict(class_dict.values())\n",
    "nclass = len(class_dict); nclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check that our class-id assignments are made correctly. Here we can see our two worlds:\n",
    "\n",
    "1. Imagenet and its class to id mapping\n",
    "2. WORDNET and its class to id mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n00001740 entity\\n',\n",
       " 'n00001930 physical_entity\\n',\n",
       " 'n00002137 abstraction\\n',\n",
       " 'n00002452 thing\\n',\n",
       " 'n00002684 object\\n']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classid_lines = (PATH/WORDS_FN).open().readlines()\n",
    "classid_lines[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the nouns in the English language and the Imagenet class ids, we need to connect each of these with the words in fastText. We will do this by creating a dictionary of synset to word vectors for both our WORDNET and Imagenet lists that __will only keep the words that are present in both datasets__ (i.e. both WORDNET and fastText for *syn_wv* and both Imagenet and fastText for *syn_wv_1k*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classids = dict(l.strip().split() for l in classid_lines)\n",
    "len(classids),len(classids_1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_vec_d = {w.lower(): ft_vecs.get_word_vector(w) for w in ft_words[-1000000:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fe7ccf6ebc30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m syn_wv = [(k, lc_vec_d[v.lower()]) for k,v in classids.items()\n\u001b[0m\u001b[1;32m      2\u001b[0m           if v.lower() in lc_vec_d]\n\u001b[1;32m      3\u001b[0m syn_wv_1k = [(k, lc_vec_d[v.lower()]) for k,v in classids_1k.items()\n\u001b[1;32m      4\u001b[0m           if v.lower() in lc_vec_d]\n\u001b[1;32m      5\u001b[0m \u001b[0msyn2wv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyn_wv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classids' is not defined"
     ]
    }
   ],
   "source": [
    "syn_wv = [(k, lc_vec_d[v.lower()]) for k,v in classids.items()\n",
    "          if v.lower() in lc_vec_d]\n",
    "syn_wv_1k = [(k, lc_vec_d[v.lower()]) for k,v in classids_1k.items()\n",
    "          if v.lower() in lc_vec_d]\n",
    "syn2wv = dict(syn_wv)\n",
    "len(syn2wv), len(syn_wv_1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'syn2wv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-46ea570c57a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyn2wv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTMP_PATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'syn2wv.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyn_wv_1k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTMP_PATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'syn_wv_1k.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'syn2wv' is not defined"
     ]
    }
   ],
   "source": [
    "pickle.dump(syn2wv, (TMP_PATH/'syn2wv.pkl').open('wb'))\n",
    "pickle.dump(syn_wv_1k, (TMP_PATH/'syn_wv_1k.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn2wv = pickle.load((TMP_PATH/'syn2wv.pkl').open('rb'))\n",
    "syn_wv_1k = pickle.load((TMP_PATH/'syn_wv_1k.pkl').open('rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is building the data we are going to train our model on. For that we are only including images with ids that are English nouns. Our _x_ variables will be our images (which we are saving in a PosixPath format) and our *y* variables will be our vectors (300 floats, one for each embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "img_vecs = []\n",
    "images_val = []\n",
    "img_vecs_val = []\n",
    "\n",
    "for d in (PATH/'train').iterdir():\n",
    "    if d.name not in syn2wv: continue\n",
    "    vec = syn2wv[d.name]\n",
    "    for f in d.iterdir():\n",
    "        images.append(str(f.relative_to(PATH)))\n",
    "        img_vecs.append(vec)\n",
    "\n",
    "n_val=0\n",
    "for d in (PATH/'valid').iterdir():\n",
    "    if d.name not in syn2wv: continue\n",
    "    vec = syn2wv[d.name]\n",
    "    for f in d.iterdir():\n",
    "        images_val.append(str(f.relative_to(PATH)))\n",
    "        img_vecs_val.append(vec)\n",
    "        n_val += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28700"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(739526, 300)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_vecs = np.stack(img_vecs)\n",
    "img_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(images, (TMP_PATH/'images.pkl').open('wb'))\n",
    "pickle.dump(img_vecs, (TMP_PATH/'img_vecs.pkl').open('wb'))\n",
    "pickle.dump(images_val, (TMP_PATH/'images_val.pkl').open('wb'))\n",
    "pickle.dump(img_vecs_val, (TMP_PATH/'img_vecs)val.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pickle.load((TMP_PATH/'images.pkl').open('rb'))\n",
    "img_vecs = pickle.load((TMP_PATH/'img_vecs.pkl').open('rb'))\n",
    "images_val = pickle.load((TMP_PATH/'images_val.pkl').open('rb'))\n",
    "img_vecs_val = pickle.load((TMP_PATH/'img_vecs_val.pkl').open('rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our dataset and create our DataBunch object. Note that we will need to tell our model how many classes we have. We will specify this manually since our ImageDataset class does not support it natively (this argument will then be passed to our model). We will resize our pictures to a 224x224 size and normalize them. Finally we will check that our data looks as we would like it to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = (PATH/\"\").absolute()\n",
    "images = [folder_path/image for image in images]\n",
    "images_val = [folder_path/image_val for image_val in images_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "739526"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_val[0]\n",
    "len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImageDataset(images, img_vecs)\n",
    "valid_ds = ImageDataset(images_val, img_vecs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.classes = range(300)\n",
    "valid_ds.classes = range(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = ([flip_lr()], [crop_pad(size=224)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch.create(train_ds, valid_ds, path=PATH, device=torch.device('cuda'), ds_tfms = get_transforms(), tfms=imagenet_norm, size=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 3, 224, 224]), torch.Size([128, 300]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = next(iter(data.valid_dl))\n",
    "x.size(),y.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to train our model. Our model will try to predict the value of each embedding for each of our images. To accomplish this we will add a fully connected layer at the end of our resnet50 architecture (with 300 output neurons) and precompute the activations of the backbone model so as to save training time. We will also initialize the weights of the backbone model with the weights of the pretrained model. Given that the pretrained model and ours are both training in the same dataset we will not need to do any finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner(data, tvm.resnet50, lin_ftrs=[1024], ps=[0.2,0.2], pretrained=True, callback_fns=BnFreeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.opt_fn = partial(AdamW, betas=(0.9,0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_loss(inp,targ): return 1 - F.cosine_similarity(inp,targ).mean()\n",
    "learn.loss_fn = cos_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2c3b81bd394825910aafe798971723",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, max=1), HTML(value='0.00% [0/1 00:00<00:00]'))), HTML(valueâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-504d94160927>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/fastai/fastai/train.py\u001b[0m in \u001b[0;36mlr_find\u001b[0;34m(learn, start_lr, end_lr, num_it, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLRFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_it\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_fp16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_scale\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_master\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         fit(epochs, self.model, self.loss_fn, opt=self.opt, data=self.data, metrics=self.metrics,\n\u001b[0;32m--> 133\u001b[0;31m             callbacks=self.callbacks+callbacks)\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, model, loss_fn, opt, data, callbacks, metrics)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_update\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader timed out after {} seconds'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn.lr_find(start_lr=1e-4, end_lr=1e15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "wd = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = np.array([lr/1000,lr/100,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "295995b124de42c181cdcbaffbf9b7d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, max=3), HTML(value='0.00% [0/3 00:00<00:00]'))), HTML(valueâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(3, lr, wd=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.precompute=False\n",
    "learn.freeze_to(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('pre0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('pre0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search imagenet classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns, wvs = list(zip(*syn_wv_1k))\n",
    "wvs = np.array(wvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.4 s, sys: 7.91 s, total: 26.3 s\n",
      "Wall time: 7.17 s\n"
     ]
    }
   ],
   "source": [
    "%time pred_wv = learn.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denorm = md.val_ds.denorm\n",
    "\n",
    "def show_img(im, figsize=None, ax=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im)\n",
    "    ax.axis('off')\n",
    "    return ax\n",
    "\n",
    "def show_imgs(ims, cols, figsize=None):\n",
    "    fig,axes = plt.subplots(len(ims)//cols, cols, figsize=figsize)\n",
    "    for i,ax in enumerate(axes.flat): show_img(ims[i], ax=ax)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_imgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e5f8ae797ce9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'show_imgs' is not defined"
     ]
    }
   ],
   "source": [
    "show_imgs(denorm(md.val_ds[start:start+25][0]), 5, (10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib\n",
    "\n",
    "def create_index(a):\n",
    "    index = nmslib.init(space='angulardist')\n",
    "    index.addDataPointBatch(a)\n",
    "    index.createIndex()\n",
    "    return index\n",
    "\n",
    "def get_knns(index, vecs):\n",
    "     return zip(*index.knnQueryBatch(vecs, k=10, num_threads=4))\n",
    "\n",
    "def get_knn(index, vec): return index.knnQuery(vec, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_wvs = create_index(wvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs,dists = get_knns(nn_wvs, pred_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['spoonbill', 'bustard', 'oystercatcher'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[classids[syns[id]] for id in ids[:3]] for ids in idxs[start:start+10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search all wordnet noun classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_syns, all_wvs = list(zip(*syn2wv.items()))\n",
    "all_wvs = np.array(all_wvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_allwvs = create_index(all_wvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs,dists = get_knns(nn_allwvs, pred_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['spoonbill', 'bustard', 'oystercatcher'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[classids[all_syns[id]] for id in ids[:3]] for ids in idxs[start:start+10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text -> image search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_predwv = create_index(pred_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecd = pickle.load(open(TRANS_PATH/'wiki.en.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = en_vecd['boat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-89391ccab9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_predwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopen_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_knn' is not defined"
     ]
    }
   ],
   "source": [
    "idxs,dists = get_knn(nn_predwv, vec)\n",
    "show_imgs([open_image(PATH/md.val_ds.fnames[i]) for i in idxs[:3]], 3, figsize=(9,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = (en_vecd['engine'] + en_vecd['boat'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-89391ccab9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_predwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopen_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_knn' is not defined"
     ]
    }
   ],
   "source": [
    "idxs,dists = get_knn(nn_predwv, vec)\n",
    "show_imgs([open_image(PATH/md.val_ds.fnames[i]) for i in idxs[:3]], 3, figsize=(9,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = (en_vecd['sail'] + en_vecd['boat'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-89391ccab9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_predwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopen_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_knn' is not defined"
     ]
    }
   ],
   "source": [
    "idxs,dists = get_knn(nn_predwv, vec)\n",
    "show_imgs([open_image(PATH/md.val_ds.fnames[i]) for i in idxs[:3]], 3, figsize=(9,3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image->image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'valid/n01440764/ILSVRC2012_val_00007197.JPEG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = open_image(PATH/fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c9b680a767fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'show_img' is not defined"
     ]
    }
   ],
   "source": [
    "show_img(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_img = md.val_ds.transform(img)\n",
    "pred = learn.predict_array(t_img[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cf37ec4574c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_predwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopen_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_knn' is not defined"
     ]
    }
   ],
   "source": [
    "idxs,dists = get_knn(nn_predwv, pred)\n",
    "show_imgs([open_image(PATH/md.val_ds.fnames[i]) for i in idxs[1:4]], 3, figsize=(9,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
