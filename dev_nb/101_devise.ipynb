{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.docs import *\n",
    "from fastai.text import *\n",
    "torch.backends.cudnn.benchmark=True\n",
    "import json\n",
    "\n",
    "import fastText as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeVise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this class, we will implement the [DeVise paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41473.pdf). What makes this paper specially interesting is that it combines image classification and text embeddings. The technique presented by the authors leverages word embeddings to assign several possible tags to each image. By doing this, the model fares considerably well (achieving up to 18% hit rates) in never seen before categories (zero-shot learning). But how can the model classify objects it has never seen before? That is the power of word embeddings.\n",
    "\n",
    "Basically, the model will use the 'closeness' of several words it knows through the embeddings to classify a new image. Perhaps this is easiest explained through a human example. When we are teaching a toddler what a motorcycle is, for example, we might say \"Well, it is a bicycle but it goes faster\". That is we relate it to what *he/she already knows*. In the same way, if the model sees a trout, it might say \"Well, I know it is very similar to a trench and I know what a trench is so I will say it is either a trench or something very similar, like a sea bass or a trout\". In 2D these relationships would look like this:\n",
    "\n",
    "![clusters](imgs/clusters.png)\n",
    "Frome et al., 2013\n",
    "\n",
    "Please consider that while you may say \"Obviously, a goldfish has to do more with a shark than with an iguana because they are both aquatic\" you are comparing these across one dimension, namely natural habitat, while if you compared them by size the results would be different. These infinite dimensions across which you can compare two words are resumed into a finite number of categories which is what we call embeddings. In this image, we are arbitrarily choosing one dimension to make the point since it is intuitive to us human beings.\n",
    "\n",
    "To create this network the authors combined a computer vision architecture with the embeddings data to create a hybrid model that we can see in the following picture:\n",
    "\n",
    "![devise_arch](imgs/devise_arch.png)\n",
    "Frome et al., 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('../data/imagenet')\n",
    "TMP_PATH = Path('../data/imagenet/tmp')\n",
    "TRANS_PATH = Path('../data/translate/')\n",
    "PATH_TRN = PATH/'train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to load our word vectors. We'll see that each word has a normalized number between [-1, 1] for each of the 300 embeddings. This is effectively a 300 dimension representation of the meaning of each word. As an example, let's see the embedding for 'king'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-3ab9b4c448c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mft_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRANS_PATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'wiki.en.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastText/FastText.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;34m\"\"\"Load a model given a filepath and return a model object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_FastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastText/FastText.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfasttext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mis_quantized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ft_vecs = ft.load_model(str((TRANS_PATH/'wiki.en.bin')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_vecs.get_word_vector('king')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see how correlated two words are in how close these numbers are for each embedding. For example we would stipulate that 'jeremy' and 'Jeremy' are more related than 'banana' and 'Jeremy'. Let's see if our embeddings think alike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.60866078],\n",
       "       [0.60866078, 1.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(ft_vecs.get_word_vector('jeremy'), ft_vecs.get_word_vector('Jeremy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.14482342],\n",
       "       [0.14482342, 1.        ]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(ft_vecs.get_word_vector('banana'), ft_vecs.get_word_vector('Jeremy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map imagenet classes to word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will get all the words in the dictionary and sort them by their frequency (how often do they appear in the aforementioned datasets). We will then count how many words do we have in our dictionary as another step in the 'discovery phase' of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_words = ft_vecs.get_words(include_freq=True)\n",
    "ft_word_dict = {k:v for k,v in zip(*ft_words)}\n",
    "ft_words = sorted(ft_word_dict.keys(), key=lambda x: ft_word_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will download the names of our 1000 imagenet classes so that we can assign each class in our imagenet dataset to a 300-long embedding (for that we need the actual word-id for each class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_FN = 'imagenet_class_index.json'\n",
    "download_url(f'http://files.fast.ai/models/{CLASSES_FN}', TMP_PATH/CLASSES_FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also download all the nouns in English from WORDNET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORDS_FN = 'classids.txt'\n",
    "download_url(f'http://files.fast.ai/data/{WORDS_FN}', PATH/WORDS_FN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will build a dictionary that maps our classes to the word-id for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_dict = json.load((TMP_PATH/CLASSES_FN).open())\n",
    "classids_1k = dict(class_dict.values())\n",
    "nclass = len(class_dict); nclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check that our class-id assignments are made correctly. Here we can see our two worlds:\n",
    "\n",
    "1. Imagenet and its class to id mapping\n",
    "2. WORDNET and its class to id mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['n00001740 entity\\n',\n",
       " 'n00001930 physical_entity\\n',\n",
       " 'n00002137 abstraction\\n',\n",
       " 'n00002452 thing\\n',\n",
       " 'n00002684 object\\n']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classid_lines = (PATH/WORDS_FN).open().readlines()\n",
    "classid_lines[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the nouns in the English language and the Imagenet class ids, we need to connect each of these with the words in fastText. We will do this by creating a dictionary of synset to word vectors for both our WORDNET and Imagenet lists that __will only keep the words that are present in both datasets__ (i.e. both WORDNET and fastText for *syn_wv* and both Imagenet and fastText for *syn_wv_1k*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classids = dict(l.strip().split() for l in classid_lines)\n",
    "len(classids),len(classids_1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_vec_d = {w.lower(): ft_vecs.get_word_vector(w) for w in ft_words[-1000000:]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-fe7ccf6ebc30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m syn_wv = [(k, lc_vec_d[v.lower()]) for k,v in classids.items()\n\u001b[0m\u001b[1;32m      2\u001b[0m           if v.lower() in lc_vec_d]\n\u001b[1;32m      3\u001b[0m syn_wv_1k = [(k, lc_vec_d[v.lower()]) for k,v in classids_1k.items()\n\u001b[1;32m      4\u001b[0m           if v.lower() in lc_vec_d]\n\u001b[1;32m      5\u001b[0m \u001b[0msyn2wv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyn_wv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classids' is not defined"
     ]
    }
   ],
   "source": [
    "syn_wv = [(k, lc_vec_d[v.lower()]) for k,v in classids.items()\n",
    "          if v.lower() in lc_vec_d]\n",
    "syn_wv_1k = [(k, lc_vec_d[v.lower()]) for k,v in classids_1k.items()\n",
    "          if v.lower() in lc_vec_d]\n",
    "syn2wv = dict(syn_wv)\n",
    "len(syn2wv), len(syn_wv_1k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'syn2wv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-46ea570c57a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyn2wv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTMP_PATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'syn2wv.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyn_wv_1k\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTMP_PATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m'syn_wv_1k.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'syn2wv' is not defined"
     ]
    }
   ],
   "source": [
    "pickle.dump(syn2wv, (TMP_PATH/'syn2wv.pkl').open('wb'))\n",
    "pickle.dump(syn_wv_1k, (TMP_PATH/'syn_wv_1k.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn2wv = pickle.load((TMP_PATH/'syn2wv.pkl').open('rb'))\n",
    "syn_wv_1k = pickle.load((TMP_PATH/'syn_wv_1k.pkl').open('rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is building the data we are going to train our model on. For that we are only including images with ids that are English nouns. Our _x_ variables will be our images (which we are saving in a PosixPath format) and our *y* variables will be our vectors (300 floats, one for each embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = []\n",
    "img_vecs = []\n",
    "images_val = []\n",
    "img_vecs_val = []\n",
    "\n",
    "for d in (PATH/'train').iterdir():\n",
    "    if d.name not in syn2wv: continue\n",
    "    vec = syn2wv[d.name]\n",
    "    for f in d.iterdir():\n",
    "        images.append(str(f.relative_to(PATH)))\n",
    "        img_vecs.append(vec)\n",
    "\n",
    "n_val=0\n",
    "for d in (PATH/'valid').iterdir():\n",
    "    if d.name not in syn2wv: continue\n",
    "    vec = syn2wv[d.name]\n",
    "    for f in d.iterdir():\n",
    "        images_val.append(str(f.relative_to(PATH)))\n",
    "        img_vecs_val.append(vec)\n",
    "        n_val += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28700"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(739526, 300)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_vecs = np.stack(img_vecs)\n",
    "img_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(images, (TMP_PATH/'images.pkl').open('wb'))\n",
    "pickle.dump(img_vecs, (TMP_PATH/'img_vecs.pkl').open('wb'))\n",
    "pickle.dump(images_val, (TMP_PATH/'images_val.pkl').open('wb'))\n",
    "pickle.dump(img_vecs_val, (TMP_PATH/'img_vecs)val.pkl').open('wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pickle.load((TMP_PATH/'images.pkl').open('rb'))\n",
    "img_vecs = pickle.load((TMP_PATH/'img_vecs.pkl').open('rb'))\n",
    "images_val = pickle.load((TMP_PATH/'images_val.pkl').open('rb'))\n",
    "img_vecs_val = pickle.load((TMP_PATH/'img_vecs_val.pkl').open('rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our dataset and create our DataBunch object. Note that we will need to tell our model how many classes we have. We will specify this manually since our ImageDataset class does not support it natively (this argument will then be passed to our model). We will resize our pictures to a 224x224 size and normalize them. Finally we will check that our data looks as we would like it to be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = (PATH/\"\").absolute()\n",
    "images = [folder_path/image for image in images]\n",
    "images_val = [folder_path/image_val for image_val in images_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "739526"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(images); n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28700"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_val = len(images_val); n_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768226"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n+n_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ImageDataset(images, img_vecs)\n",
    "valid_ds = ImageDataset(images_val, img_vecs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.classes = range(300)\n",
    "valid_ds.classes = range(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [[flip_lr(), crop_pad(size=224)], [flip_lr(), crop_pad(size=224)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataBunch.create(train_ds, valid_ds, path=PATH, device=torch.device('cuda'), ds_tfms = tfms, tfms=imagenet_norm, size=224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??get_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11556"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 3, 224, 224]), torch.Size([64, 300]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = next(iter(data.train_dl))\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-9.8158e-01, -1.1368e+00, -1.6751e+00,  ..., -4.6912e-01,\n",
       "           -6.1763e-01, -6.3541e-01],\n",
       "          [-1.1836e+00, -1.2096e+00, -1.5905e+00,  ..., -1.0800e+00,\n",
       "           -1.2846e+00, -1.3173e+00],\n",
       "          [-1.3124e+00, -1.2135e+00, -1.5620e+00,  ..., -3.1017e-01,\n",
       "           -5.1102e-01, -6.0622e-01],\n",
       "          ...,\n",
       "          [ 6.3395e-01,  6.3395e-01,  6.5108e-01,  ..., -6.0146e-01,\n",
       "           -5.8521e-01, -4.5680e-01],\n",
       "          [ 6.3918e-01,  6.5631e-01,  6.6237e-01,  ..., -7.0414e-01,\n",
       "           -6.1438e-01, -5.2898e-01],\n",
       "          [ 6.3843e-01,  6.3843e-01,  6.5556e-01,  ..., -8.5470e-01,\n",
       "           -6.4182e-01, -6.2282e-01]],\n",
       " \n",
       "         [[-9.0070e-01, -1.0373e+00, -1.5940e+00,  ..., -2.0173e-01,\n",
       "           -3.5355e-01, -3.7173e-01],\n",
       "          [-1.1331e+00, -1.1247e+00, -1.5140e+00,  ..., -8.5209e-01,\n",
       "           -1.0613e+00, -1.0947e+00],\n",
       "          [-1.2647e+00, -1.1286e+00, -1.4849e+00,  ..., -5.2921e-02,\n",
       "           -2.7041e-01, -3.6774e-01],\n",
       "          ...,\n",
       "          [ 1.0577e+00,  1.0577e+00,  1.0752e+00,  ..., -5.2043e-01,\n",
       "           -5.0382e-01, -3.7255e-01],\n",
       "          [ 1.0630e+00,  1.0805e+00,  1.0867e+00,  ..., -6.2541e-01,\n",
       "           -5.3364e-01, -4.4634e-01],\n",
       "          [ 1.0623e+00,  1.0623e+00,  1.0798e+00,  ..., -7.7933e-01,\n",
       "           -5.6170e-01, -5.4227e-01]],\n",
       " \n",
       "         [[-5.2779e-01, -6.9034e-01, -1.2473e+00,  ...,  3.1313e-01,\n",
       "            1.6198e-01,  1.4388e-01],\n",
       "          [-7.2477e-01, -7.5801e-01, -1.1805e+00,  ..., -3.4721e-01,\n",
       "           -5.5546e-01, -5.8879e-01],\n",
       "          [-8.7999e-01, -7.6192e-01, -1.1515e+00,  ...,  4.4235e-01,\n",
       "            2.3188e-01,  1.3499e-01],\n",
       "          ...,\n",
       "          [ 1.7284e+00,  1.7284e+00,  1.7458e+00,  ..., -2.7847e-01,\n",
       "           -2.6193e-01, -1.3124e-01],\n",
       "          [ 1.7337e+00,  1.7511e+00,  1.7573e+00,  ..., -3.8298e-01,\n",
       "           -2.9162e-01, -2.0470e-01],\n",
       "          [ 1.7329e+00,  1.7329e+00,  1.7504e+00,  ..., -5.3469e-01,\n",
       "           -3.1803e-01, -2.9869e-01]]], device='cuda:0'),\n",
       " tensor([ 0.3167, -0.0645, -0.3623,  0.1700, -0.4466,  0.1927,  0.1409, -0.0451,\n",
       "          0.4726,  0.4472,  0.3024, -0.1173, -0.2417, -0.3897,  0.0673,  0.3469,\n",
       "         -0.2313,  0.4819,  0.2103,  0.4354,  0.1030,  0.0453, -0.1540, -0.0962,\n",
       "         -0.2382,  0.0683, -0.0746, -0.1762,  0.0278,  0.3857,  0.1495,  0.4445,\n",
       "         -0.5214, -0.0303,  0.3831, -0.2394, -0.3135, -0.1793, -0.3322,  0.1825,\n",
       "         -0.2056, -0.0736,  0.1209,  0.1670,  0.0959,  0.0743, -0.2273, -0.2545,\n",
       "         -0.0265,  0.2586,  0.0521, -0.0628, -0.2349,  0.0576, -0.0088,  0.1566,\n",
       "          0.4126, -0.2985, -0.0082,  0.2531,  0.0597,  0.0155, -0.0171, -0.1294,\n",
       "          0.0341, -0.3156,  0.1639,  0.1323, -0.0586,  0.2314,  0.1898, -0.1778,\n",
       "          0.0363,  0.0020, -0.0977,  0.0953,  0.4902,  0.2215,  0.2060, -0.4008,\n",
       "         -0.3689,  0.2536, -0.3174, -0.1033, -0.4112, -0.3126,  0.1836, -0.1644,\n",
       "          0.3541,  0.2626, -0.1269, -0.4269,  0.1292,  0.2831,  0.4548,  0.1550,\n",
       "          0.0608, -0.0717, -0.1943, -0.0099,  0.1186,  0.1697, -0.1345, -0.1938,\n",
       "          0.1452,  0.0699,  0.0242, -0.4073,  0.2911, -0.1541, -0.0743, -0.3559,\n",
       "         -0.2164,  0.2531, -0.1802,  0.3876, -0.2984, -0.4867,  0.3601,  0.2273,\n",
       "          0.1144, -0.1603,  0.1223,  0.2374, -0.1199, -0.1231,  0.2667,  0.3411,\n",
       "          0.1368,  0.2470, -0.2594,  0.2431,  0.0948,  0.2917,  0.2059,  0.2149,\n",
       "         -0.1352, -0.2450, -0.6078,  0.2153,  0.0072,  0.0108,  0.3761,  0.4400,\n",
       "         -0.2017,  0.2619, -0.1606,  0.0363, -0.0304,  0.3390,  0.3325, -0.6247,\n",
       "         -0.2384, -0.2656,  0.0452,  0.2642, -0.4017,  0.2696,  0.4299,  0.3200,\n",
       "         -0.0114,  0.3259,  0.1959, -0.1410, -0.0803,  0.0801, -0.2056,  0.1512,\n",
       "          0.4426, -0.0152, -0.1632, -0.4189, -0.1438, -0.2735, -0.2591,  0.5583,\n",
       "         -0.1809, -0.0009, -0.2180, -0.3104,  0.2783, -0.0524,  0.3583, -0.1081,\n",
       "          0.1356, -0.0690, -0.2960, -0.3368,  0.0419,  0.2229,  0.0481, -0.6764,\n",
       "          0.1203,  0.4605, -0.2141,  0.1850,  0.1623,  0.2758, -0.2364,  0.0705,\n",
       "         -0.2124,  0.1535, -0.5251,  0.1271,  0.0973, -0.4923, -0.2488, -0.4250,\n",
       "         -0.0791, -0.0862,  0.2849, -0.0489, -0.3854, -0.0593, -0.2036, -0.1381,\n",
       "         -0.1265,  0.0245, -0.1602, -0.0907,  0.0548,  0.1338, -0.0164,  0.1786,\n",
       "          0.1182, -0.3282,  0.3686,  0.1096,  0.2931,  0.0272, -0.2740, -0.4218,\n",
       "         -0.0394, -0.3447,  0.1390, -0.0866, -0.0505,  0.0662, -0.3996, -0.0341,\n",
       "         -0.0915,  0.5011,  0.3365,  0.2282,  0.1168,  0.3469,  0.3158, -0.1236,\n",
       "         -0.3411,  0.4177,  0.1539, -0.5631,  0.0840, -0.1313,  0.2795, -0.1680,\n",
       "          0.1712, -0.0321,  0.0357, -0.0311,  0.2474,  0.3917,  0.1275,  0.1764,\n",
       "         -0.0353,  0.1927, -0.4254,  0.2914,  0.1996,  0.0987, -0.1081, -0.1856,\n",
       "         -0.1283, -0.3510, -0.1388, -0.2758, -0.1350,  0.0777,  0.0751,  0.6038,\n",
       "         -0.7754, -0.1821,  0.0403,  0.2340, -0.1381,  0.4003,  0.5271,  0.1708,\n",
       "         -0.3373, -0.2176,  0.2928, -0.1220,  0.1723, -0.0609, -0.5717,  0.0128,\n",
       "         -0.0943,  0.4617,  0.6086, -0.0412], device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0],y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 3, 224, 224]), torch.Size([128, 300]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val,y_val = next(iter(data.valid_dl))\n",
    "x_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-1.3776e-01, -7.8805e-01, -5.0191e-01,  ...,  1.3069e+00,\n",
       "            1.1611e+00,  9.1329e-01],\n",
       "          [-3.3070e-01, -6.6828e-01, -4.4326e-01,  ...,  1.3934e+00,\n",
       "            1.1664e+00,  1.0396e+00],\n",
       "          [-1.3471e-01, -7.3052e-01, -5.2672e-01,  ...,  1.3240e+00,\n",
       "            1.2627e+00,  1.0238e+00],\n",
       "          ...,\n",
       "          [ 7.7761e-01,  9.3214e-01,  1.5432e+00,  ..., -1.9640e+00,\n",
       "           -1.9799e+00, -1.9208e+00],\n",
       "          [ 1.8132e+00,  1.7025e+00,  1.6532e+00,  ..., -1.9595e+00,\n",
       "           -1.9500e+00, -1.9211e+00],\n",
       "          [ 1.5832e+00,  1.7062e+00,  1.6979e+00,  ..., -2.0236e+00,\n",
       "           -1.9645e+00, -1.9188e+00]],\n",
       " \n",
       "         [[-6.8987e-01, -8.6800e-01, -1.2450e-01,  ...,  1.8895e+00,\n",
       "            1.6079e+00,  1.4199e+00],\n",
       "          [-5.2701e-01, -6.3565e-01, -2.3500e-01,  ...,  1.3297e+00,\n",
       "            1.4961e+00,  1.4386e+00],\n",
       "          [-6.1235e-01, -6.3542e-01, -3.8813e-01,  ...,  1.5001e+00,\n",
       "            1.1147e+00,  1.2731e+00],\n",
       "          ...,\n",
       "          [ 1.8164e+00,  1.8238e+00,  1.7345e+00,  ..., -1.9077e+00,\n",
       "           -1.9288e+00, -1.9167e+00],\n",
       "          [ 1.8049e+00,  1.8318e+00,  1.7496e+00,  ..., -1.9000e+00,\n",
       "           -1.9062e+00, -1.9176e+00],\n",
       "          [ 1.7896e+00,  1.7831e+00,  1.7167e+00,  ..., -1.9307e+00,\n",
       "           -1.9048e+00, -1.9154e+00]],\n",
       " \n",
       "         [[-9.2205e-01, -9.6784e-01, -3.0830e-01,  ...,  2.3791e+00,\n",
       "            2.1639e+00,  1.7600e+00],\n",
       "          [-5.9039e-01, -7.5332e-01, -3.9602e-01,  ...,  1.9041e+00,\n",
       "            2.0134e+00,  1.9818e+00],\n",
       "          [-8.6562e-01, -7.5944e-01, -5.2563e-01,  ...,  1.9891e+00,\n",
       "            1.7018e+00,  1.6754e+00],\n",
       "          ...,\n",
       "          [ 2.0158e+00,  2.0701e+00,  1.9754e+00,  ..., -1.5869e+00,\n",
       "           -1.5916e+00, -1.5652e+00],\n",
       "          [ 2.0445e+00,  1.9600e+00,  1.9308e+00,  ..., -1.5803e+00,\n",
       "           -1.5776e+00, -1.5649e+00],\n",
       "          [ 1.9494e+00,  2.0717e+00,  1.8876e+00,  ..., -1.6272e+00,\n",
       "           -1.5870e+00, -1.5626e+00]]], device='cuda:0'),\n",
       " tensor([-0.1107, -0.1736, -0.0038,  0.1746, -0.1236, -0.6549, -0.4744,  0.0325,\n",
       "         -0.1336, -0.1156,  0.3649,  0.1495,  0.0000, -0.1709,  0.0529, -0.2853,\n",
       "          0.0005, -0.2351,  0.1205,  0.4488,  0.0577,  0.0125, -0.1883,  0.3893,\n",
       "         -0.3382, -0.0121, -0.1885,  0.1367, -0.3678, -0.2675, -0.0680,  0.1211,\n",
       "          0.2964, -0.3075, -0.1781,  0.1018,  0.1483, -0.4029, -0.0696, -0.2488,\n",
       "          0.1950, -0.2511,  0.0481, -0.5417,  0.3134,  0.3978, -0.4520, -0.1492,\n",
       "          0.0337,  0.1724,  0.2291, -0.4183, -0.1300,  0.1841, -0.3208,  0.1746,\n",
       "          0.1086,  0.2374,  0.2398,  0.0381,  0.1380,  0.1076,  0.1059, -0.0558,\n",
       "          0.1319, -0.0802, -0.3633,  0.4147, -0.0073,  0.0412, -0.3440,  0.4359,\n",
       "          0.2078,  0.2500, -0.1417,  0.1798,  0.2826,  0.2681, -0.3813,  0.3117,\n",
       "          0.1951, -0.0035,  0.0837,  0.0794, -0.6255,  0.1746,  0.1969,  0.1456,\n",
       "         -0.0442, -0.4004,  0.2379, -0.8081,  0.3013,  0.0176,  0.7336,  0.3967,\n",
       "          0.1042,  0.0669,  0.3179, -0.4758, -0.4091, -0.0522, -0.0303, -0.1305,\n",
       "          0.4228, -0.4081, -0.2787, -0.2209,  0.1372, -0.0616,  0.3865, -0.3246,\n",
       "         -0.0741, -0.2632, -0.1524, -0.4661,  0.5138, -0.2323, -0.0428,  0.1883,\n",
       "          0.4687,  0.0790, -0.0839,  0.0191,  0.3439, -0.0479,  0.3104,  0.2263,\n",
       "          0.3377, -0.0280,  0.0824,  0.3350,  0.3441,  0.2985, -0.1171, -0.3464,\n",
       "         -0.1736,  0.1693, -0.3295,  0.4194,  0.0272,  0.2012,  0.0704, -0.2150,\n",
       "          0.2046,  0.1711, -0.0903, -0.1310, -0.0303, -0.2329,  0.4892, -0.1471,\n",
       "          0.4557, -0.3659,  0.1088, -0.2778,  0.1362, -0.6304, -0.1215, -0.0513,\n",
       "         -0.2367,  0.2678, -0.0198,  0.0862,  0.4089, -0.2013, -0.2807,  0.3421,\n",
       "         -0.1447, -0.0812, -0.3954, -0.1479,  0.0423, -0.1071, -0.1468,  0.0336,\n",
       "          0.3522, -0.4780,  0.7516, -0.3967, -0.0084, -0.3015,  0.2172, -0.4034,\n",
       "          0.0402,  0.0040,  0.0507, -0.4561,  0.1260, -0.1577, -0.2075, -0.1206,\n",
       "          0.8166, -0.1404, -0.2658, -0.2872,  0.3887,  0.0874,  0.0820, -0.4444,\n",
       "         -0.1577,  0.4209, -0.0257, -0.0132, -0.5655, -0.6516, -0.1308, -0.0445,\n",
       "          0.4619, -0.3763,  0.6371, -0.1277, -0.0968,  0.2222,  0.3628, -0.0475,\n",
       "         -0.3104, -0.5644,  0.2348,  0.1393, -0.3068,  0.2321, -0.2645,  0.1351,\n",
       "          0.1317, -0.2562, -0.0402, -0.0553, -0.5122, -0.3892, -0.1094,  0.2163,\n",
       "         -0.1776, -0.0040,  0.1176,  0.2409,  0.0443,  0.4262,  0.1839,  0.3218,\n",
       "         -0.0901,  0.4004, -0.0532, -0.0162, -0.1642, -0.2236,  0.2274, -0.3333,\n",
       "          0.3400, -0.4743, -0.1589, -0.6112, -0.1350, -0.1791,  0.2093,  0.5346,\n",
       "          0.1264, -0.1933,  0.0231, -0.5316, -0.4851,  0.1830,  0.0135, -0.1543,\n",
       "         -0.4501,  0.1720, -0.0021,  0.0727,  0.0794,  0.2340, -0.2595,  0.1175,\n",
       "          0.3421, -0.3541, -0.1368, -0.5671,  0.1436,  0.8282, -0.0520,  0.1456,\n",
       "          0.2538, -0.0575,  0.1675,  0.0153,  0.7640,  0.1083, -0.2041, -0.4602,\n",
       "          0.2193,  0.0354,  0.2436, -0.1990, -0.4602,  0.0043,  0.0088,  0.1235,\n",
       "         -0.0810,  0.6075,  0.1032,  0.0233], device='cuda:0'))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val[0],y_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2612\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2613\u001b[0;31m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2614\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-c24a7b9ad46d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2613\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2614\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2615\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2616\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "Image.open(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to train our model. Our model will try to predict the value of each embedding for each of our images. To accomplish this we will add a fully connected layer at the end of our resnet50 architecture (with 300 output neurons) and precompute the activations of the backbone model so as to save training time. We will also initialize the weights of the backbone model with the weights of the pretrained model. Given that the pretrained model and ours are both training in the same dataset we will not need to do any finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we are going to start by precomputing our activations for the convolutional backbone and we will then train our head from these activations (so our model will not have to calculate them again for each epoch). Since we already have a pretrained backbone, we will just need to run one forward pass to compute the final activations; that is we will not need any optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bcolz, threading\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??ConvLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner(data, tvm.resnet50, ps=[0.2,0.2], lin_ftrs=[1024], pretrained=True, callback_fns=BnFreeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = learn.model[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = list(body.children())\n",
    "layers += [AdaptiveConcatPool2d(), Flatten()]   \n",
    "body = nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveConcatPool2d(\n",
       "    (ap): AdaptiveAvgPool2d(output_size=1)\n",
       "    (mp): AdaptiveMaxPool2d(output_size=1)\n",
       "  )\n",
       "  (9): Lambda()\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = num_features(body)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.opt_fn = partial(AdamW, betas=(0.9,0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_loss(inp,targ): return 1 - F.cosine_similarity(inp,targ).mean()\n",
    "learn.loss_fn = cos_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activations(path, model_name, tmp_path, nf, force=False): \n",
    "    tmpl = f'_{model_name}.bc'\n",
    "    names = [os.path.join(path/'tmp', p+tmpl) for p in ('x_act', 'x_act_val')]\n",
    "    if os.path.exists(names[0]) and not force:\n",
    "        activations = [bcolz.open(p) for p in names]\n",
    "    else:\n",
    "        activations = [create_empty_bcolz(nf,p) for p in names]\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_empty_bcolz(n, name):\n",
    "    return bcolz.carray(np.zeros((0,n), np.float32), chunklen=1, mode='w', rootdir=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_to_bcolz(m, gen, arr, workers=4):\n",
    "    arr.trim(len(arr))\n",
    "    lock=threading.Lock()\n",
    "    m.eval()\n",
    "    for x,*_ in tqdm(gen):\n",
    "        y = to_np(m(x.data).detach())\n",
    "        with lock:\n",
    "            arr.append(y)\n",
    "    arr.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fc1(data, model, path, model_name, tmp_path, nf):\n",
    "    act, val_act = get_activations(path, model_name, tmp_path, nf)\n",
    "    m=model\n",
    "    if len(act)!=len(data.train_ds):\n",
    "        predict_to_bcolz(m, data.train_dl, act)\n",
    "    if len(val_act)!=len(data.valid_ds):\n",
    "        predict_to_bcolz(m, data.valid_dl, val_act)\n",
    "    \n",
    "    fc_data = FCDataset(act, img_vecs)\n",
    "    fc_data_val = FCDataset(val_act, img_vecs_val)\n",
    "    \n",
    "    fc_data.classes = data.classes\n",
    "    fc_data_val.classes = data.classes\n",
    "    \n",
    "    fc_db = DataBunch.create(fc_data, fc_data_val, path=PATH, device=torch.device('cuda'), bs=128)\n",
    "    return fc_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_features(m:Model)->int:\n",
    "    \"Return the number of output features for a `model`.\"\n",
    "    for l in reversed(flatten_model(m)):\n",
    "        if hasattr(l, 'num_features'): \n",
    "            return l.num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|       | 2774/11556 [19:58<1:03:29,  2.31it/s]/home/francisco/anaconda3/envs/fastai/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:754: UserWarning: Possibly corrupt EXIF data.  Expecting to read 2555904 bytes but only got 0. Skipping tag 0\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      " 30%|       | 3484/11556 [25:07<57:14,  2.35it/s]  /home/francisco/anaconda3/envs/fastai/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:754: UserWarning: Possibly corrupt EXIF data.  Expecting to read 1835008 bytes but only got 0. Skipping tag 0\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      " 48%|     | 5520/11556 [39:58<43:33,  2.31it/s]  /home/francisco/anaconda3/envs/fastai/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:754: UserWarning: Possibly corrupt EXIF data.  Expecting to read 2555904 bytes but only got 0. Skipping tag 0\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      " 71%|   | 8161/11556 [59:15<24:41,  2.29it/s]  /home/francisco/anaconda3/envs/fastai/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:754: UserWarning: Possibly corrupt EXIF data.  Expecting to read 2555904 bytes but only got 0. Skipping tag 0\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      " 74%|  | 8583/11556 [1:02:23<21:17,  2.33it/s]/home/francisco/anaconda3/envs/fastai/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:754: UserWarning: Possibly corrupt EXIF data.  Expecting to read 2555904 bytes but only got 0. Skipping tag 0\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      " 85%| | 9833/11556 [1:11:35<12:13,  2.35it/s]/home/francisco/anaconda3/envs/fastai/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:771: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 97%|| 11267/11556 [1:21:59<02:03,  2.33it/s]/home/francisco/anaconda3/envs/fastai/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:754: UserWarning: Possibly corrupt EXIF data.  Expecting to read 19660800 bytes but only got 0. Skipping tag 0\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/francisco/anaconda3/envs/fastai/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:754: UserWarning: Possibly corrupt EXIF data.  Expecting to read 18481152 bytes but only got 0. Skipping tag 0\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/francisco/anaconda3/envs/fastai/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:754: UserWarning: Possibly corrupt EXIF data.  Expecting to read 37093376 bytes but only got 0. Skipping tag 0\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/francisco/anaconda3/envs/fastai/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:754: UserWarning: Possibly corrupt EXIF data.  Expecting to read 39976960 bytes but only got 0. Skipping tag 0\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/francisco/anaconda3/envs/fastai/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:754: UserWarning: Possibly corrupt EXIF data.  Expecting to read 34865152 bytes but only got 0. Skipping tag 0\n",
      "  \" Skipping tag %s\" % (size, len(data), tag))\n",
      "/home/francisco/anaconda3/envs/fastai/lib/python3.6/site-packages/PIL/TiffImagePlugin.py:771: UserWarning: Corrupt EXIF data.  Expecting to read 12 bytes but only got 10. \n",
      "  warnings.warn(str(msg))\n",
      "100%|| 11556/11556 [1:24:07<00:00,  1.10it/s]\n",
      "100%|| 225/225 [03:23<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "fc_db = save_fc1(data, body, learn.path, 'resnet50', TMP_PATH, nf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([256, 4096]) torch.Size([256, 300])\n",
      "torch.Size([28, 4096]) torch.Size([28, 300])\n"
     ]
    }
   ],
   "source": [
    "for x,y in iter(fc_db.valid_dl):\n",
    "        print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to train our custom head from our computed activations. Notice that we are training our head **as part of the original custom ConvNet** and not as a separate sequential object. This is important because it means that we can load our pretrained model into our backbone like we did before, train our head only with the precomputed activations and then train the whole network **without having to join our two parts** (they are trained separately but are still connected in our memory). In summary, once we have loaded our pretrained weights on our backbone and trained our head we can directly train our whole network with differential learning rates without having to do any adjustment. Cool huh?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head = learn.model[1][2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (2): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): Dropout(p=0.2)\n",
       "  (4): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "  (5): ReLU(inplace)\n",
       "  (6): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): Dropout(p=0.2)\n",
       "  (8): Linear(in_features=1024, out_features=300, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_head = Learner(data=fc_db, model=head, opt_fn = partial(AdamW, betas=(0.9,0.99)), loss_fn = cos_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f6fac003d15457fa0c9aee466c68d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, max=1), HTML(value='0.00% [0/1 00:00<00:00]'))), HTML(value"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_head.lr_find(start_lr=1e-4, end_lr=1e15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn_head.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "wd = 1e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af3171aa054f4b138a22ea02dbe94128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, max=2), HTML(value='0.00% [0/2 00:00<00:00]'))), HTML(value"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-5ceb741f2348>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlearn_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_one_cycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiv_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpct_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/fastai/fastai/train.py\u001b[0m in \u001b[0;36mfit_one_cycle\u001b[0;34m(learn, cyc_len, max_lr, moms, div_factor, pct_start, wd, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     cbs = [OneCycleScheduler(learn, max_lr, moms=moms, div_factor=div_factor,\n\u001b[1;32m     17\u001b[0m                              pct_start=pct_start, **kwargs)]\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcyc_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlr_find\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mLearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_it\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         fit(epochs, self.model, self.loss_fn, opt=self.opt, data=self.data, metrics=self.metrics,\n\u001b[0;32m--> 133\u001b[0;31m             callbacks=self.callbacks+callbacks)\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fastai/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, model, loss_fn, opt, data, callbacks, metrics)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_update\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    324\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader timed out after {} seconds'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrebuild_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_writable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;31m# Also note we want to avoid sending a 0-length buffer separately,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             \u001b[0;31m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0mremaining\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn_head.fit_one_cycle(cyc_len=2, max_lr=lr, wd=wd, div_factor=20, pct_start=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_head.save('pre0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = np.array([lr/1000,lr/100,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(3, lrs, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search imagenet classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syns, wvs = list(zip(*syn_wv_1k))\n",
    "wvs = np.array(wvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learn' is not defined"
     ]
    }
   ],
   "source": [
    "%time pred_wv = learn.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denorm = md.val_ds.denorm\n",
    "\n",
    "def show_img(im, figsize=None, ax=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im)\n",
    "    ax.axis('off')\n",
    "    return ax\n",
    "\n",
    "def show_imgs(ims, cols, figsize=None):\n",
    "    fig,axes = plt.subplots(len(ims)//cols, cols, figsize=figsize)\n",
    "    for i,ax in enumerate(axes.flat): show_img(ims[i], ax=ax)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_imgs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e5f8ae797ce9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdenorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'show_imgs' is not defined"
     ]
    }
   ],
   "source": [
    "show_imgs(denorm(md.val_ds[start:start+25][0]), 5, (10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nmslib\n",
    "\n",
    "def create_index(a):\n",
    "    index = nmslib.init(space='angulardist')\n",
    "    index.addDataPointBatch(a)\n",
    "    index.createIndex()\n",
    "    return index\n",
    "\n",
    "def get_knns(index, vecs):\n",
    "     return zip(*index.knnQueryBatch(vecs, k=10, num_threads=4))\n",
    "\n",
    "def get_knn(index, vec): return index.knnQuery(vec, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_wvs = create_index(wvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs,dists = get_knns(nn_wvs, pred_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['spoonbill', 'bustard', 'oystercatcher'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[classids[syns[id]] for id in ids[:3]] for ids in idxs[start:start+10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search all wordnet noun classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_syns, all_wvs = list(zip(*syn2wv.items()))\n",
    "all_wvs = np.array(all_wvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_allwvs = create_index(all_wvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs,dists = get_knns(nn_allwvs, pred_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['spoonbill', 'bustard', 'oystercatcher'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill'],\n",
       " ['limpkin', 'oystercatcher', 'spoonbill']]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[classids[all_syns[id]] for id in ids[:3]] for ids in idxs[start:start+10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text -> image search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_predwv = create_index(pred_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_vecd = pickle.load(open(TRANS_PATH/'wiki.en.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = en_vecd['boat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-89391ccab9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_predwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopen_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_knn' is not defined"
     ]
    }
   ],
   "source": [
    "idxs,dists = get_knn(nn_predwv, vec)\n",
    "show_imgs([open_image(PATH/md.val_ds.fnames[i]) for i in idxs[:3]], 3, figsize=(9,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = (en_vecd['engine'] + en_vecd['boat'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-89391ccab9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_predwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopen_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_knn' is not defined"
     ]
    }
   ],
   "source": [
    "idxs,dists = get_knn(nn_predwv, vec)\n",
    "show_imgs([open_image(PATH/md.val_ds.fnames[i]) for i in idxs[:3]], 3, figsize=(9,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = (en_vecd['sail'] + en_vecd['boat'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-89391ccab9a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_predwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopen_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_knn' is not defined"
     ]
    }
   ],
   "source": [
    "idxs,dists = get_knn(nn_predwv, vec)\n",
    "show_imgs([open_image(PATH/md.val_ds.fnames[i]) for i in idxs[:3]], 3, figsize=(9,3));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image->image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'valid/n01440764/ILSVRC2012_val_00007197.JPEG'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = open_image(PATH/fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'show_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c9b680a767fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshow_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'show_img' is not defined"
     ]
    }
   ],
   "source": [
    "show_img(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_img = md.val_ds.transform(img)\n",
    "pred = learn.predict_array(t_img[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-cf37ec4574c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0midxs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_knn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn_predwv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mshow_imgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mopen_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_knn' is not defined"
     ]
    }
   ],
   "source": [
    "idxs,dists = get_knn(nn_predwv, pred)\n",
    "show_imgs([open_image(PATH/md.val_ds.fnames[i]) for i in idxs[1:4]], 3, figsize=(9,3));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
