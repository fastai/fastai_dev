{"cells": [{"cell_type": "markdown", "metadata": {}, "source": "# Additional training functions"}, {"cell_type": "markdown", "metadata": {}, "source": "[`train`](/train.html#train) provides a number of extension methods that are added to [`Learner`](/basic_train.html#Learner) (see below for a list and details), along with three simple callbacks:\n\n- [`ShowGraph`](/train.html#ShowGraph)\n- [`GradientClipping`](/train.html#GradientClipping)\n- [`BnFreeze`](/train.html#BnFreeze)"}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [], "source": "from fastai.gen_doc.nbdoc import *\nfrom fastai.train import *\nfrom fastai.docs import *"}, {"cell_type": "markdown", "metadata": {}, "source": "## [`Learner`](/basic_train.html#Learner) extension methods"}, {"cell_type": "markdown", "metadata": {}, "source": "These methods are automatically added to all [`Learner`](/basic_train.html#Learner) objects created after importing this module. They provide convenient access to a number of callbacks, without requiring them to be manually created."}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [{"data": {"text/markdown": "#### <a id=fit_one_cycle></a>`fit_one_cycle`\n`fit_one_cycle`(`learn`:[`Learner`](/basic_train.html#Learner), `cyc_len`:`int`, `max_lr`:`Union`\\[`float`, `Collection`\\[`float`\\], `slice`\\]=`slice(None, 0.003, None)`, `moms`:`Tuple`\\[`float`, `float`\\]=`(0.95, 0.85)`, `div_factor`:`float`=`25.0`, `pct_start`:`float`=`0.3`, `wd`:`float`=`None`, `kwargs`)\n\n\nFits a model following the 1cycle policy <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/train.py#L11\">[source]</a>", "text/plain": "<IPython.core.display.Markdown object>"}, "metadata": {}, "output_type": "display_data"}], "source": "show_doc(fit_one_cycle)"}, {"cell_type": "markdown", "metadata": {}, "source": "Fit a model with 1cycle training. See [`OneCycleScheduler`](/callbacks.one_cycle.html#OneCycleScheduler) for details."}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [{"data": {"text/markdown": "#### <a id=lr_find></a>`lr_find`\n`lr_find`(`learn`:[`Learner`](/basic_train.html#Learner), `start_lr`:`float`=`1e-05`, `end_lr`:`float`=`10`, `num_it`:`int`=`100`, `kwargs`:`Any`)\n\n\nExplore lr from `start_lr` to `end_lr` over `num_it` iterations of `learn` <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/train.py#L20\">[source]</a>", "text/plain": "<IPython.core.display.Markdown object>"}, "metadata": {}, "output_type": "display_data"}], "source": "show_doc(lr_find)"}, {"cell_type": "markdown", "metadata": {}, "source": "See [`LRFinder`](/callbacks.lr_finder.html#LRFinder) for details."}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [{"data": {"text/markdown": "#### <a id=to_fp16></a>`to_fp16`\n`to_fp16`(`learn`:[`Learner`](/basic_train.html#Learner), `loss_scale`:`float`=`512.0`, `flat_master`:`bool`=`False`) -> [`Learner`](/basic_train.html#Learner)\n\n\nTransforms the learner in FP16 precision <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/train.py#L26\">[source]</a>", "text/plain": "<IPython.core.display.Markdown object>"}, "metadata": {}, "output_type": "display_data"}], "source": "show_doc(to_fp16)"}, {"cell_type": "markdown", "metadata": {}, "source": "See [`MixedPrecision`](/callbacks.fp16.html#MixedPrecision) for details."}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [{"data": {"text/markdown": "## <a id=ShowGraph></a>class `ShowGraph`\n`ShowGraph`(`learn`:[`Learner`](/basic_train.html#Learner)) :: [`LearnerCallback`](/basic_train.html#LearnerCallback)\n\n\nUpdates a graph of learner stats and metrics after each epoch <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/train.py#L44\">[source]</a>", "text/plain": "<IPython.core.display.Markdown object>"}, "metadata": {}, "output_type": "display_data"}], "source": "show_doc(ShowGraph)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "data = get_mnist()"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "learn = ConvLearner(data, tvm.resnet18, metrics=accuracy, callback_fns=ShowGraph)\nlearn.fit(3)"}, {"cell_type": "markdown", "metadata": {}, "source": "![Training graph](imgs/train_graph.gif)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Internal callback implementation"}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [{"data": {"text/markdown": "#### <a id=on_epoch_end></a>`on_epoch_end`\n`on_epoch_end`(`n_epochs`:`int`, `last_metrics`:`Collection`[`Union`[`Tensor`, `Number`]], `kwargs`) -> `bool`\n\n\nIf we have metrics plot them in our pbar graph <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/train.py#L46\">[source]</a>", "text/plain": "<IPython.core.display.Markdown object>"}, "metadata": {}, "output_type": "display_data"}], "source": "show_doc(ShowGraph.on_epoch_end)"}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [{"data": {"text/markdown": "## <a id=GradientClipping></a>class `GradientClipping`\n`GradientClipping`(`learn`:[`Learner`](/basic_train.html#Learner), `clip`:`float`) :: [`LearnerCallback`](/basic_train.html#LearnerCallback)\n\n\nTo do gradient clipping during training. <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/train.py#L64\">[source]</a>", "text/plain": "<IPython.core.display.Markdown object>"}, "metadata": {}, "output_type": "display_data"}], "source": "show_doc(GradientClipping)"}, {"cell_type": "markdown", "metadata": {}, "source": "Clips gradient at a maximum absolute value of `clip` during training. For instance:"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "", "version_major": 2, "version_minor": 0}, "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, max=1), HTML(value=''))), HTML(value='epoch  train loss  va\u2026"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "Total time: 00:05\nepoch  train loss  valid loss  accuracy\n0      0.080691    0.043826    0.986261  (00:05)\n\n"}], "source": "learn = ConvLearner(data, tvm.resnet18, metrics=accuracy,\n    callback_fns=partial(GradientClipping, clip=0.1))\nlearn.fit(1)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Internal callback implementation"}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [{"data": {"text/markdown": "#### <a id=on_backward_end></a>`on_backward_end`\n`on_backward_end`(`kwargs`)\n\n\nCalled after backprop but before optimizer step. Useful for true weight decay in AdamW <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/train.py#L68\">[source]</a>", "text/plain": "<IPython.core.display.Markdown object>"}, "metadata": {}, "output_type": "display_data"}], "source": "show_doc(GradientClipping.on_backward_end)"}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [{"data": {"text/markdown": "## <a id=BnFreeze></a>class `BnFreeze`\n`BnFreeze`(`learn`:[`Learner`](/basic_train.html#Learner)) :: [`LearnerCallback`](/basic_train.html#LearnerCallback)\n\n\nFreezes moving average statistics in all batchnorm layers <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/train.py#L57\">[source]</a>", "text/plain": "<IPython.core.display.Markdown object>"}, "metadata": {}, "output_type": "display_data"}], "source": "show_doc(BnFreeze)"}, {"cell_type": "markdown", "metadata": {}, "source": "For batchnorm layers where `requires_grad==False`, you generally don't want to update their moving average statistics, in order to avoid the model's statistics getting out of sync with its pre-trained weights. You can add this callback to automate this freezing of statistics (internally, it calls `eval` on these layers)."}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [{"data": {"application/vnd.jupyter.widget-view+json": {"model_id": "", "version_major": 2, "version_minor": 0}, "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, max=1), HTML(value=''))), HTML(value='epoch  train loss  va\u2026"}, "metadata": {}, "output_type": "display_data"}, {"name": "stdout", "output_type": "stream", "text": "Total time: 00:05\nepoch  train loss  valid loss  accuracy\n0      0.085482    0.052628    0.984789  (00:05)\n\n"}], "source": "learn = ConvLearner(data, tvm.resnet18, metrics=accuracy, callback_fns=BnFreeze)\nlearn.fit(1)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Internal callback implementation"}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [{"data": {"text/markdown": "#### <a id=on_epoch_begin></a>`on_epoch_begin`\n`on_epoch_begin`(`kwargs`:`Any`)\n\n\nPut bn layers in eval mode on epoch_begin <a href=\"https://github.com/fastai/fastai_pytorch/blob/master/fastai/train.py#L59\">[source]</a>", "text/plain": "<IPython.core.display.Markdown object>"}, "metadata": {}, "output_type": "display_data"}], "source": "show_doc(BnFreeze.on_epoch_begin)"}, {"cell_type": "markdown", "metadata": {}, "source": "## Undocumented Methods - Methods moved below this line will intentionally be hidden"}, {"cell_type": "code", "execution_count": null, "metadata": {"hide_input": true}, "outputs": [], "source": "show_doc(one_cycle_scheduler)"}], "metadata": {"jekyll": {"summary": "Provides advanced training extensions to `fastai.basic_train`. Includes half-precision, learning rate finder, mixup, and one-cycle", "title": "train"}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "nbformat": 4, "nbformat_minor": 2}