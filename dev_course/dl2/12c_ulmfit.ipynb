{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ULMFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_12a import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = datasets.untar_data(datasets.URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path/'ll_lm.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,bptt = 128,70\n",
    "data = lm_databunchify(ll, bs, bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ll.train.proc_x[1].vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning the LM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before tackling the classification task, we have to finetune our language model to the IMDB corpus. Make sure you have the pretrained.pth and vocab.pkl files in your IMDB data folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/ubuntu/.fastai/data/imdb/imdb.vocab'),\n",
       " PosixPath('/home/ubuntu/.fastai/data/imdb/data_lm.pkl'),\n",
       " PosixPath('/home/ubuntu/.fastai/data/imdb/tmp_lm'),\n",
       " PosixPath('/home/ubuntu/.fastai/data/imdb/models'),\n",
       " PosixPath('/home/ubuntu/.fastai/data/imdb/vocab.pkl'),\n",
       " PosixPath('/home/ubuntu/.fastai/data/imdb/test'),\n",
       " PosixPath('/home/ubuntu/.fastai/data/imdb/pretrained.pth'),\n",
       " PosixPath('/home/ubuntu/.fastai/data/imdb/unsup'),\n",
       " PosixPath('/home/ubuntu/.fastai/data/imdb/tmp_clas'),\n",
       " PosixPath('/home/ubuntu/.fastai/data/imdb/README'),\n",
       " PosixPath('/home/ubuntu/.fastai/data/imdb/ll_lm.pkl'),\n",
       " PosixPath('/home/ubuntu/.fastai/data/imdb/train')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dps = tensor([0.25, 0.1, 0.2, 0.02, 0.15]) * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sz, nh, nl = 300, 300, 2\n",
    "model = get_language_model(len(vocab), emb_sz, nh, nl, 0, input_p=dps[0], output_p=dps[1], weight_p=dps[2], \n",
    "                           embed_p=dps[3], hidden_p=dps[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_wgts  = torch.load(path/'pretrained.pth')\n",
    "old_vocab = pickle.load(open(path/'vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(347, 231)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.index('house'),old_vocab.index('house')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_wgt  = old_wgts['0.encoder.weight'][231]\n",
    "house_bias = old_wgts['1.decoder.bias'][231] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_embeds(old_wgts, old_vocab, new_vocab):\n",
    "    wgts = old_wgts['0.encoder.weight']\n",
    "    bias = old_wgts['1.decoder.bias']\n",
    "    wgts_m,bias_m = wgts.mean(dim=0),bias.mean()\n",
    "    new_wgts = wgts.new_zeros(len(new_vocab), wgts.size(1))\n",
    "    new_bias = bias.new_zeros(len(new_vocab))\n",
    "    for i,w in enumerate(new_vocab): \n",
    "        if w in old_vocab:\n",
    "            idx = old_vocab.index(w)\n",
    "            new_wgts[i],new_bias[i] = wgts[idx],bias[idx]\n",
    "        else: new_wgts[i],new_bias[i] = wgts_m,bias_m\n",
    "    old_wgts['0.encoder.weight']    = new_wgts\n",
    "    old_wgts['0.encoder_dp.emb.weight'] = new_wgts\n",
    "    old_wgts['1.decoder.weight']    = new_wgts\n",
    "    old_wgts['1.decoder.bias']      = new_bias\n",
    "    return old_wgts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgts = match_embeds(old_wgts, old_vocab, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(wgts['0.encoder.weight'][347],house_wgt)\n",
    "assert torch.allclose(wgts['1.decoder.bias'][347],house_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(wgts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict, path/'tmp_clas'/'init.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): AWD_LSTM(\n",
       "    (encoder): Embedding(60003, 300, padding_idx=0)\n",
       "    (encoder_dp): EmbeddingDropout(\n",
       "      (emb): Embedding(60003, 300, padding_idx=0)\n",
       "    )\n",
       "    (rnns): ModuleList(\n",
       "      (0): WeightDropout(\n",
       "        (module): LSTM(300, 300, batch_first=True)\n",
       "      )\n",
       "      (1): WeightDropout(\n",
       "        (module): LSTM(300, 300, batch_first=True)\n",
       "      )\n",
       "    )\n",
       "    (input_dp): RNNDropout()\n",
       "    (hidden_dps): ModuleList(\n",
       "      (0): RNNDropout()\n",
       "      (1): RNNDropout()\n",
       "    )\n",
       "  )\n",
       "  (1): LinearDecoder(\n",
       "    (decoder): Linear(in_features=300, out_features=60003, bias=True)\n",
       "    (output_dp): RNNDropout()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lm_splitter(m):\n",
    "    groups = []\n",
    "    for i in range(len(m[0].rnns)): groups.append(nn.Sequential(m[0].rnns[i], m[0].hidden_dps[i]))\n",
    "    groups = [nn.Sequential(m[0].encoder, m[0], m[0].input_dp, m[1])]\n",
    "    return [list(o.parameters()) for o in groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rnn in model[0].rnns:\n",
    "    for p in rnn.parameters(): p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [partial(AvgStatsCallback,accuracy_flat),\n",
    "       CudaCallback,\n",
    "       Recorder,\n",
    "       partial(GradientClipping, clip=0.1),\n",
    "       partial(RNNTrainer, alpha=2., beta=1.),\n",
    "       ProgressCallback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(model, data, cross_entropy_flat, opt_func=adam_opt(), cb_funcs=cbs, splitter=lm_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-2\n",
    "sched_lr  = combine_scheds([0.5,0.5], cos_1cycle_anneal(lr/10., lr, lr/1e5))\n",
    "sched_mom = combine_scheds([0.5,0.5], cos_1cycle_anneal(0.8, 0.7, 0.8))\n",
    "cbsched = [ParamScheduler('lr', sched_lr), ParamScheduler('mom', sched_mom)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy_flat</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy_flat</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.503475</td>\n",
       "      <td>0.245633</td>\n",
       "      <td>4.301658</td>\n",
       "      <td>0.262508</td>\n",
       "      <td>07:17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(1, cbs=cbsched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rnn in model[0].rnns:\n",
    "    for p in rnn.parameters(): p.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-3\n",
    "sched_lr  = combine_scheds([0.25,0.75], cos_1cycle_anneal(lr/10.,lr, 0))\n",
    "sched_lr1 = combine_scheds([0.25,0.75], cos_1cycle_anneal(lr/20.,lr/2., 0))\n",
    "sched_mom = combine_scheds([0.25,0.75], cos_1cycle_anneal(0.8,0.7, 0.8))\n",
    "cbsched = [ParamScheduler('lr', [sched_lr1, sched_lr1, sched_lr]), ParamScheduler('mom', sched_mom)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy_flat</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy_flat</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>4.281489</td>\n",
       "      <td>0.261222</td>\n",
       "      <td>4.236016</td>\n",
       "      <td>0.269402</td>\n",
       "      <td>07:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.212239</td>\n",
       "      <td>0.268824</td>\n",
       "      <td>4.186624</td>\n",
       "      <td>0.274908</td>\n",
       "      <td>07:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.157397</td>\n",
       "      <td>0.274520</td>\n",
       "      <td>4.152080</td>\n",
       "      <td>0.278439</td>\n",
       "      <td>07:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.118742</td>\n",
       "      <td>0.278401</td>\n",
       "      <td>4.128485</td>\n",
       "      <td>0.280957</td>\n",
       "      <td>07:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.092115</td>\n",
       "      <td>0.280964</td>\n",
       "      <td>4.111547</td>\n",
       "      <td>0.282777</td>\n",
       "      <td>07:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.069495</td>\n",
       "      <td>0.283180</td>\n",
       "      <td>4.099864</td>\n",
       "      <td>0.283730</td>\n",
       "      <td>07:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.054394</td>\n",
       "      <td>0.284509</td>\n",
       "      <td>4.090035</td>\n",
       "      <td>0.285012</td>\n",
       "      <td>07:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.042381</td>\n",
       "      <td>0.285620</td>\n",
       "      <td>4.085082</td>\n",
       "      <td>0.285447</td>\n",
       "      <td>07:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.035013</td>\n",
       "      <td>0.286289</td>\n",
       "      <td>4.082782</td>\n",
       "      <td>0.285659</td>\n",
       "      <td>07:40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.031946</td>\n",
       "      <td>0.286588</td>\n",
       "      <td>4.082381</td>\n",
       "      <td>0.285740</td>\n",
       "      <td>07:39</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(10, cbs=cbsched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learn.model[0].state_dict(), path/'finetuned_enc.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(vocab, open(path/'vocab_lm.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(learn.model.state_dict(), path/'finetuned.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to process the data again otherwise pickle will complain. We also have to use the same vocab as the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pickle.load(open(path/'vocab_lm.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 00:22<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 00:22<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "il = TextList.from_files(path, include=['train', 'test'])\n",
    "sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='test'))\n",
    "ll = label_by_func(sd, parent_labeler, proc_x = [TokenizeProcessor(), NumericalizeProcessor(vocab=vocab)], \n",
    "                   proc_y=CategoryProcessor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_tok,proc_num,proc_cat = TokenizeProcessor(),NumericalizeProcessor(),CategoryProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 00:15<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 00:16<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "il = TextList.from_files(path, include=['train', 'test'])\n",
    "sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='test'))\n",
    "ll = label_by_func(sd, parent_labeler, proc_x = [proc_tok, proc_num], proc_y=proc_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ll, open(path/'ll_clas.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path/'ll_clas.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate(samples, pad_idx=1, pad_first=False):\n",
    "    lens = tensor([len(s[0]) for s in samples])\n",
    "    idxs = lens.argsort(descending=True)\n",
    "    samples = [samples[i] for i in idxs]\n",
    "    res = torch.zeros(len(samples), len(samples[0][0])).long() + pad_idx\n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: res[i,-len(s[0]):] = LongTensor(s[0])\n",
    "        else:         res[i,:len(s[0])] = LongTensor(s[0])\n",
    "    return res, tensor([s[1] for s in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xxbos a quite usual trashy xxmaj italo - xxmaj western , stupid storyline full of clich√©s and lack of logic , some mediocre actors , dirty settings , lots of punch - fights and people shoot dead on a massive scale . \\n\\n xxmaj this has nothing to do with xxmaj django . - xxmaj at least not in my xxmaj german translated version , this xxmaj german dvd - release is called \" xxmaj adios xxmaj companeros \" and has xxmaj macho xxmaj callaghan fighting against xxmaj butch xxmaj cassidy and xxmaj ironhead because their gang killed his one ( he \\'s the only survivor ) . xxmaj then you have xxmaj butch xxmaj cassidy and xxmaj ironhead fighting each other because they quarreled and the gang split . xxmaj and you have xxmaj ironhead fighting against everyone because he \\'s just the biggest and most greedy asshole anyway . xxmaj yeah , that \\'s it , no more cleverness in the storyline , hehe . \\n\\n a small role by xxmaj klaus xxmaj kinski as xxmaj reverend xxmaj cotton is remarkable ( that \\'s why i bought this dvd ) . xxmaj in one scene he attempts to separate two men fighting by hitting them and screaming \" i said love ! \" and in another scene he wins a competition in throwing xxbos and goes nuts for a second - xxbos ! ! ! \\n\\n xxmaj it \\'s also remarkable that joe d\\'amato aka xxmaj aristide xxmaj xxbos did the cinematography - i love this master of incompetent exploitation - thrash , so it was an \" xxbos \" for me . xxeos',\n",
       "  'pos'),\n",
       " ('xxbos revolt of the zombies ( 2 outta 5 stars ) xxmaj no , this is not a long - lost ancestor to the classic xxmaj george a. xxmaj romero zombie flicks . xxmaj this is a low - budget potboiler from 1936 that probably seemed very cool to audiences of the time ... but seems awfully routine these days . xxmaj there is actually a pretty good scene at the start of a soldier firing off his pistol into a horde of approaching zombie soldiers ... and a close - up of bullets entering the bare chest of one of them . xxmaj the effect looks hopelessly fake these days but in 1936 i \\'m sure it had audiences gasping . xxmaj the story concerns the search for the secret of mind control ... ostensibly to create an unstoppable zombie army ... but later as a means for one character to win the woman he loves . xxmaj the movie is barely an hour long but moves at a snail \\'s pace so it seems feature - length , believe me ! xxmaj there really is n\\'t much to recommend it ... you may get some amusement from the faked studio shots of the star \" wading \" through a \" swamp \" . xxmaj the ending is interesting ... so i \\'d say the movie is worth seeing at least once . xxmaj more than likely you will see it as an extra feature on some cheap \" 4 movies on 1 dvd \" compilation at xxmaj wal - xxmaj mart for five bucks . xxmaj hey , it \\'s well worth the money ... xxeos',\n",
       "  'pos'),\n",
       " (\"xxbos i liked it but then i think i might have been xxbos at the same time . xxmaj this reworking of xxmaj xxbos de xxmaj xxbos / xxmaj roxanne is an utterly undemanding , formulaic romcom rescued from straight - to - video xxbos on its release by the sharp turn of xxmaj janeane xxmaj garofalo . xxmaj playing the xxmaj frasier of xxmaj pets , she finds herself caught in a love trap when insecurity leads her to pass her best friend ( xxmaj uma xxmaj thurman ) off as herself when a caller comes a - xxbos ' . \\n\\n xxmaj this is an interesting film in the fascinating career of xxmaj ben xxmaj chaplin . xxmaj an average xxmaj british actor , he gave the xxmaj hollywood treadmill a shot with this film . xxmaj he is unremarkable and his anonymity in studio productions is unsurprising on the basis of it , although he has appeared in substantial cameos in both the later xxmaj terence xxmaj malick films . xxmaj uma xxmaj thurman does a ditzy turn on autopilot and xxmaj michael xxmaj lehmann packages it all together competently . xxmaj icky phone sex though . 4 / 10 xxeos\",\n",
       "  'pos'),\n",
       " ('xxbos xxmaj with one of my very favorite actors , xxmaj james xxmaj spader , i expected this film to be at least tolerable . xxmaj it was n\\'t . xxmaj after the first half hour i watched the rest of it with the remote control in my hand so the fast forward was at the ready . xxmaj so trite , so standard , one knows what \\'s going to happen in each scene . xxmaj one can even predict the dialogue word for word . xxmaj this is one of those movies that makes one scratch ones head and say , \" xxmaj how did this movie ever get made ? \" xxmaj in an effort to say something positive , i \\'ll add that there are some mildly entertaining special effects . xxmaj but , on the whole , if you \\'ve seen 5 xxmaj sci / fi movies , or you are over 9 years old , do yourself a favor and skip this one . xxeos',\n",
       "  'pos'),\n",
       " ('xxbos i gave this film 2 stars only because xxmaj dominic xxmaj monaghan actually put effort through in his acting . xxmaj everything else about this film is extremely amateur . xxmaj everything associated with the direction of this film was very poorly executed . xxmaj not only should the director rethink what she is doing for a life career but maybe she should watch a few films . xxmaj as xxmaj dominic xxmaj monaghan is a very credible actor , placing him in a film of this caliber makes him look awful . xxmaj whomever the \" actor \" was that played xxmaj jack \\'s best friend should never have stepped in front of the camera . i did n\\'t expect much from such a small film , but perhaps a little more time and effort should be put into the characters and their surroundings . xxmaj do n\\'t waste your time or money on this film ( like i did ) you will be sorely disappointed . xxeos',\n",
       "  'pos'),\n",
       " (\"xxbos xxmaj wow ... speechless as to the making of this film , i ca n't say much . xxmaj the xxbos at the local videostore should 've said it all ... nothing but 6 actors / actresses who get lost on the set of xxmaj scream and decide to shoot a movie ! \\n\\n xxmaj the acting was apparently not in the budget , but they were able to afford nudity and good - looking actors ! xxmaj style over substance almost makes its mark here , except most of these acting - class failures keep forgetting that there is a plot that needs to go somewhere when they were reading this script . xxmaj after only 4 or 5 kills by the so - called masked murderer and a confusing tie - in plot about a xxmaj murder xxmaj club which the dumb lead actress thinks is a real club that she can join ( only if she can get over a girl bumping into her car ) , you want to stab your hands with the nearest sharp object to remind yourself never to get overly excited by a possibly good movie such as this . \\n\\n i feel bad for the people who bought this film and ca n't find anyone to take it off their hands . xxmaj another example of what 's wrong with the growing number of straight to video horror releases with no thought put into the essentials . xxmaj throw it away if you did buy this . xxeos\",\n",
       "  'pos'),\n",
       " ('xxbos xxmaj this was a quite brutal movie . xxmaj there were huge implausibilities , and a silly script , bad acting , etc . \\n\\n xxmaj the only reason to watch this movie is that from time to time some quite impressive sets of breasts were exposed . xxeos',\n",
       "  'pos'),\n",
       " ('xxbos xxmaj this movie was a real torture fest to sit through . xxmaj its first mistake is treating nuclear power as so self - evidently a \\' bad thing \\' that it barely needs to convince the audience of it . xxmaj when it does stoop to putting in its argument , it has the participants breathlessly deliver barely xxbos facts ; all that \\'s missing is someone crying \" when is someone going to think of the children ! \" . xxmaj while watching this movie , i kept thinking \" where \\'d you hear that ? \" or \" that ca n\\'t possibly be true \" - yet little of the info was backed up by any reliable sources . xxmaj and bless \\'em , the \\' regular folks \\' in the movie came across more like xxmaj xxbos than people with any understanding of the pros and cons of nuclear power ; to be fair , that might be the fault of the film - makers , but equally fairly , it \\'s a condition shared by the movie \\'s rock stars . \\n\\n xxmaj as for the performers xxrep 11 . xxmaj now some of these people are highly respected musicians whose music i \\'ve enjoyed , and i \\'m sure a few of them really did believe in this cause . xxmaj but they all come across as wheezing old hippies desperately searching for something to get worked up over , now that the 60s have passed them by . xxmaj particularly embarrassing are xxmaj graham xxmaj nash and xxmaj james xxmaj taylor . xxmaj nash seems to be trying too hard - he looks like he ca n\\'t possibly believe the things he \\'s being told ( not that i blame him ) , but desperate to feel noticed and included . xxmaj james xxmaj taylor performs what has to be the wimpiest protest \" anthem \" ever , \" xxmaj stand and xxmaj fight \" , in the most sickeningly cheerful way you can imagine . xxmaj in fact , most of the performances are pretty bland when they \\'re not being patronizing . xxmaj nobody seems worked up by this event , as if it really does n\\'t mean much to them at all . xxmaj it \\'s worth noting that the driving force behind this whole event seems to be xxmaj john xxmaj hall , of the band xxmaj orleans , and responsible for some of the wimpiest mor pop of the 70s . ( xxmaj remember , if you dare , \" xxmaj dance xxmaj with xxmaj me \" and \" xxmaj still the xxmaj one \" . ) xxmaj it \\'s worth noting because that \\'s symbolic of how the cause here fails to inspire any real passion in the music . xxmaj the cause is supposedly life - or - death , but everybody sleepwalks through their numbers like they \\'re playing the xxmaj catskills . xxmaj except maybe xxmaj gil - xxmaj scott xxmaj heron - his protest number \" xxmaj we xxmaj almost xxmaj lost xxmaj detroit \" is on topic at least , but delivered with all the smugness of a high - schooler impressed with how \\' controversial \\' he \\'s being . \\n\\n xxmaj only xxmaj bruce xxmaj springsteen \\'s performance raises a pulse ; i \\'ve never been a big fan of the xxmaj boss , but he absolutely smokes , no question . xxmaj part of me thinks he was taped separately , at another event , and edited into this movie to give wake the audience . xxmaj compared to the general blandness and air of self - satisfaction here , it \\'s no wonder xxmaj bruce was hailed as the savior of rock\\'n\\'roll . \\n\\n xxmaj but even his performance is hobbled by the lifeless concert shooting . i do n\\'t expect a lot of flashy camera movement from a \\' 70s film , but the shots are unnecessarily static , broken up only by split - second cutaways to a back - up singer \\'s xxbos . xxmaj now , some of this may be because the performers are lifeless to start with ; and * maybe * the film - makers are more skilled at shooting documentaries than concert footage - but all you have to do is watch \" xxmaj rust xxmaj never xxmaj sleeps \" or \" xxmaj the xxmaj last xxmaj waltz \" to see a movie like this done with more skill . xxmaj and with more exciting musicians . \\n\\n xxmaj so really , there \\'s only two things to watch this movie for : xxmaj springsteen \\'s stellar performance , and as a sad snapshot about a counter - culture in decline . xxeos',\n",
       "  'pos'),\n",
       " ('xxbos xxmaj stephen xxmaj king adaptation ( scripted by xxmaj king himself ) in which a young family , newcomers to rural xxmaj maine , find out about the pet cemetery close to their home . xxmaj the father ( xxmaj dale xxmaj midkiff ) then finds out about the xxmaj micmac burial ground beyond the pet cemetery that has powers of resurrection - only of course anything buried there comes back not quite right . \\n\\n xxmaj below average \" horror \" picture starts out clumsy , insulting , and inept , and continues that way for a while , with the absolute worst element being xxmaj midkiff \\'s worthless performance . xxmaj it gets a little better toward the end , with genuinely disturbing finale . xxmaj in point of fact , the whole movie is really disturbing , which is why i ca n\\'t completely dismiss it - at least it has something to make it memorable . xxmaj decent supporting performances by xxmaj fred xxmaj gwynne , as the wise old aged neighbor , and xxmaj brad xxmaj greenquist , as the disfigured spirit xxmaj victor xxmaj pascow are not enough to really redeem film . \\n\\n xxmaj king has his usual cameo as the minister . \\n\\n xxmaj followed by a sequel also directed by xxmaj mary xxmaj lambert ( is it any wonder that she \\'s had no mainstream film work since ? ) . \\n\\n 4 / 10 xxeos',\n",
       "  'pos'),\n",
       " ('xxbos a young scientist is trying to carry on his dead father \\'s work on limb regeneration . xxmaj his overbearing mother has convinced him that he murdered his own father and is monitoring his progress for her own evil purposes . a young doctor uses reptilian dna he extracts from a large creature and when his arm is conveniently ripped off a few minutes later , he injects himself with his formula and grows a new murderous arm ... xxmaj admittedly the special effects in \" xxmaj severed xxmaj ties \" are pretty good and grotesque , but the rest of the film is awful . xxmaj the severed arm is behaving like a snake and kills few people . xxmaj big deal . xxmaj the acting is mediocre and the climax is xxbos out of 10 . xxeos',\n",
       "  'pos')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ll.train.x_obj(i),ll.train.y_obj(i)) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clas_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=bs  , collate_fn=pad_collate, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, collate_fn=pad_collate, **kwargs))\n",
    "\n",
    "def clas_databunchify(sd, bs, **kwargs):\n",
    "    return DataBunch(*get_clas_dls(sd.train, sd.valid, bs, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,bptt = 64,70\n",
    "data = clas_databunchify(ll, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['pos', 'neg'], ['pos', 'neg'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_ds.proc_y.vocab, data.valid_ds.proc_y.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pickle.load(open(path/'vocab_lm.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = proc_num.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(data.train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxbos i can not believe this woodenly written and directed piece of cliche film got made . xxmaj there are about four good looking shots ( the director should think about switching to still photography ) and that 's it . a strong cast is utterly wasted , scenes repeatedly end at the least interesting moments and the script says nothing new . xxmaj please spare yourself this movie . xxeos\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 61\n",
    "length = x.size(1) - (x[idx]==1).long().sum()\n",
    "' '.join([vocab[i] for i in x[idx,:length]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(data.valid_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj the 1970s are often regarded as a golden age of xxmaj british television comedy , a period which saw numerous classic sitcoms as well as sketch shows such as \" xxmaj monty xxmaj python \\'s xxmaj flying xxmaj circus \" . xxmaj the period was , however , emphatically not a golden age of xxmaj british film comedy , and what worked well on television rarely transferred successfully to the big screen . xxmaj the most triumphant exceptions to this rule were provided by the xxmaj pythons , but their best films ( \" xxmaj monty xxmaj python and the xxmaj holy xxmaj grail \" and \" xxmaj life of xxmaj brian \" ) were very different in conception to their tv show . \\n\\n xxmaj the main problem with adapting sitcoms for the cinema is that concepts devised to fit the bbc \\'s 30 minute slots ( 25 minutes on itv , which has to find room for commercials ) do not always work as well when expanded into a feature film three or four times as long . xxmaj few people will remember the film versions of , say , \" xxmaj up xxmaj pompeii ! \" or \" xxmaj steptoe and xxmaj son \" with the same affection as the television versions . xxmaj in the case of many classic tv comedy shows ( \" xxmaj some xxmaj mothers xxmaj do \\' xxmaj ave \\' xxmaj em \" , \" xxmaj yes , xxmaj minister \" , \" xxmaj fawlty xxmaj towers \" , \" xxmaj the xxmaj goodies \" ) no attempt was made to film them at all , for which we can be grateful . xxmaj characters such as xxmaj michael xxmaj crawford \\'s xxmaj frank xxmaj spencer or xxmaj john xxmaj cleese \\'s xxmaj basil xxmaj fawlty can be hilarious in half - hour doses , but i doubt if they would remain as funny over two hours . xxmaj one comedy programme ( albeit a dramatisation of a comic novel rather than a sitcom in the normal sense ) which might have worked in the cinema was \" xxmaj the xxmaj fall and xxmaj rise of xxmaj reginald xxmaj perrin \" , but any hopes of a film were dashed by the tragically early death of its star xxmaj leonard xxmaj rossiter . \\n\\n \" xxmaj dad \\'s xxmaj army \" was one of the few television sitcoms of the period which was turned into a decent film . ( xxmaj about the only other one i can think of was \" xxmaj porridge \" ) . xxmaj this was possibly because it had an unusually large number of well - developed characters and derived most of its humour from the interactions between them . xxmaj the original sitcom ran between 1968 and 1977 and told of the misadventures of a xxmaj home xxmaj guard platoon in the small seaside town of xxmaj xxunk - on - xxmaj sea . ( xxmaj the xxmaj home xxmaj guard , initially known as the xxmaj local xxmaj defence xxmaj volunteers , was an auxiliary militia during xxmaj world xxmaj war ii made up , for the most part , of men too old to serve in the regular forces ) . xxmaj the film version is a three - act drama . xxmaj act i deals with the formation of the platoon and the recruitment of its members . xxmaj in xxmaj act ii they cause havoc during an xxmaj army training exercise . xxmaj in xxmaj act iii they succeed in capturing a group of xxmaj nazi airmen whose plane has been shot down . \\n\\n xxmaj the three key players in this drama are the platoon \\'s commander , xxmaj captain xxmaj george xxmaj mainwaring ( xxmaj arthur xxmaj lowe ) , and his two subordinates xxmaj sergeant xxmaj arthur xxmaj wilson ( xxmaj john xxmaj le xxmaj mesurier ) and xxmaj corporal xxmaj jack xxmaj jones ( xxmaj clive xxmaj dunn ) . xxmaj mainwaring , who in civilian life is the local bank manager , is a fussy little man , peering at the world through a pair of thick spectacles . xxmaj it is he who takes the initiative in forming the xxmaj home xxmaj guard unit and who appoints himself its commander . xxmaj he is pompous , officious , with an exaggerated sense of his own importance and of his own powers of leadership , the sort of man who does not suffer fools gladly . ( xxmaj and in xxmaj george xxmaj mainwaring \\'s world - view the term \" fool \" covers most of the rest of the human race ) . xxmaj he does , however , have his good qualities . xxmaj he is motivated by a genuine patriotic idealism and is capable of great physical courage , shown in his encounter with the xxmaj germans . \\n\\n xxmaj wilson is xxmaj mainwaring \\'s deputy at the bank . xxmaj the two men are very different in character , something emphasised by a difference in appearance , xxmaj wilson being tall and thin whereas xxmaj mainwaring is short and stout . xxmaj he comes across as being both more intelligent and better educated than his boss . ( xxmaj his accent suggests he may be a former public schoolboy ) . xxmaj nevertheless , he has ended up playing second fiddle both in civilian and military life , probably because he has the sort of passive personality which leads to pessimism and defeatism and an inability to take anything altogether seriously . xxmaj jones is an old soldier who now runs the local butcher \\'s shop . ( xxmaj his promotion to xxmaj corporal is due mainly to his ability to bribe xxmaj mainwaring with black market sausages ) . xxmaj his enthusiasm for his new role is matched only by his incompetence and ability to cause chaos . xxmaj although his catchphrase is \" xxmaj do n\\'t panic ! \" he is prone to panicking at any given opportunity . \\n\\n xxmaj several other members of the platoon are featured . xxmaj private xxmaj fraser , the dour xxmaj scottish undertaker , is even more of a pessimist than xxmaj wilson . ( xxmaj catchphrase : \" xxmaj we \\'re doomed , man , doomed ! \" ) . xxmaj private xxmaj godfrey is a gentle old man whose main concern is the whereabouts of the nearest lavatory . xxmaj private xxmaj walker is a sharp xxmaj cockney spiv and xxmaj private xxmaj pike ( another bank employee ) a spoilt mummy \\'s boy . ( xxmaj pike \\'s mother is xxmaj wilson \\'s mistress , although xxmaj wilson tries to keep this liaison secret from the disapproving xxmaj mainwaring ) . xxmaj two significant outsiders are the mild - mannered xxmaj vicar and the xxunk warden , xxmaj mainwaring \\'s detested enemy and quite his equal in pompousness and xxunk . \\n\\n xxmaj there are occasional bawdy doubles entendres ( \" xxmaj keep your hands off my xxunk xxmaj mainwaring is ostensibly referring to those soldiers who hold that rank ) , more so than in the television show which was surprisingly free of innuendo . ( xxmaj its creators , xxmaj david xxmaj croft and xxmaj jimmy xxmaj perry , would later go on to create comedy shows such as \" xxmaj are xxmaj you xxmaj being xxmaj served ? \" and \" xxmaj hi - de - hi \" which were notorious for suggestive humour ) . xxmaj the film does , however , preserve much of the mixture of gentle wit , nostalgia and sharp characterisation which made the tv series so successful . 7 / 10 xxeos'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 61\n",
    "length = x.size(1) - (x[idx]==1).long().sum()\n",
    "' '.join([vocab[i] for i in x[idx,:length]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(data.train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3311])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3311, 2425, 1782, 1486, 1481])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = x.size(1) - (x == 1).sum(1)\n",
    "lengths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_emb = nn.Embedding(len(vocab), 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed = pack_padded_sequence(tst_emb(x), lengths, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = nn.Dropout(p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst = nn.LSTM(300, 300, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,h = tst(packed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpack = pad_packed_sequence(y, batch_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to change our model a little bit to use this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AWD_LSTM1(nn.Module):\n",
    "    \"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182.\"\n",
    "    initrange=0.1\n",
    "\n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n",
    "                 hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n",
    "        super().__init__()\n",
    "        self.bs,self.emb_sz,self.n_hid,self.n_layers,self.pad_token = 1,emb_sz,n_hid,n_layers,pad_token\n",
    "        self.encoder = nn.Embedding(vocab_sz, emb_sz, padding_idx=pad_token)\n",
    "        self.encoder_dp = EmbeddingDropout(self.encoder, embed_p)\n",
    "        self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l != n_layers - 1 else emb_sz), 1,\n",
    "                             batch_first=True) for l in range(n_layers)]\n",
    "        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns])\n",
    "        self.encoder.weight.data.uniform_(-self.initrange, self.initrange)\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)])\n",
    "\n",
    "    def forward(self, input):\n",
    "        bs,sl = input.size()\n",
    "        mask = (input == self.pad_token)\n",
    "        lengths = sl - mask.long().sum(1)\n",
    "        raw_output = self.input_dp(self.encoder_dp(input))\n",
    "        raw_outputs,outputs = [],[]\n",
    "        for l, (rnn,hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output = pack_padded_sequence(raw_output, lengths, batch_first=True)\n",
    "            raw_output, hid = rnn(raw_output)#, self.hidden[l])\n",
    "            raw_output = pad_packed_sequence(raw_output, batch_first=True)[0]\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l != self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "        return raw_outputs, outputs, mask\n",
    "\n",
    "    def _one_hidden(self, l):\n",
    "        \"Return one hidden state.\"\n",
    "        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n",
    "        return next(self.parameters()).new(1, self.bs, nh).zero_()\n",
    "\n",
    "    def reset(self):\n",
    "        \"Reset the hidden states.\"\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use three things for the classification head of the model: the last hidden state, the average of all the hidden states and the maximum of all the hidden states. The trick is just to, once again, ignore the padding in the last elemnt/average/maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling(nn.Module):\n",
    "    def forward(self, input):\n",
    "        raw_outputs,outputs,mask = input\n",
    "        output = outputs[-1]\n",
    "        lengths = output.size(1) - mask.long().sum(dim=1)\n",
    "        avg_pool = output.masked_fill(mask[:,:,None], 0).sum(dim=1)\n",
    "        avg_pool.div_(lengths.type(avg_pool.dtype)[:,None])\n",
    "        max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "        x = torch.cat([output[torch.arange(0, output.size(0)),lengths-1], max_pool, avg_pool], 1) #Concat pooling.\n",
    "        return output,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = AWD_LSTM1(len(vocab), emb_sz, n_hid=nh, n_layers=nl, pad_token=1)\n",
    "pool = Pooling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(data.train_dl))\n",
    "output,c = pool(enc(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check we have padding with 1s at the end of each text (except the first which is the longest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,     7,  1150,  ..., 16134,    24,     3],\n",
       "        [    2,     7,    65,  ...,     1,     1,     1],\n",
       "        [    2,     7,  4844,  ...,     1,     1,     1],\n",
       "        ...,\n",
       "        [    2,    12,  1480,  ...,     1,     1,     1],\n",
       "        [    2,     7,   584,  ...,     1,     1,     1],\n",
       "        [    2,    12,   655,  ...,     1,     1,     1]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch puts 0s everywhere we had padding in the `output` when unpacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose((output.sum(dim=2) == 0).float(), (x==1).float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the last hidden state isn't the last element of `output`. Let's check we got everything right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(bs):\n",
    "    length = x.size(1) - (x[i]==1).long().sum()\n",
    "    out_unpad = output[i,:length]\n",
    "    assert torch.allclose(out_unpad[-1], c[i,:300])\n",
    "    assert torch.allclose(out_unpad.max(0)[0], c[i,300:600])\n",
    "    assert torch.allclose(out_unpad.mean(0), c[i,600:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pooling layer properly ignored the padding, so now let's group it with a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_drop_lin(n_in, n_out, bn=True, p=0., actn=None):\n",
    "    layers = [nn.BatchNorm1d(n_in)] if bn else []\n",
    "    if p != 0: layers.append(nn.Dropout(p))\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if actn is not None: layers.append(actn)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingLinearClassifier(nn.Module):\n",
    "    \"Create a linear classifier with pooling.\"\n",
    "\n",
    "    def __init__(self, layers, drops):\n",
    "        super().__init__()\n",
    "        mod_layers = []\n",
    "        activs = [nn.ReLU(inplace=True)] * (len(layers) - 2) + [None]\n",
    "        for n_in, n_out, p, actn in zip(layers[:-1], layers[1:], drops, activs):\n",
    "            mod_layers += bn_drop_lin(n_in, n_out, p=p, actn=actn)\n",
    "        self.layers = nn.Sequential(*mod_layers)\n",
    "\n",
    "    def forward(self, input):\n",
    "        raw_outputs,outputs,mask = input\n",
    "        output = outputs[-1]\n",
    "        lengths = output.size(1) - mask.long().sum(dim=1)\n",
    "        avg_pool = output.masked_fill(mask[:,:,None], 0).sum(dim=1)\n",
    "        avg_pool.div_(lengths.type(avg_pool.dtype)[:,None])\n",
    "        max_pool = output.masked_fill(mask[:,:,None], -float('inf')).max(dim=1)[0]\n",
    "        x = torch.cat([output[torch.arange(0, output.size(0)),lengths-1], max_pool, avg_pool], 1) #Concat pooling.\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just have to feed our texts to those two blocks, (but we can't give them all at once to the AWD_LSTM or we'll get OOM error: we'll go for chunks of bptt length to regularly detach the history of our hidden states.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Not used\n",
    "class SentenceEncoder(nn.Module):\n",
    "    def __init__(self, bptt, module, max_batch=500):\n",
    "        super().__init__()\n",
    "        self.bptt,self.module,self.max_batch = bptt,module,max_batch\n",
    "    \n",
    "    def reset(self): \n",
    "        if hasattr(self.module, 'reset'): self.module.reset()\n",
    "    \n",
    "    def concat(self, arrs):\n",
    "        return [torch.cat([l[i] for l in arrs], dim=1) for i in range(len(arrs[0]))]\n",
    "    \n",
    "    def forward(self, input):\n",
    "        bs,sl = input.size()\n",
    "        self.reset()\n",
    "        raw_outputs,outputs,masks = [],[],[]\n",
    "        for i in range(0, sl, self.bptt):\n",
    "            #We only keep what is at the bptt * max_len first positions.\n",
    "            r,o,m = self.module(input[:,i: min(i+self.bptt, sl)])\n",
    "            if i>(sl-self.max_batch * self.bptt):\n",
    "                masks.append(m)\n",
    "                raw_outputs.append(r)\n",
    "                outputs.append(o)\n",
    "        return self.concat(raw_outputs),self.concat(outputs),torch.cat(masks,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_classifier(vocab_sz, emb_sz, n_hid, n_layers, n_out, pad_token, bptt, max_batch=500, layers=None,\n",
    "                        drops=None, output_p=0.4, hidden_p=0.2, input_p=0.6, embed_p=0.1, weight_p=0.5):\n",
    "    \"To create a full AWD-LSTM\"\n",
    "    rnn_enc = AWD_LSTM1(vocab_sz, emb_sz, n_hid=n_hid, n_layers=n_layers, pad_token=pad_token,\n",
    "                        hidden_p=hidden_p, input_p=input_p, embed_p=embed_p, weight_p=weight_p)\n",
    "    #enc = SentenceEncoder(bptt, rnn_enc, max_batch=max_batch)\n",
    "    if layers is None: layers = [100]\n",
    "    if drops is None:  drops = [0.1] * len(layers)\n",
    "    layers = [3 * emb_sz] + layers + [n_out] \n",
    "    drops = [output_p] + drops\n",
    "    return SequentialRNN(rnn_enc, PoolingLinearClassifier(layers, drops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sz, nh, nl = 300, 300, 2\n",
    "dps = tensor([0.4, 0.4, 0.5, 0.05, 0.3]) * 0.\n",
    "model = get_text_classifier(len(vocab), emb_sz, nh, nl, 2, 1, bptt, input_p=dps[0], output_p=dps[1], weight_p=dps[2], \n",
    "                           embed_p=dps[3], hidden_p=dps[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load our pretrained encoder and freeze it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_splitter(m):\n",
    "    groups = [nn.Sequential(m[0].encoder, m[0], m[0].input_dp)]\n",
    "    for i in range(len(m[0].rnns)): groups.append(nn.Sequential(m[0].rnns[i], m[0].hidden_dps[i]))\n",
    "    groups.append(m[1])\n",
    "    return [list(o.parameters()) for o in groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model[0].parameters(): p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [partial(AvgStatsCallback,accuracy),\n",
    "       CudaCallback,\n",
    "       Recorder,\n",
    "       #partial(GradientClipping, clip=0.1),\n",
    "       #partial(RNNTrainer, alpha=0., beta=0.),\n",
    "       ProgressCallback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model[0].load_state_dict(torch.load(path/'finetuned_enc.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), path/'init_clas.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(model, data, F.cross_entropy, lr=1e-2, opt_func=adam_opt(), cb_funcs=cbs)#, splitter=class_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.693469</td>\n",
       "      <td>0.545040</td>\n",
       "      <td>0.687073</td>\n",
       "      <td>0.552640</td>\n",
       "      <td>00:47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "sched_lr  = combine_scheds([0.3,0.7], cos_1cycle_anneal(lr/25., lr, lr/1e5))\n",
    "#sched_mom = combine_scheds([0.5,0.5], cos_1cycle_anneal(0.8, 0.7, 0.8))\n",
    "cbsched = [ParamScheduler('lr', sched_lr), ParamScheduler('mom', sched_mom)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.696424</td>\n",
       "      <td>0.532800</td>\n",
       "      <td>0.688015</td>\n",
       "      <td>0.545240</td>\n",
       "      <td>01:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(1, cbs=cbsched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(data.train_dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions should be the same in a batch with padding or one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_batch = learn.model.eval()(x.cuda())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ind = []\n",
    "for inp in x:\n",
    "    length = x.size(1) - (inp == 1).long().sum()\n",
    "    inp = inp[:length]\n",
    "    pred_ind.append(learn.model.eval()(inp[None].cuda())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(pred_batch, torch.cat(pred_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = learn.model.state_dict()\n",
    "wgts = torch.load(path/'init_clas.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0440, -0.0541], device='cuda:0')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd['1.layers.5.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0503, -0.0478])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wgts['1.layers.5.bias']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model[0].rnns[-1].parameters(): p.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "sched_lr  = combine_scheds([0.25,0.75], cos_1cycle_anneal(lr/10., lr, lr/1e5))\n",
    "sched_lr1  = combine_scheds([0.25,0.75], cos_1cycle_anneal(lr/20., lr/2, lr/2e5))\n",
    "sched_mom = combine_scheds([0.25,0.75], cos_1cycle_anneal(0.8, 0.7, 0.8))\n",
    "cbsched = [ParamScheduler('lr', [sched_lr1, sched_lr1, sched_lr1, sched_lr]), ParamScheduler('mom', sched_mom)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.684263</td>\n",
       "      <td>0.561458</td>\n",
       "      <td>0.684181</td>\n",
       "      <td>0.565600</td>\n",
       "      <td>00:56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(1, cbs=cbsched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model[0].rnns[-1].parameters(): p.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 5e-3\n",
    "sched_lr  = combine_scheds([0.25,0.75], cos_1cycle_anneal(lr/10., lr, lr/1e5))\n",
    "sched_lr1  = combine_scheds([0.25,0.75], cos_1cycle_anneal(lr/20., lr/2, lr/2e5))\n",
    "sched_mom = combine_scheds([0.25,0.75], cos_1cycle_anneal(0.8, 0.7, 0.8))\n",
    "cbsched = [ParamScheduler('lr', [sched_lr1, sched_lr1, sched_lr1, sched_lr]), ParamScheduler('mom', sched_mom)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.338730</td>\n",
       "      <td>0.859000</td>\n",
       "      <td>0.211006</td>\n",
       "      <td>0.918760</td>\n",
       "      <td>02:23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(1, cbs=cbsched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model[0].parameters(): p.requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "sched_lrs = [combine_scheds([0.25,0.75], cos_1cycle_anneal(lr/((2**i)*10.), lr/(2**i), lr/((2**i)*1e5))) for i in range(4)]\n",
    "sched_lrs.reverse()\n",
    "sched_mom = combine_scheds([0.25,0.75], cos_1cycle_anneal(0.8, 0.7, 0.8))\n",
    "cbsched = [ParamScheduler('lr', sched_lrs), ParamScheduler('mom', sched_mom)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.292223</td>\n",
       "      <td>0.882160</td>\n",
       "      <td>0.228467</td>\n",
       "      <td>0.911200</td>\n",
       "      <td>03:33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.269084</td>\n",
       "      <td>0.891840</td>\n",
       "      <td>0.191580</td>\n",
       "      <td>0.925920</td>\n",
       "      <td>03:33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit(2, cbs=cbsched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
