{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_11a import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = datasets.untar_data(datasets.URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/jhoward/.fastai/data/imdb/unsup'),\n",
       " PosixPath('/home/jhoward/.fastai/data/imdb/imdb.vocab'),\n",
       " PosixPath('/home/jhoward/.fastai/data/imdb/tmp_lm'),\n",
       " PosixPath('/home/jhoward/.fastai/data/imdb/train'),\n",
       " PosixPath('/home/jhoward/.fastai/data/imdb/test'),\n",
       " PosixPath('/home/jhoward/.fastai/data/imdb/README'),\n",
       " PosixPath('/home/jhoward/.fastai/data/imdb/tmp_clas')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An ItemList that will read the texts in the corresponding filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_file(fn): \n",
    "    with open(fn, 'r', encoding = 'utf8') as f: return f.read()\n",
    "    \n",
    "class TextList(ItemList):\n",
    "    @classmethod\n",
    "    def from_files(cls, path, extensions='.txt', recurse=True, include=None, **kwargs):\n",
    "        return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs)\n",
    "    \n",
    "    def get(self, i):\n",
    "        if isinstance(i, Path): return read_file(i)\n",
    "        return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "il = TextList.from_files(path, include=['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(il.items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text classification, we will split by the grand parent folder as before, but for language modeling, we take all the texts and just put 10% aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SplitData\n",
       "Train: TextList (89877 items)\n",
       "[PosixPath('/home/jhoward/.fastai/data/imdb/unsup/5003_0.txt'), PosixPath('/home/jhoward/.fastai/data/imdb/unsup/13043_0.txt'), PosixPath('/home/jhoward/.fastai/data/imdb/unsup/1072_0.txt'), PosixPath('/home/jhoward/.fastai/data/imdb/unsup/35637_0.txt'), PosixPath('/home/jhoward/.fastai/data/imdb/unsup/30019_0.txt'), PosixPath('/home/jhoward/.fastai/data/imdb/unsup/25190_0.txt'), PosixPath('/home/jhoward/.fastai/data/imdb/unsup/27746_0.txt'), PosixPath('/home/jhoward/.fastai/data/imdb/unsup/44322_0.txt'), PosixPath('/home/jhoward/.fastai/data/imdb/unsup/2255_0.txt'), PosixPath('/home/jhoward/.fastai/data/imdb/unsup/31544_0.txt')...]\n",
       "Path: /home/jhoward/.fastai/data/imdb\n",
       "Valid: TextList (10123 items)\n",
       "[PosixPath('/home/jhoward/.fastai/data/imdb/unsup/5762_0.txt'), PosixPath('/home/jhoward/.fastai/data/imdb/unsup/27507_0.txt'), PosixPath('/home/jhoward/.fastai/data/imdb/unsup/15508_0.txt'), PosixPath('/home/jhoward/.fastai/data/imdb/unsup/33657_0.txt'), PosixPath('/home/jhoward/.fastai/data/imdb/unsup/32691_0.txt'), PosixPath('/home/jhoward/.fastai/data/imdb/unsup/5120_0.txt'), PosixPath('/home/jhoward/.fastai/data/imdb/unsup/4831_0.txt'), PosixPath('/home/jhoward/.fastai/data/imdb/unsup/18568_0.txt'), PosixPath('/home/jhoward/.fastai/data/imdb/unsup/20612_0.txt'), PosixPath('/home/jhoward/.fastai/data/imdb/unsup/9685_0.txt')...]\n",
       "Path: /home/jhoward/.fastai/data/imdb"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tokenize the dataset first. We will use a processor for this, in conjunction with the [spacy library](https://spacy.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import spacy,html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before even tokenizeing, we will apply a bit of preprocessing on the texts to clean them up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "BOS, EOS, UNK, PAD, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxbox xxeos xxunk xxpad xxrep xxwrep xxup xxmaj\".split()\n",
    "\n",
    "def sub_br(t):\n",
    "    \"Replaces the <br /> by \\n\"\n",
    "    re_br = re.compile(r'<\\s*br\\s*/?>', re.IGNORECASE)\n",
    "    return re_br.sub(\"\\n\", t)\n",
    "\n",
    "def spec_add_spaces(t):\n",
    "    \"Add spaces between special characters\"\n",
    "    return re.sub(r'([/#])', r' \\1 ', t)\n",
    "\n",
    "def rm_useless_spaces(t):\n",
    "    \"Remove multiple spaces\"\n",
    "    return re.sub(' {2,}', ' ', t)\n",
    "\n",
    "def replace_rep(t):\n",
    "    \"Replace repetitions at the character level\"\n",
    "    def _replace_rep(m:Collection[str]) -> str:\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_REP} {len(cc)+1} {c} '\n",
    "    re_rep = re.compile(r'(\\S)(\\1{3,})')\n",
    "    return re_rep.sub(_replace_rep, t)\n",
    "    \n",
    "def replace_wrep(t):\n",
    "    \"Replace word repetitions\"\n",
    "    def _replace_wrep(m:Collection[str]) -> str:\n",
    "        c,cc = m.groups()\n",
    "        return f' {TK_WREP} {len(cc.split())+1} {c} '\n",
    "    re_wrep = re.compile(r'(\\b\\w+\\W+)(\\1{3,})')\n",
    "    return re_wrep.sub(_replace_wrep, t)\n",
    "\n",
    "def fixup_text(x):\n",
    "    \"Various messy things we've seen in documents\"\n",
    "    re1 = re.compile(r'  +')\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>',UNK).replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "    \n",
    "default_pre_rules = [fixup_text, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces, sub_br]\n",
    "default_spec_tok = [BOS, UNK, PAD, TK_REP, TK_WREP, TK_UP, TK_MAJ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_all_caps(x):\n",
    "    \"Replace tokens in ALL CAPS by their lower version and add `TK_UP` before.\"\n",
    "    res = []\n",
    "    for t in x:\n",
    "        if t.isupper() and len(t) > 1: res.append(TK_UP); res.append(t.lower())\n",
    "        else: res.append(t)\n",
    "    return res\n",
    "\n",
    "def deal_caps(x):\n",
    "    \"Replace all Capitalized tokens in by their lower version and add `TK_MAJ` before.\"\n",
    "    res = []\n",
    "    for t in x:\n",
    "        if t == '': continue\n",
    "        if t[0].isupper() and len(t) > 1 and t[1:].islower(): res.append(TK_MAJ)\n",
    "        res.append(t.lower())\n",
    "    return res\n",
    "\n",
    "def add_eos_bos(x): return [BOS] + x + [EOS]\n",
    "\n",
    "default_post_rules = [deal_caps, replace_all_caps, add_eos_bos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from spacy.symbols import ORTH\n",
    "\n",
    "class TokenizeProcessor(Processor):\n",
    "    def __init__(self, lang=\"en\", chunksize=5000, pre_rules=None, post_rules=None): \n",
    "        self.chunksize = chunksize\n",
    "        self.tokenizer = spacy.blank(lang).tokenizer\n",
    "        for w in default_spec_tok:\n",
    "            self.tokenizer.add_special_case(w, [{ORTH: w}])\n",
    "        self.pre_rules  = default_pre_rules  if pre_rules  is None else pre_rules\n",
    "        self.post_rules = default_post_rules if post_rules is None else post_rules\n",
    "    \n",
    "    def process(self, items): \n",
    "        toks = []\n",
    "        if isinstance(items[0], Path): items = [read_file(i) for i in items]\n",
    "        for i in progress_bar(range(0, len(items), self.chunksize)):\n",
    "            chunk = items[i: i+self.chunksize]\n",
    "            chunk = [compose(t, self.pre_rules) for t in chunk]\n",
    "            docs = [[d.text for d in doc] for doc in self.tokenizer.pipe(chunk)]\n",
    "            docs = [compose(t, self.post_rules) for t in docs]\n",
    "            toks += docs\n",
    "        return toks\n",
    "    \n",
    "    def proc1(self, item): \n",
    "        text = compose(item, self.pre_rules)\n",
    "        toks = list(self.tokenizer(text))\n",
    "        return compose(toks, self.post_rules)\n",
    "    \n",
    "    def deprocess(self, toks): return [self.deproc1(tok) for tok in toks]\n",
    "    def deproc1(self, tok):    return \" \".join(tok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numericalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import collections\n",
    "\n",
    "class NumericalizeProcessor(Processor):\n",
    "    def __init__(self, vocab=None, max_vocab=60000, min_freq=2): \n",
    "        self.vocab,self.max_vocab,self.min_freq = vocab,max_vocab,min_freq\n",
    "    \n",
    "    def process(self, items):\n",
    "        #The vocab is defined on the first use.\n",
    "        if self.vocab is None:\n",
    "            freq = Counter(p for o in items for p in o)\n",
    "            self.vocab = [o for o,c in freq.most_common(self.max_vocab) if c >= self.min_freq]\n",
    "            for o in reversed(default_spec_tok):\n",
    "                if o in self.vocab: self.vocab.remove(o)\n",
    "                self.vocab.insert(0, o)\n",
    "        self.otoi = collections.defaultdict(int,{v:k for k,v in enumerate(self.vocab)})  \n",
    "        return [self.proc1(o) for o in items]\n",
    "    def proc1(self, item):  return [self.otoi[o] for o in item]\n",
    "    \n",
    "    def deprocess(self, idxs):\n",
    "        assert self.vocab is not None\n",
    "        return [self.deproc1(idx) for idx in idxs]\n",
    "    def deproc1(self, idx): return [self.vocab[i] for i in idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do language modeling, we will infer the labels from the text during training, so there's no need to label. The training loop expects labels however, so we need to add dummy ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='18' class='' max='18', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [18/18 04:12<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='2' class='' max='2', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [2/2 00:28<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ll = label_by_func(sd, lambda x: 0, proc_x = [TokenizeProcessor(), NumericalizeProcessor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ll, open(path/'ld.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path/'ld.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a bit of work to convert our `LabelList` in a `DataBunch` as we don't just want batches of IMDB reviews. We want to stream through all the texts concatenated. We also have to prepare the targets that are the newt words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LanguageModelPreLoader():\n",
    "    def __init__(self, data, bs=64, bptt=70, shuffle=False):\n",
    "        self.data,self.bs,self.bptt,self.shuffle = data,bs,bptt,shuffle\n",
    "        total_len = sum([len(t) for t in data.x])\n",
    "        self.n_batch = total_len // bs\n",
    "        self.batchify()\n",
    "    \n",
    "    def __len__(self): return ((self.n_batch-1) // self.bptt) * self.bs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source = self.batched_data[idx % self.bs]\n",
    "        seq_idx = (idx // self.bs) * self.bptt\n",
    "        return source[seq_idx:seq_idx+self.bptt],source[seq_idx+1:seq_idx+self.bptt+1]\n",
    "    \n",
    "    def batchify(self):\n",
    "        texts = self.data.x\n",
    "        if self.shuffle: texts = texts[torch.randperm(len(texts))]\n",
    "        stream = torch.cat([tensor(t) for t in texts])\n",
    "        self.batched_data = stream[:self.n_batch * self.bs].view(self.bs, self.n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(LanguageModelPreLoader(ll.valid, shuffle=True), batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dl = iter(dl)\n",
    "x1,y1 = next(iter_dl)\n",
    "x2,y2 = next(iter_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 70]), torch.Size([64, 70]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.size(),y1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = ll.train.x.processors[1].vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxbox xxmaj when watching a xxmaj bug 's xxmaj life for the first time in a long while , i could n't help but see the comparisons with last year 's xxmaj happy xxmaj feet . xxmaj as far as the main storyline goes , they are very similar , an outcast doing what he can to fit in while also attempting to be special . xxmaj it just goes\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab[o] for o in x1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"to show you how much better that film could have been without its liberal diatribe conclusion . a lot of people disagree with me when i say that i really like xxmaj pixar 's sophomore effort . xxmaj sure it does n't manage to capture the splendor of xxmaj toy xxmaj story , nor is the animation out of this world . xxmaj however , the story is top -\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab[o] for o in x2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxmaj when watching a xxmaj bug 's xxmaj life for the first time in a long while , i could n't help but see the comparisons with last year 's xxmaj happy xxmaj feet . xxmaj as far as the main storyline goes , they are very similar , an outcast doing what he can to fit in while also attempting to be special . xxmaj it just goes to\""
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab[o] for o in y1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_lm_dls(train_ds, valid_ds, bs, bptt, **kwargs):\n",
    "    return (DataLoader(LanguageModelPreLoader(train_ds, bs, bptt, shuffle=True), batch_size=bs, **kwargs),\n",
    "            DataLoader(LanguageModelPreLoader(valid_ds, bs, bptt, shuffle=False), batch_size=2*bs, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def lm_databunchify(sd, bs, bptt, **kwargs):\n",
    "    dls = get_lm_dls(sd.train, sd.valid, bs, bptt, **kwargs)\n",
    "    return DataBunch(*dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,bptt = 64,70\n",
    "data = lm_databunchify(ll, bs, bptt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we will want to tackle classification, gathering the data will be a bit different: first we will label our texts with the folder they come from, and then we will need to apply padding to batch them together. To avoid mixing very long texts with very short ones, we will also use `Sampler` to sort (with a bit of randomness for the training set) our samples by length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 01:09<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='5' class='' max='5', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5/5 01:05<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "il = TextList.from_files(path, include=['train', 'test'])\n",
    "sd = SplitData.split_by_func(il, partial(grandparent_splitter, valid_name='test'))\n",
    "ll = label_by_func(sd, parent_labeler, proc_x = [TokenizeProcessor(), NumericalizeProcessor()], proc_y=CategoryProcessor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ll, open(path/'ll_clas.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path/'ll_clas.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw samplers in notebook 03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "class SortSampler(Sampler):\n",
    "    def __init__(self, data_source, key): self.data_source,self.key = data_source,key\n",
    "    def __len__(self): return len(self.data_source)\n",
    "    def __iter__(self):\n",
    "        return iter(sorted(list(range(len(self.data_source))), key=self.key, reverse=True))\n",
    "\n",
    "class SortishSampler(Sampler):\n",
    "    def __init__(self, data_source, key, bs):\n",
    "        self.data_source,self.key,self.bs = data_source,key,bs\n",
    "\n",
    "    def __len__(self) -> int: return len(self.data_source)\n",
    "\n",
    "    def __iter__(self):\n",
    "        idxs = torch.randperm(len(self.data_source))\n",
    "        megabatches = [idxs[i:i+self.bs*50] for i in range(0, len(idxs), self.bs*50)]\n",
    "        sorted_idx = torch.cat([tensor(sorted(s, key=self.key, reverse=True)) for s in megabatches])\n",
    "        batches = [sorted_idx[i:i+self.bs] for i in range(0, len(sorted_idx), self.bs)]\n",
    "        max_idx = torch.argmax(tensor([self.key(ck[0]) for ck in batches]))  # find the chunk with the largest key,\n",
    "        batches[0],batches[max_idx] = batches[max_idx],batches[0]            # then make sure it goes first.\n",
    "        batch_idxs = torch.randperm(len(batches)-2)\n",
    "        sorted_idx = torch.cat([batches[i+1] for i in batch_idxs]) if len(batches) > 1 else LongTensor([])\n",
    "        sorted_idx = torch.cat([batches[0], sorted_idx, batches[-1]])\n",
    "        return iter(sorted_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pad_collate(samples, pad_idx=1, pad_first=False):\n",
    "    max_len = max([len(s[0]) for s in samples])\n",
    "    res = torch.zeros(len(samples), max_len).long() + pad_idx\n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: res[i,-len(s[0]):] = LongTensor(s[0])\n",
    "        else:         res[i,:len(s[0]):] = LongTensor(s[0])\n",
    "    return res, tensor([s[1] for s in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = SortishSampler(ll.train.x, key=lambda t: len(ll.train[int(t)][0]), bs=bs)\n",
    "train_dl = DataLoader(ll.train, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dl = iter(train_dl)\n",
    "x,y = next(iter_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3311])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_clas_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    train_sampler = SortishSampler(train_ds.x, key=lambda t: len(train_ds.x[t]), bs=bs)\n",
    "    valid_sampler = SortSampler(valid_ds.x, key=lambda t: len(valid_ds.x[t]))\n",
    "    return (DataLoader(train_ds, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=bs*2, sampler=valid_sampler, collate_fn=pad_collate, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def clas_databunchify(sd, bs, **kwargs):\n",
    "    dls = get_clas_dls(sd.train, sd.valid, bs, **kwargs)\n",
    "    return DataBunch(*dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,bptt = 64,70\n",
    "data = clas_databunchify(ll, bs, bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dl = iter(data.train_dl)\n",
    "x,y = next(iter_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dl = iter(data.valid_dl)\n",
    "x,y = next(iter_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 12_text.ipynb to exp/nb_12.py\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 12_text.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
