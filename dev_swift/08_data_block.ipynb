{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data block foundations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/home/ubuntu/fastai_docs/dev_swift/FastaiNotebook_07_batchnorm\")\n",
      "\t\tFastaiNotebook_07_batchnorm\n",
      "With SwiftPM flags: []\n",
      "Working in: /tmp/tmpu1owybh9/swift-install\n",
      "Compile Swift Module 'FastaiNotebook_07_batchnorm' (11 sources)\n",
      "Compile Swift Module 'jupyterInstalledPackages' (1 sources)\n",
      "Linking ./.build/x86_64-unknown-linux/debug/libjupyterInstalledPackages.so\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install-location $cwd/swift-install\n",
    "%install '.package(path: \"$cwd/FastaiNotebook_07_batchnorm\")' FastaiNotebook_07_batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "import Path\n",
    "import TensorFlow\n",
    "import Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebook_07_batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('inline', 'module://ipykernel.pylab.backend_inline')\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image ItemList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Imagenette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public let dataPath = Path.home/\".fastai\"/\"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public func downloadImagette(path: Path = dataPath) -> Path {\n",
    "    let url = \"https://s3.amazonaws.com/fast-ai-imageclas/imagenette-160.tgz\"\n",
    "    let fname = \"imagenette-160\"\n",
    "    let file = path/fname\n",
    "    try! path.mkdir(.p)\n",
    "    if !file.exists {\n",
    "        downloadFile(url, dest:(path/\"\\(fname).tgz\").string)\n",
    "        _ = shellCommand(\"/bin/tar\", [\"-xzf\", (path/\"\\(fname).tgz\").string, \"-C\", path.string])\n",
    "    }\n",
    "    return file\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let path = downloadImagette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(path.ls())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((path/\"val\").ls())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look inside a class folder (the first class is tench):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let pathTench = path/\"val\"/\"n01440764\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let imgFn = Path.home/\".fastai/data/imagenette-160/val/n01440764/ILSVRC2012_val_00006697.JPEG\"\n",
    "imgFn.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let imgBytes = Raw.readFile(filename: StringTensor(imgFn.string))\n",
    "let decodedImg = Raw.decodeJpeg(contents: imgBytes, channels: 3, dctMethod: \"\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decodedImg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let numpyImg = decodedImg.makeNumpyArray()\n",
    "plt.imshow(numpyImg) \n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public func fetchFiles(path: Path, recurse: Bool = false, extensions: [String]? = nil) -> [Path]{\n",
    "    var res: [Path] = []\n",
    "    for p in try! path.ls(){\n",
    "        if p.kind == .directory && recurse { \n",
    "            res += fetchFiles(path: p.path, recurse: recurse, extensions: extensions)\n",
    "        } else if extensions == nil || extensions!.contains(p.path.extension.lowercased) {\n",
    "            res.append(p.path)\n",
    "        }\n",
    "    }\n",
    "    return res\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time() { let fNames = fetchFiles(path: path, recurse: true, extensions: [\"jpeg\", \"jpg\"]) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let fNames = fetchFiles(path: path, recurse: true, extensions: [\"jpeg\", \"jpg\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fNames.count == 13394"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Dataset` can handle all the transforms that go on a `Tensor`, including opening an image and resizing it since it takes `StringTensor`. That makes the `tfms` attribute of `ItemList` irrelevant, so `ItemList` is just an array of `Item` with a path (if get method seems useful later, we can add it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public struct ItemList<Item>{\n",
    "    public var items: [Item]\n",
    "    public let path: Path\n",
    "    \n",
    "    public init(items: [Item], path: Path){\n",
    "        (self.items,self.path) = (items,path)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public extension ItemList where Item == Path{\n",
    "    init(fromFolder path: Path, extensions: [String], recurse: Bool = true) {\n",
    "        self.init(items: fetchFiles(path: path, recurse: recurse, extensions: extensions),\n",
    "                  path:  path)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let il = ItemList(fromFolder: path, extensions: [\"jpeg\", \"jpg\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public struct SplitData<Item>{\n",
    "    public let train: ItemList<Item>\n",
    "    public let valid: ItemList<Item>\n",
    "    public var path: Path { return train.path }\n",
    "    \n",
    "    public init(train: ItemList<Item>, valid: ItemList<Item>){\n",
    "        (self.train, self.valid) = (train, valid)\n",
    "    }\n",
    "    \n",
    "    public init(_ il: ItemList<Item>, fromFunc: (Item) -> Bool){\n",
    "        var (trn, val): ([Item], [Item]) = ([], [])\n",
    "        for x in il.items {\n",
    "            if fromFunc(x) { val.append(x) }\n",
    "            else           { trn.append(x) }\n",
    "        }\n",
    "        self.init(train: ItemList(items: trn, path: il.path),\n",
    "                  valid: ItemList(items: val, path: il.path))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func grandParentSplitter(fName: Path, valid: String = \"valid\") -> Bool{\n",
    "    return fName.parent.parent.basename() == valid\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let sd = SplitData(il, fromFunc: {grandParentSplitter(fName: $0, valid: \"val\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public protocol Processor {\n",
    "    associatedtype Input\n",
    "    associatedtype Output\n",
    "    \n",
    "    mutating func initState(items: [Input])\n",
    "    func process1(item: Input) -> Output\n",
    "    func deprocess1(item: Output) -> Input\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public extension Processor {\n",
    "    \n",
    "    func process(items: [Input]) -> [Output]{\n",
    "        return items.map(){process1(item: $0)}\n",
    "    }\n",
    "    \n",
    "    func deprocess(items: [Output]) -> [Input]{\n",
    "        return items.map(){deprocess1(item: $0)}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public struct NoopProcessor<Item>: Processor {\n",
    "    public init() {}\n",
    "    public typealias Input = Item\n",
    "    public typealias Output = Item\n",
    "    \n",
    "    public mutating func initState(items: [Input]){}\n",
    "    \n",
    "    public func process1  (item: Input)  -> Output { return item }\n",
    "    public func deprocess1(item: Output) -> Input  { return item }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public struct CategoryProcessor: Processor {\n",
    "    public init() {}\n",
    "    public typealias Input = String\n",
    "    public typealias Output = Int32\n",
    "    public var vocab: [Input]? = nil\n",
    "    public var reverseMap: [Input: Output]? = nil\n",
    "    \n",
    "    public mutating func initState(items: [Input]){\n",
    "        vocab = Array(Set(items)).sorted()\n",
    "        reverseMap = [:]\n",
    "        for (i,x) in vocab!.enumerated(){ reverseMap![x] = Int32(i) }\n",
    "    }\n",
    "    \n",
    "    public func process1  (item: Input)  -> Output { return reverseMap![item]! }\n",
    "    public func deprocess1(item: Output) -> Input  { return vocab![Int(item)] }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we build the datasets, we don't need to return a tupe (item, label) but to have the tensor(s) with the items and the tensor(s) with the labels separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public struct LabeledItemList<PI,PL> where PI: Processor, PL: Processor{\n",
    "    public var items: [PI.Output]\n",
    "    public var labels: [PL.Output]\n",
    "    public let path: Path\n",
    "    public var procItem: PI\n",
    "    public var procLabel: PL\n",
    "    \n",
    "    public init(rawItems: [PI.Input], rawLabels: [PL.Input], path: Path, procItem: PI, procLabel: PL){\n",
    "        (self.procItem,self.procLabel,self.path) = (procItem,procLabel,path)\n",
    "        self.items = procItem.process(items: rawItems)\n",
    "        self.labels = procLabel.process(items: rawLabels)\n",
    "    }\n",
    "    \n",
    "    public init(_ il: ItemList<PI.Input>, fromFunc: (PI.Input) -> PL.Input, procItem: PI, procLabel: PL){\n",
    "        self.init(rawItems:  il.items,\n",
    "                  rawLabels: il.items.map{ fromFunc($0)},\n",
    "                  path:      il.path,\n",
    "                  procItem:  procItem,\n",
    "                  procLabel: procLabel)\n",
    "    }\n",
    "    \n",
    "    public func rawItem (_ idx: Int) -> PI.Input { return procItem.deprocess1 (item: items[idx])  }\n",
    "    public func rawLabel(_ idx: Int) -> PL.Input { return procLabel.deprocess1(item: labels[idx]) }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public struct SplitLabeledData<PI,PL> where PI: Processor, PL: Processor{\n",
    "    public let train: LabeledItemList<PI,PL>\n",
    "    public let valid: LabeledItemList<PI,PL>\n",
    "    public var path: Path { return train.path }\n",
    "    \n",
    "    public init(train: LabeledItemList<PI,PL>, valid: LabeledItemList<PI,PL>){\n",
    "        (self.train, self.valid) = (train, valid)\n",
    "    }\n",
    "    \n",
    "    public init(_ sd: SplitData<PI.Input>, fromFunc: (PI.Input) -> PL.Input, procItem: inout PI, procLabel: inout PL){\n",
    "        procItem.initState (items: sd.train.items)\n",
    "        let trainLabels = sd.train.items.map{ fromFunc($0) }\n",
    "        procLabel.initState(items: trainLabels)\n",
    "        self.init(train: LabeledItemList(rawItems: sd.train.items, rawLabels: trainLabels, path: sd.path, \n",
    "                                         procItem: procItem, procLabel: procLabel),\n",
    "                  valid: LabeledItemList(sd.valid, fromFunc: fromFunc, procItem: procItem, procLabel: procLabel))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public func parentLabeler(_ fName: Path) -> String { return fName.parent.basename() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var (procItem,procLabel) = (NoopProcessor<Path>(),CategoryProcessor())\n",
    "let sld = SplitLabeledData(sd, fromFunc: parentLabeler, procItem: &procItem, procLabel: &procLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sld.train.labels[0])\n",
    "print(sld.train.rawLabel(0))\n",
    "print(sld.train.procLabel.vocab!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To go in a Dataset, our array of items and array of labels need to be converted to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public struct LabeledElement<I: TensorGroup, L: TensorGroup>: TensorGroup {\n",
    "    public var xb: I\n",
    "    public var yb: L    \n",
    "    \n",
    "    public init(xb: I, yb: L){\n",
    "        (self.xb, self.yb) = (xb, yb)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public extension SplitLabeledData {\n",
    "    func toDataBunch<XB, YB> (\n",
    "        itemToTensor: ([PI.Output]) -> XB, labelToTensor: ([PL.Output]) -> YB, bs: Int = 64\n",
    "    ) -> DataBunch<LabeledElement<XB, YB>> where XB: TensorGroup, YB: TensorGroup {\n",
    "        let trainDs = Dataset<LabeledElement<XB, YB>>(\n",
    "            elements: LabeledElement(xb: itemToTensor(train.items), yb: labelToTensor(train.labels)))\n",
    "        let validDs = Dataset<LabeledElement<XB, YB>>(\n",
    "            elements: LabeledElement(xb: itemToTensor(valid.items), yb: labelToTensor(valid.labels)))\n",
    "        return DataBunch(train: trainDs, \n",
    "                         valid: validDs, \n",
    "                         trainLen: train.items.count, \n",
    "                         validLen: valid.items.count,\n",
    "                         bs: bs)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func pathsToTensor(_ paths: [Path]) -> StringTensor { return StringTensor(paths.map{ $0.string })}\n",
    "public func intsToTensor(_ items: [Int32]) -> Tensor<Int32> { return Tensor<Int32>(items)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let dataset = sld.toDataBunch(itemToTensor: pathsToTensor, labelToTensor: intsToTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We directly plug in to the dataset the transforms we want to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func transformData<I,TI,L>(\n",
    "    _ data: DataBunch<LabeledElement<I,L>>, \n",
    "    tfmItem: (I) -> TI\n",
    ") -> DataBunch<DataBatch<TI,L>> \n",
    "where I: TensorGroup, TI: TensorGroup & Differentiable, L: TensorGroup{\n",
    "    return DataBunch(train: data.train.innerDs.map{ DataBatch(xb: tfmItem($0.xb), yb: $0.yb) },\n",
    "                     valid: data.valid.innerDs.map{ DataBatch(xb: tfmItem($0.xb), yb: $0.yb) },\n",
    "                     trainLen: data.train.dsCount, \n",
    "                     validLen: data.valid.dsCount,\n",
    "                     bs: data.train.bs)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func openAndResize(fname: StringTensor, size: Int) -> TF{\n",
    "    let imgBytes = Raw.readFile(filename: fname)\n",
    "    let decodedImg = Raw.decodeJpeg(contents: imgBytes, channels: 3, dctMethod: \"\")\n",
    "    let resizedImg = Tensor<Float>(Raw.resizeNearestNeighbor(\n",
    "        images: Tensor<UInt8>([decodedImg]), \n",
    "        size: Tensor<Int32>([Int32(size), Int32(size)]))) / 255.0\n",
    "    return resizedImg.reshaped(to: TensorShape(size, size, 3))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let tfmData = transformData(dataset, tfmItem: { openAndResize(fname: $0, size: 128) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var firstBatch: DataBatch<TF, TI>? = nil\n",
    "for batch in tfmData.train.ds {\n",
    "    firstBatch = batch\n",
    "    break\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firstBatch!.xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (rows,cols) = (3,3)\n",
    "plt.figure(figsize: [9, 9])\n",
    "for i in 0..<(rows * cols) {\n",
    "    let img = plt.subplot(rows, cols, i + 1)\n",
    "    img.axis(\"off\")\n",
    "    let x = firstBatch!.xb[i].makeNumpyArray()\n",
    "    img.imshow(x)\n",
    "    let title = sld.train.procLabel.vocab![Int(firstBatch!.yb[i].scalarized())]\n",
    "    img.set_title(title)\n",
    "    if (i + 1) >= (rows * cols) { break }\n",
    "}\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To summarize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let il = ItemList(fromFolder: path, extensions: [\"jpeg\", \"jpg\"])\n",
    "let sd = SplitData(il, fromFunc: {grandParentSplitter(fName: $0, valid: \"val\")})\n",
    "var (procItem,procLabel) = (NoopProcessor<Path>(),CategoryProcessor())\n",
    "let sld = SplitLabeledData(sd, fromFunc: parentLabeler, procItem: &procItem, procLabel: &procLabel)\n",
    "var rawData = sld.toDataBunch(itemToTensor: pathsToTensor, labelToTensor: intsToTensor, bs: 16)\n",
    "let data = transformData(rawData, tfmItem: { openAndResize(fname: $0, size: 128) })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export \n",
    "public let imagenetStats = (mean: TF([0.485, 0.456, 0.406]), std: TF([0.229, 0.224, 0.225]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func optFunc(_ model: CnnModelBN) -> SGD<CnnModelBN> { return SGD(for: model, learningRate: 0.1) }\n",
    "func modelInit() -> CnnModelBN { return CnnModelBN(channelIn: 3, nOut: 10, filters: [8, 16, 32, 64, 128]) }\n",
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.addDelegate(learner.makeNormalize(mean: imagenetStats.mean, std: imagenetStats.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public func prevPow2(_ x: Int) -> Int { \n",
    "    var res = 1\n",
    "    while res <= x { res *= 2 }\n",
    "    return res / 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public struct CNNModel: Layer {\n",
    "    public var convs: [ConvBN<Float>]\n",
    "    public var pool = FAGlobalAvgPool2D<Float>()\n",
    "    public var linear: FADense<Float>\n",
    "    \n",
    "    public init(channelIn: Int, nOut: Int, filters: [Int]){\n",
    "        convs = []\n",
    "        let (l1,l2) = (channelIn, prevPow2(channelIn * 9))\n",
    "        convs = [ConvBN(l1,   l2,   stride: 1),\n",
    "                 ConvBN(l2,   l2*2, stride: 2),\n",
    "                 ConvBN(l2*2, l2*4, stride: 2)]\n",
    "        let allFilters = [l2*4] + filters\n",
    "        for i in 0..<filters.count { convs.append(ConvBN(allFilters[i], allFilters[i+1])) }\n",
    "        linear = FADense<Float>(inputSize: filters.last!, outputSize: nOut)\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func call(_ input: TF) -> TF {\n",
    "        return linear(pool(convs(input)))\n",
    "                                         //.sequenced(through: convs, pool, linear)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func optFunc(_ model: CNNModel) -> SGD<CNNModel> { return SGD(for: model, learningRate: 0.1) }\n",
    "func modelInit() -> CNNModel { return CNNModel(channelIn: 3, nOut: 10, filters: [64, 64, 128, 256]) }\n",
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.addDelegate(learner.makeNormalize(mean: imagenetStats.mean, std: imagenetStats.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebookToScript(fname: Path.cwd / \"08_data_block.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
