{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/home/ubuntu/fastai_docs/dev_swift/FastaiNotebook_03_minibatch_training\")\n",
      "\t\tFastaiNotebook_03_minibatch_training\n",
      "With SwiftPM flags: []\n",
      "Working in: /tmp/tmpx4vzabsi/swift-install\n",
      "Fetching https://github.com/mxcl/Path.swift\n",
      "Fetching https://github.com/JustHTTP/Just\n",
      "Fetching https://github.com/latenitesoft/NotebookExport\n",
      "Completed resolution in 2.54s\n",
      "Cloning https://github.com/JustHTTP/Just\n",
      "Resolving https://github.com/JustHTTP/Just at 0.7.1\n",
      "Cloning https://github.com/mxcl/Path.swift\n",
      "Resolving https://github.com/mxcl/Path.swift at 0.16.2\n",
      "Cloning https://github.com/latenitesoft/NotebookExport\n",
      "Resolving https://github.com/latenitesoft/NotebookExport at fastai\n",
      "Compile Swift Module 'Just' (1 sources)\n",
      "Compile Swift Module 'Path' (9 sources)\n",
      "Compile Swift Module 'NotebookExport' (2 sources)\n",
      "Compile Swift Module 'FastaiNotebook_00_load_data' (1 sources)\n",
      "Compile Swift Module 'FastaiNotebook_01_matmul' (2 sources)\n",
      "Compile Swift Module 'FastaiNotebook_01a_fastai_layers' (3 sources)\n",
      "Compile Swift Module 'FastaiNotebook_02_fully_connected' (4 sources)\n",
      "Compile Swift Module 'FastaiNotebook_02a_why_sqrt5' (5 sources)\n",
      "Compile Swift Module 'FastaiNotebook_03_minibatch_training' (6 sources)\n",
      "Compile Swift Module 'jupyterInstalledPackages' (1 sources)\n",
      "Linking ./.build/x86_64-unknown-linux/debug/libjupyterInstalledPackages.so\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install-location $cwd/swift-install\n",
    "%install '.package(path: \"$cwd/FastaiNotebook_03_minibatch_training\")' FastaiNotebook_03_minibatch_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "import Path\n",
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebook_03_minibatch_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var (xTrain,yTrain,xValid,yValid) = loadMNIST(path: mnistPath, flat: true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 784 10\r\n"
     ]
    }
   ],
   "source": [
    "let (n,m) = (xTrain.shape[0],xTrain.shape[1])\n",
    "let c = yTrain.max().scalarized()+1\n",
    "print(n,m,c)\n",
    "let nHid = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public struct BasicModel: Layer {\n",
    "    public var layer1: FADense<Float>\n",
    "    public var layer2: FADense<Float>\n",
    "    \n",
    "    public init(nIn: Int, nHid: Int, nOut: Int){\n",
    "        layer1 = FADense(nIn, nHid, activation: relu)\n",
    "        layer2 = FADense(nHid, nOut)\n",
    "    }\n",
    "    \n",
    "    @differentiable\n",
    "    public func call(_ input: Tensor<Float>) -> Tensor<Float> {\n",
    "        return input.sequenced(through: layer1, layer2)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var model: [FADense<Float>] = [\n",
    "    FADense(m, nHid, activation: relu),\n",
    "    FADense(nHid, Int(c))] // BasicModel(nIn: m, nHid: nHid, nOut: Int(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export \n",
    "public struct FADataset<Element> where Element: TensorGroup{\n",
    "    public var innerDs: Dataset<Element>\n",
    "    public var shuffle: Bool = false\n",
    "    public var bs: Int = 64 \n",
    "    public var dsCount: Int\n",
    "    \n",
    "    public var count: Int {\n",
    "        return dsCount%bs == 0 ? dsCount/bs : dsCount/bs+1\n",
    "    }\n",
    "    \n",
    "    public var ds: Dataset<Element> { \n",
    "        if !shuffle { return innerDs.batched(bs)}\n",
    "        let seed = Int64.random(in: Int64.min..<Int64.max)\n",
    "        return innerDs.shuffled(sampleCount: dsCount, randomSeed: seed).batched(bs)\n",
    "    }\n",
    "    \n",
    "    public init(_ ds: Dataset<Element>, len: Int, shuffle: Bool = false, bs: Int = 64){\n",
    "        (self.innerDs,self.dsCount,self.shuffle,self.bs) = (ds, len, shuffle, bs)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public struct DataBunch<Element> where Element: TensorGroup{\n",
    "    public var train: FADataset<Element>\n",
    "    public var valid: FADataset<Element> \n",
    "    \n",
    "    public init(train: Dataset<Element>, valid: Dataset<Element>, trainLen: Int, validLen: Int,  bs: Int = 64) {\n",
    "        self.train = FADataset(train, len: trainLen, shuffle: true,  bs: bs)\n",
    "        self.valid = FADataset(valid, len: validLen, shuffle: false, bs: 2*bs)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public func mnistDataBunch(path: Path = mnistPath, flat: Bool = false, bs: Int = 64\n",
    "                          ) -> DataBunch<DataBatch<TF, TI>>{\n",
    "    let (xTrain,yTrain,xValid,yValid) = loadMNIST(path: path, flat: flat)\n",
    "    return DataBunch(train: Dataset(elements:DataBatch(xb:xTrain, yb:yTrain)), \n",
    "                     valid: Dataset(elements:DataBatch(xb:xValid, yb:yValid)),\n",
    "                     trainLen: xTrain.shape[0],\n",
    "                     validLen: xValid.shape[0],\n",
    "                     bs: bs)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let data = mnistDataBunch(flat: true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average: 242.2049787 ms,   min: 231.357586 ms,   max: 335.178255 ms\r\n"
     ]
    }
   ],
   "source": [
    "time(repeating: 10) {\n",
    "  var tst = data.train.ds\n",
    "  var firstBatch: DataBatch<TF,TI>? = nil\n",
    "  for batch in tst{\n",
    "      firstBatch = batch\n",
    "      break\n",
    "  }\n",
    "  firstBatch!.yb\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 5, 4, 7, 6, 7, 3, 2, 2, 7, 2, 0, 8, 5, 7, 2, 0, 7, 4, 5, 5, 4, 1, 5, 8, 1, 3, 4, 4, 7, 7, 2, 7, 5, 9, 6, 3, 2, 5, 3, 8, 0, 2, 6, 7, 4, 4, 5, 8, 6, 9, 1, 3, 1, 4, 7, 3, 9, 1, 4, 5, 3]\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var tst = data.train.ds\n",
    "var firstBatch: DataBatch<TF, TI>? = nil\n",
    "for batch in tst{\n",
    "    firstBatch = batch\n",
    "    break\n",
    "}\n",
    "firstBatch!.yb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public enum LearnerAction: Error {\n",
    "    case skipEpoch\n",
    "    case skipBatch\n",
    "    case stop\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "/// A model learner, responsible for initializing and training a model on a given dataset.\n",
    "public final class Learner<Label: TensorGroup,\n",
    "                           Opt: TensorFlow.Optimizer & AnyObject>\n",
    "    where Opt.Scalar: Differentiable,\n",
    "          Opt.Model: Layer,\n",
    "          // Constrain model input to Tensor<Float>, to work around\n",
    "          // https://forums.fast.ai/t/fix-ad-crash-in-learner/42970.\n",
    "          Opt.Model.Input == Tensor<Float>\n",
    "{\n",
    "    // Common type aliases.\n",
    "    public typealias Model = Opt.Model\n",
    "    public typealias Input = Model.Input\n",
    "    public typealias Output = Model.Output\n",
    "    public typealias Data = DataBunch<DataBatch<Input, Label>>\n",
    "    public typealias Loss = Tensor<Float>\n",
    "    public typealias Optimizer = Opt\n",
    "    public typealias Variables = Model.AllDifferentiableVariables\n",
    "    public typealias EventHandler = (Learner) throws -> Void\n",
    "    \n",
    "    /// A wrapper class to hold the loss function, to work around\n",
    "    // https://forums.fast.ai/t/fix-ad-crash-in-learner/42970.\n",
    "    public final class LossFunction {\n",
    "        public typealias F = @differentiable (Model.Output, @nondiff Label) -> Loss\n",
    "        public var f: F\n",
    "        init(_ f: @escaping F) { self.f = f }\n",
    "    }\n",
    "    \n",
    "    /// The dataset on which the model will be trained.\n",
    "    public var data: Data\n",
    "    /// The optimizer used for updating model parameters along gradient vectors.\n",
    "    public var opt: Optimizer\n",
    "    /// The function that computes a loss value when given a prediction and a label.\n",
    "    public var lossFunc: LossFunction\n",
    "    /// The model being trained.\n",
    "    public var model: Model\n",
    "    \n",
    "    //Is there a better way to initialize those to not make them Optionals?\n",
    "    public var currentInput: Input!\n",
    "    public var currentTarget: Label!\n",
    "    public var currentOutput: Output!\n",
    "    \n",
    "    /// The number of total epochs.\n",
    "    public private(set) var epochCount: Int = .zero\n",
    "    /// The current epoch.\n",
    "    public private(set) var currentEpoch: Int = .zero\n",
    "    /// The current gradient.\n",
    "    public private(set) var currentGradient: Model.CotangentVector = .zero\n",
    "    /// The current loss.\n",
    "    public private(set) var currentLoss: Loss = .zero\n",
    "    /// In training mode or not\n",
    "    public private(set) var inTrain: Bool = false\n",
    "    /// The current epoch + iteration, float between 0.0 and epochCount\n",
    "    public private(set) var pctEpochs: Float = 0.0\n",
    "    /// The current iteration\n",
    "    public private(set) var currentIter: Int = 0\n",
    "    /// The number of iterations in the current dataset\n",
    "    public private(set) var iterCount: Int = 0\n",
    "    \n",
    "    open class Delegate {\n",
    "        open var order: Int { return 0 }\n",
    "        public init () {}\n",
    "        \n",
    "        open func trainingWillStart(learner: Learner) throws {}\n",
    "        /// The completion of model training.\n",
    "        open func trainingDidFinish(learner: Learner) throws {}\n",
    "        /// A closure which will be called upon the start of an epoch.\n",
    "        open func epochWillStart(learner: Learner) throws {}\n",
    "        /// A closure which will be called upon the completion of an epoch.\n",
    "        open func epochDidFinish(learner: Learner) throws {}\n",
    "        /// A closure which will be called upon the start of model validation.\n",
    "        open func validationWillStart(learner: Learner) throws {}\n",
    "        /// A closure which will be called upon the start of training on a batch.\n",
    "        open func batchWillStart(learner: Learner) throws {}\n",
    "        /// A closure which will be called upon the completion of training on a batch.\n",
    "        open func batchDidFinish(learner: Learner) throws {}\n",
    "        /// A closure which will be called when a new gradient has been computed.\n",
    "        open func didProduceNewGradient(learner: Learner) throws {}\n",
    "        /// A closure which will be called upon the completion of an optimizer update.\n",
    "        open func optimizerDidUpdate(learner: Learner) throws {}\n",
    "        ///\n",
    "        /// TODO: learnerDidProduceNewOutput and learnerDidProduceNewLoss need to\n",
    "        /// be differentiable once we can have the loss function inside the Learner\n",
    "    }\n",
    "    \n",
    "    public var delegates: [Delegate] = [] {\n",
    "        didSet { delegates.sort { $0.order < $1.order } }\n",
    "    }\n",
    "    \n",
    "    /// Creates a learner.\n",
    "    ///\n",
    "    /// - Parameters:\n",
    "    ///   - data: The databunch used for training and validation.\n",
    "    ///   - lossFunction: The loss function.\n",
    "    ///   - optimizer: The optimizer used for updating model parameters.\n",
    "    ///   - modelInitializer: The closure that produces the model to be trained.\n",
    "    public init(data: Data,\n",
    "                lossFunc: @escaping LossFunction.F,\n",
    "                optFunc: (Model) -> Optimizer,\n",
    "                modelInit: () -> Model) {\n",
    "        self.data = data\n",
    "        self.lossFunc = LossFunction(lossFunc)\n",
    "        model = modelInit()\n",
    "        opt = optFunc(self.model)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's write the parts of the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "extension Learner {\n",
    "    /// Trains the model on the given batch.\n",
    "    ///\n",
    "    /// - Parameter batch: The batch of input data and labels to be trained on.\n",
    "    ///\n",
    "    private func evaluate(onBatch batch: DataBatch<Input, Label>) throws {\n",
    "        currentOutput = model(currentInput)\n",
    "        currentLoss = lossFunc.f(currentOutput, currentTarget)\n",
    "    }\n",
    "    \n",
    "    private func train(onBatch batch: DataBatch<Input, Label>) throws {\n",
    "        let (xb,yb) = (currentInput!,currentTarget!)\n",
    "        (currentLoss, currentGradient) = model.valueWithGradient { model -> Loss in \n",
    "            let y = model(xb)                                      \n",
    "            currentOutput = y\n",
    "            return lossFunc.f(y, yb)\n",
    "        }\n",
    "        try delegates.forEach { try $0.didProduceNewGradient(learner: self) }\n",
    "        opt.update(&model.allDifferentiableVariables, along: self.currentGradient)\n",
    "    }\n",
    "    \n",
    "    /// Performs a training epoch on a Dataset.\n",
    "    private func train(onDataset ds: FADataset<DataBatch<Input, Label>>) throws {\n",
    "        iterCount = ds.count\n",
    "        for batch in ds.ds {\n",
    "            (currentInput, currentTarget) = (batch.xb, batch.yb)\n",
    "            try delegates.forEach { try $0.batchWillStart(learner: self) }\n",
    "            do { if inTrain { try train(onBatch: batch) } else { try evaluate(onBatch: batch) }}\n",
    "            catch LearnerAction.skipBatch {}\n",
    "            try delegates.forEach { try $0.batchDidFinish(learner: self) }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the whole fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "extension Learner {\n",
    "    /// Starts fitting.\n",
    "    /// - Parameter epochCount: The number of epochs that will be run.\n",
    "    public func fit(_ epochCount: Int) throws {\n",
    "        self.epochCount = epochCount\n",
    "        do {\n",
    "            try delegates.forEach { try $0.trainingWillStart(learner: self) }\n",
    "            for i in 0..<epochCount {\n",
    "                self.currentEpoch = i\n",
    "                do {\n",
    "                    try delegates.forEach { try $0.epochWillStart(learner: self) }\n",
    "                    do { try train(onDataset: data.train) }\n",
    "                    try delegates.forEach { try $0.validationWillStart(learner: self) }\n",
    "                    do { try train(onDataset: data.valid) }\n",
    "                    \n",
    "                } catch LearnerAction.skipEpoch {}\n",
    "                try delegates.forEach { try $0.epochDidFinish(learner: self) }\n",
    "            }\n",
    "        } catch LearnerAction.stop {}\n",
    "        try delegates.forEach { try $0.trainingDidFinish(learner: self) }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func optFunc(_ model: BasicModel) ->  SGD<BasicModel> { return SGD(for: model, learningRate: 1e-2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func modelInit() -> BasicModel {return BasicModel(nIn: m, nHid: nHid, nOut: Int(c))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's add Callbacks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extension with convenience methods to add delegates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public extension Learner {\n",
    "    func addDelegate(_ delegate: Learner.Delegate) {\n",
    "        delegates.append(delegate)\n",
    "    }\n",
    "    \n",
    "    func addDelegates(_ delegates: [Learner.Delegate]) {\n",
    "        self.delegates += delegates\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callback classes are defined as extensions of the Learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "extension Learner {\n",
    "    public class TrainEvalDelegate: Delegate {\n",
    "        public override func trainingWillStart(learner: Learner) {\n",
    "            learner.pctEpochs = 0.0\n",
    "        }\n",
    "\n",
    "        public override func epochWillStart(learner: Learner) {\n",
    "            Context.local.learningPhase = .training\n",
    "            learner.pctEpochs = Float(learner.currentEpoch)\n",
    "            learner.inTrain = true\n",
    "            learner.currentIter = 0\n",
    "        }\n",
    "        \n",
    "        public override func batchDidFinish(learner: Learner) {\n",
    "            learner.currentIter += 1\n",
    "            if learner.inTrain{ learner.pctEpochs += 1.0 / Float(learner.iterCount) }\n",
    "        }\n",
    "        \n",
    "        public override func validationWillStart(learner: Learner) {\n",
    "            Context.local.learningPhase = .inference\n",
    "            learner.inTrain = false\n",
    "            learner.currentIter = 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    public func makeTrainEvalDelegate() -> TrainEvalDelegate { return TrainEvalDelegate() }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.delegates = [learner.makeTrainEvalDelegate()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AverageMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "extension Learner {\n",
    "    public class AvgMetric: Delegate {\n",
    "        public let metrics: [(Output, Label) -> TF]\n",
    "        var total: Int = 0\n",
    "        var partials: [TF] = []\n",
    "        \n",
    "        public init(metrics: [(Output, Label) -> TF]){ self.metrics = metrics}\n",
    "        \n",
    "        public override func epochWillStart(learner: Learner) {\n",
    "            total = 0\n",
    "            partials = Array(repeating: Tensor(0), count: metrics.count + 1)\n",
    "        }\n",
    "        \n",
    "        public override func batchDidFinish(learner: Learner) {\n",
    "            if !learner.inTrain{\n",
    "                let bs = learner.currentInput!.shape[0] //Possible because Input is TF for now\n",
    "                total += bs\n",
    "                partials[0] += Float(bs) * learner.currentLoss\n",
    "                for i in 1...metrics.count{\n",
    "                    partials[i] += Float(bs) * metrics[i-1](learner.currentOutput!, learner.currentTarget!)\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        public override func epochDidFinish(learner: Learner) {\n",
    "            for i in 0...metrics.count {partials[i] = partials[i] / Float(total)}\n",
    "            print(\"Epoch \\(learner.currentEpoch): \\(partials)\")\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    public func makeAvgMetric(metrics: [(Output, Label) -> TF]) -> AvgMetric{\n",
    "        return AvgMetric(metrics: metrics)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.delegates = [learner.makeTrainEvalDelegate(), learner.makeAvgMetric(metrics: [accuracy])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: [0.5110913, 0.8748]\n",
      "Epoch 1: [0.3804018, 0.8981]\n"
     ]
    }
   ],
   "source": [
    "learner.fit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "extension Learner {\n",
    "    public class Normalize: Delegate {\n",
    "        public let mean, std: TF\n",
    "        public init(mean: TF, std: TF){ \n",
    "            (self.mean,self.std) = (mean,std)\n",
    "        }\n",
    "        \n",
    "        public override func batchWillStart(learner: Learner) {\n",
    "            learner.currentInput = (learner.currentInput! - mean) / std\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    public func makeNormalize(mean: TF, std: TF) -> Normalize{\n",
    "        return Normalize(mean: mean, std: std)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "â–¿ 2 elements\n",
       "  - .0 : 0.13066047\n",
       "  - .1 : 0.3081079\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xTrain.mean(), xTrain.standardDeviation())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public let mnistStats = (mean: TF(0.13066047), std: TF(0.3081079))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.delegates = [learner.makeTrainEvalDelegate(), learner.makeAvgMetric(metrics: [accuracy]),\n",
    "                     learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: [0.30172938, 0.9158]\n",
      "Epoch 1: [0.2513381, 0.93]\n"
     ]
    }
   ],
   "source": [
    "learner.fit(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success\r\n"
     ]
    }
   ],
   "source": [
    "import NotebookExport\n",
    "let exporter = NotebookExport(Path.cwd/\"04_callbacks.ipynb\")\n",
    "print(exporter.export(usingPrefix: \"FastaiNotebook_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
