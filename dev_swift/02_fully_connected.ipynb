{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/home/clattner/fastai_docs/dev_swift/FastaiNotebook_01a_fastai_layers\")\n",
      "\t\tFastaiNotebook_01a_fastai_layers\n",
      "With SwiftPM flags: []\n",
      "Working in: /tmp/tmp1ofhyo8f\n",
      "/home/clattner/swift/usr/bin/swift-build: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/lib/swift/linux/libFoundation.so)\n",
      "Fetching https://github.com/mxcl/Path.swift\n",
      "Fetching https://github.com/JustHTTP/Just\n",
      "Fetching https://github.com/latenitesoft/NotebookExport\n",
      "Completed resolution in 2.04s\n",
      "Cloning https://github.com/mxcl/Path.swift\n",
      "Resolving https://github.com/mxcl/Path.swift at 0.16.2\n",
      "Cloning https://github.com/JustHTTP/Just\n",
      "Resolving https://github.com/JustHTTP/Just at 0.7.1\n",
      "Cloning https://github.com/latenitesoft/NotebookExport\n",
      "Resolving https://github.com/latenitesoft/NotebookExport at 0.5.0\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "Compile Swift Module 'Path' (9 sources)\n",
      "Compile Swift Module 'Just' (1 sources)\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "Compile Swift Module 'NotebookExport' (2 sources)\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "Compile Swift Module 'FastaiNotebook_00_load_data' (1 sources)\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "Compile Swift Module 'FastaiNotebook_01_matmul' (2 sources)\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "Compile Swift Module 'FastaiNotebook_01a_fastai_layers' (3 sources)\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "Compile Swift Module 'jupyterInstalledPackages' (1 sources)\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift)\n",
      "\n",
      "Linking ./.build/x86_64-unknown-linux/debug/libjupyterInstalledPackages.so\n",
      "/home/clattner/swift/usr/bin/swiftc: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swiftc)\n",
      "\n",
      "/home/clattner/swift/usr/bin/swift-autolink-extract: /home/clattner/anaconda3/envs/swift/lib/libuuid.so.1: no version information available (required by /home/clattner/swift/usr/bin/swift-autolink-extract)\n",
      "\n",
      "Initializing Swift...\n",
      "Loading library...\n",
      "Installation complete!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "error: <Cell 1>:1:18: error: consecutive statements on a line must be separated by ';'\n%install-location $cwd/swift-install\n                 ^\n                 ;\n\n"
     ]
    }
   ],
   "source": [
    "%install-location $cwd/swift-install\n",
    "%install '.package(path: \"$cwd/FastaiNotebook_01a_fastai_layers\")' FastaiNotebook_01a_fastai_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "import Path\n",
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FastaiNotebook_01a_fastai_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The forward and backward passes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typing `Tensor<Float>` all the time is tedious. The S4TF team expects to make `Float` be the default so we can just say `Tensor`.  Until that happens though, we can define our own alias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public typealias TF=Tensor<Float>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to normalize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func normalize(_ x:TF, mean:TF, std:TF) -> TF {\n",
    "    return (x-mean)/std\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var (xTrain, yTrain, xValid, yValid) = loadMNIST(path: mnistPath, flat: true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the training and validation sets with the training set statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13066047 0.3081078\r\n"
     ]
    }
   ],
   "source": [
    "let trainMean = xTrain.mean()\n",
    "let trainStd  = xTrain.std()\n",
    "print(trainMean, trainStd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain = normalize(xTrain, mean: trainMean, std: trainStd)\n",
    "xValid = normalize(xValid, mean: trainMean, std: trainStd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test everything is going well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public func testNearZero(_ a: TF, tolerance: Float = 1e-3) {\n",
    "    assert(abs(a) < tolerance, \"Near zero: \\(a)\")\n",
    "}\n",
    "\n",
    "public func testSame(_ a: TF, _ b: TF) {\n",
    "    // Check shapes match so broadcasting doesn't hide shape errors.\n",
    "    assert(a.shape == b.shape)\n",
    "    testNearZero(a-b)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNearZero(xTrain.mean())\n",
    "testNearZero(xTrain.std() - 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 784 10\r\n"
     ]
    }
   ],
   "source": [
    "let (n,m) = (xTrain.shape[0],xTrain.shape[1])\n",
    "let c = yTrain.max()+1\n",
    "print(n, m, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundations version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//num hidden\n",
    "let nh = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// simplified kaiming init / he init\n",
    "let w1 = TF(randomNormal: [m, nh]) / sqrt(Float(m))\n",
    "let b1 = TF(zeros: [nh])\n",
    "let w2 = TF(randomNormal: [nh,1]) / sqrt(Float(nh))\n",
    "let b2 = TF(zeros: [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testNearZero(w1.mean())\n",
    "testNearZero(w1.std()-1/sqrt(Float(m)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0060178353 1.0077001\r\n"
     ]
    }
   ],
   "source": [
    "// This should be ~ (0,1) (mean,std)...\n",
    "print(xValid.mean(), xValid.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of `@` in python we use `‚Ä¢` (or `matmul`) in Swift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func lin(_ x: TF, _ w: TF, _ b: TF) -> TF { return x‚Ä¢w+b }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let t = lin(xValid, w1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008671894 0.984596\r\n"
     ]
    }
   ],
   "source": [
    "//...so should this, because we used kaiming init, which is designed to do this\n",
    "print(t.mean(), t.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func myRelu(_ x:TF) -> TF { return max(x, 0) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let t = myRelu(lin(xValid, w1, b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39344302 0.5840166\r\n"
     ]
    }
   ],
   "source": [
    "//...actually it really should be this!\n",
    "print(t.mean(),t.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// kaiming init / he init for relu\n",
    "let w1 = TF(randomNormal: [m,nh]) * sqrt(2.0/Float(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.00028694386 0.050297257\r\n"
     ]
    }
   ],
   "source": [
    "print(w1.mean(), w1.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48257005 0.7595384\r\n"
     ]
    }
   ],
   "source": [
    "let t = myRelu(lin(xValid, w1, b1))\n",
    "print(t.mean(), t.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a simple basic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func model(_ xb: TF) -> TF {\n",
    "    let l1 = lin(xb, w1, b1)\n",
    "    let l2 = myRelu(l1)\n",
    "    let l3 = lin(l2, w2, b2)\n",
    "    return l3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average: 0.3743267 ms,   min: 0.311244 ms,   max: 0.484276 ms\r\n"
     ]
    }
   ],
   "source": [
    "time(repeating: 10) { _ = model(xValid) }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with the mean squared error to have easier gradient computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let preds = model(xTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func mse(_ out: TF, _ targ: TF) -> TF {\n",
    "    return (out.squeezingShape(at: -1) - targ).squared().mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more step comapred to Python, we have to make sure our labels are properly converted to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Convert these to Float dtype.\n",
    "var yTrainF = TF(yTrain)\n",
    "var yValidF = TF(yValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.37175\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(preds, yTrainF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients and backward pass\n",
    "\n",
    "Here we should how to calculate gradients for a simple model the hard way, manually.\n",
    "\n",
    "To store the gradients a bit like in PyTorch we introduce a `TensorWithGrad` class that has two attributes: the original tensor and the gradient. We choose a class to easily replicate the Python notebook: classes are reference types (which means they are mutable) while structures are value types.\n",
    "\n",
    "In fact, since this is the first time we're discovering Swift classes, let's jump into a [sidebar discussion about Value Semantics vs Reference Semantics](https://docs.google.com/presentation/d/1dc6o2o-uYGnJeCeyvgsgyk05dBMneArxdICW5vF75oU/edit#slide=id.g5669969ead_0_145) since it is a pretty fundamental part of the programming model and a huge step forward that Swift takes.\n",
    "\n",
    "When we get back, we'll keep charging on, even though this is very non-idiomatic Swift code!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/// WARNING: This is designed to be similar to the PyTorch 02_fully_connected lesson,\n",
    "/// this isn't idiomatic Swift code.\n",
    "class TensorWithGrad {\n",
    "    var inner, grad:  TF\n",
    "    \n",
    "    init(_ x: TF) {\n",
    "        inner = x\n",
    "        grad = TF(zeros: x.shape)\n",
    "    } \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Redefine our functions on TensorWithGrad.\n",
    "func lin(_ x: TensorWithGrad, _ w: TensorWithGrad, _ b: TensorWithGrad) -> TensorWithGrad {\n",
    "    return TensorWithGrad(matmul(x.inner, w.inner) + b.inner)\n",
    "}\n",
    "func myRelu(_ x: TensorWithGrad) -> TensorWithGrad {\n",
    "    return TensorWithGrad(max(x.inner, 0))\n",
    "}\n",
    "func mse(_ inp: TensorWithGrad, _ targ: TF) -> TF {\n",
    "    //grad of loss with respect to output of previous layer\n",
    "    return (inp.inner.squeezingShape(at: -1) - targ).squared().mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Define our gradient functions.\n",
    "func mseGrad(_ inp: TensorWithGrad, _ targ: TF) {\n",
    "    //grad of loss with respect to output of previous layer\n",
    "    inp.grad = 2.0 * (inp.inner.squeezingShape(at: -1) - targ).expandingShape(at: -1) / Float(inp.inner.shape[0])\n",
    "}\n",
    "\n",
    "func reluGrad(_ inp: TensorWithGrad, _ out: TensorWithGrad) {\n",
    "    //grad of relu with respect to input activations\n",
    "    inp.grad = (inp.inner .> 0).selecting(out.grad, TF(zeros: inp.inner.shape))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our python version (we've renamed the python `g` to `grad` for consistency):\n",
    "\n",
    "```python\n",
    "def lin_grad(inp, out, w, b):\n",
    "    inp.grad = out.grad @ w.t()\n",
    "    w.grad = (inp.unsqueeze(-1) * out.grad.unsqueeze(1)).sum(0)\n",
    "    b.grad = out.grad.sum(0)\n",
    "```\n",
    "\n",
    "In Swift `@` is spelled `‚Ä¢`, which is <kbd>option</kbd>-<kbd>8</kbd> on Mac or <kbd>compose</kbd>-<kbd>.</kbd>-<kbd>=</kbd> elsewhere. Or just use the `matmul()` function we've seen already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func linGrad(_ inp:TensorWithGrad, _ out:TensorWithGrad, _ w:TensorWithGrad, _ b:TensorWithGrad){\n",
    "    // grad of linear layer with respect to input activations, weights and bias\n",
    "    inp.grad = out.grad ‚Ä¢ w.inner.transposed()\n",
    "    w.grad = inp.inner.transposed() ‚Ä¢ out.grad\n",
    "    b.grad = out.grad.sum(squeezingAxes: 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let w1a = TensorWithGrad(w1)\n",
    "let b1a = TensorWithGrad(b1)\n",
    "let w2a = TensorWithGrad(w2)\n",
    "let b2a = TensorWithGrad(b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func forwardAndBackward(_ inp:TensorWithGrad, _ targ:TF){\n",
    "    // forward pass:\n",
    "    let l1 = lin(inp, w1a, b1a)\n",
    "    let l2 = myRelu(l1)\n",
    "    let out = lin(l2, w2a, b2a)\n",
    "    //we don't actually need the loss in backward!\n",
    "    let loss = mse(out, targ)\n",
    "    \n",
    "    // backward pass:\n",
    "    mseGrad(out, targ)\n",
    "    linGrad(l2, out, w2a, b2a)\n",
    "    reluGrad(l1, l2)\n",
    "    linGrad(inp, l1, w1a, b1a)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let inp = TensorWithGrad(xTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forwardAndBackward(inp, yTrainF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Differentiation: the functional way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few challenges with the code above:\n",
    "\n",
    " * It doesn't follow the principle of value semantics, because TensorGrad is a class.  Mutating a tensor would produce the incorrect results.\n",
    " * It doesn't compose very well - we need to keep track of values passed in the forward pass and also pass them in the backward pass.\n",
    " * It is fully dynamic, keeping track of gradients at runtime.  This interferes with the compiler's ability to perform fusion and other advanced optimizations.\n",
    " \n",
    "Swift for TensorFlow uses a language and compiler integrated automatic differentiation system, which works in a different way.  Each differentiable function gets an associated \"pullback\" (described below) that defines its gradient.  When you write a function that like `model` that calls a bunch of these in sequence, the compiler calls the function and its collects its pullback, then stitches together the pullbacks using the chain rule from Calculus.\n",
    "\n",
    "Let's remember the chain rule - it is written:\n",
    "\n",
    "$$\\frac{d}{dx}\\left[f\\left(g(x)\\right)\\right] = f'\\left(g(x)\\right)g'(x)$$\n",
    "\n",
    "Notice how the chain rule requires mixing together expressions from both the forward pass (`g()`) and the backward pass (`f'()` and `g'()`) of a computation to get the derivative.  While it is possible to calculate all the forward versions of a computation, then recompute everything needed again on the backward pass, this would be incredibly inefficient - it makes more sense to save intermediate values from the forward pass and reuse them on the backward pass.\n",
    "\n",
    "The Swift language provides the atoms we need to express this: we can represent math with function calls, and the pullback can be represented with closures.  This works out well because closures provide a natural way to capture interesting values from the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic expressions in MSE\n",
    "\n",
    "To explore this, let's look at a really simple example of this, the inner computation of MSE.  The full body of MSE looks like this:\n",
    "\n",
    "```swift\n",
    "func mse(_ inp: TF, _ targ: TF) -> TF {\n",
    "    //grad of loss with respect to output of previous layer\n",
    "    return (inp.squeezingShape(at: -1) - targ).squared().mean()\n",
    "}\n",
    "```\n",
    "\n",
    "For the purposes of our example, we're going to keep it super super simple and just focus on the `x.squared().mean()` part of the computation, which we'll write as `mseInner(x) = mean(square(x))` to align better with function composition notation.  We want a way to visualize what functions get called, so let's define a little helper that prints the name of its caller whenever it is called:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo(a:b:)\r\n",
      "bar(x:)\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "731\n"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// This function takes and returns an arbitrary value (just passing\n",
    "// it through as an identity function) but prints out the calling\n",
    "// functions name.  This is useful to see what is going on in your program.\n",
    "func trace<T>(_ v: T, function: String = #function) -> T {\n",
    "  print(function)\n",
    "  return v\n",
    "}\n",
    "\n",
    "// Try out the trace helper function.\n",
    "func foo(a: Int, b: Int) -> Int {\n",
    "  return trace(a+b)\n",
    "}\n",
    "func bar(x: Int) -> Int {\n",
    "  return trace(x*42+17)\n",
    "}\n",
    "\n",
    "foo(a: 1, b: 2)\n",
    "bar(x: 17)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ok, given that, we start by writing the implementation and gradients of these functions, and we put print statements in them so we can tell when they are called.  This looks like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func square(_ x: TF) -> TF {\n",
    "    let result = x * x\n",
    "    return trace(result)\n",
    "}\n",
    "func ùõÅsquare(_ x: TF) -> TF {\n",
    "    let result = 2 * x\n",
    "    return trace(result)\n",
    "}\n",
    "\n",
    "func mean(_ x: TF) -> TF {\n",
    "    let result = x.mean()  // this is a reduction.  (can someone write this out longhand?)\n",
    "    return trace(result)\n",
    "}\n",
    "func ùõÅmean(_ x: TF) -> TF {\n",
    "    let result = TF(1).broadcast(to: x.shape) / Float(x.shape[0])\n",
    "    return trace(result)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given these definitions we can now compute the forward and derivative of the `mseInner` function that composes `square` and `mean`, using the chain rule:\n",
    "\n",
    "$$\\frac{d}{dx}\\left[f\\left(g(x)\\right)\\right] = f'\\left(g(x)\\right)g'(x)$$\n",
    "\n",
    "where `f` is `mean` and `g` is `square`.  This gives us:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func mseInner(_ x: TF) -> TF {\n",
    "    return mean(square(x))\n",
    "}\n",
    "\n",
    "func ùõÅmseInner(_ x: TF) -> TF {\n",
    "    return ùõÅmean(square(x)) * ùõÅsquare(x)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all simple, but we have a small problem if (in the common case for deep nets) we want to calculate both the forward and the gradient computation at the same time: we end up redundantly computing `square(x)` in both the forward and backward paths!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "square(_:)\r\n",
      "mean(_:)\r\n",
      "square(_:)\r\n",
      "ùõÅmean(_:)\r\n",
      "ùõÅsquare(_:)\r\n",
      "\r\n",
      "result: 7.5\r\n",
      "gradient: [0.5, 1.0, 1.5, 2.0]\r\n"
     ]
    }
   ],
   "source": [
    "func mseInnerAndGrad(_ x: TF) -> (TF, TF) {\n",
    "  return (mseInner(x), ùõÅmseInner(x))    \n",
    "}\n",
    "\n",
    "let exampleData = TF([1, 2, 3, 4])\n",
    "\n",
    "let (mseInnerResult1, mseInnerGrad1) = mseInnerAndGrad(exampleData)\n",
    "print()\n",
    "\n",
    "print(\"result:\", mseInnerResult1)\n",
    "print(\"gradient:\", mseInnerGrad1)\n",
    "\n",
    "// Check that our gradient matches builtin S4TF's autodiff.\n",
    "let builtinGrad = gradient(at: exampleData, in: { x in (x*x).mean() })\n",
    "testSame(mseInnerGrad1, builtinGrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note above how `square` got called two times: once in the forward function and once in the gradient.  In more complicated cases, this can be an incredible amount of redundant computation, which would make performance unacceptably slow.\n",
    "\n",
    "**Exercise:** take a look what happens when you use the same techniques to implement more complex functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing recomputation with Pullbacks and VJPs\n",
    "\n",
    "We can fix this by refactoring our code.  We want to preserve the linear structure of `mseInner` that calls `square` and then `mean`, but we want to make it so the ultimate *user* of the computation can choose whether they want the gradient computation (or not) and if so, we want to minimize computation.  To do this, we have to slightly generalize our derivative functions.  While it is true that the derivative of $square(x)$ is `2*x`, this is only true for a given point `x`.\n",
    "\n",
    "If we generalize the derivative of `square` to work with an arbitrary **function**, instead of point, then we need to remember that $\\frac{d}{dx}x^2 = 2x\\frac{d}{dx}$, and therefore the derivative for `square` needs to get $\\frac{d}{dx}$ passed in from its nested function.  This form of gradient is known as a \"pullback\", and can be written like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// The pullback for the gradient of square(x).\n",
    "func ùõÅsquarePB(x: TF, ddx: TF) -> TF {\n",
    "  return ddx * trace(2*x)\n",
    "}\n",
    "\n",
    "// The pullback for the gradient of mean(x).\n",
    "func ùõÅmeanPB(x: TF, ddx: TF) -> TF {\n",
    "  return trace(ddx.broadcast(to: x.shape) / Float(x.shape[0]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this very general way of describing gradients, we now want to pull them together in a single bundle that we can keep track of: we do this by changing each atom of computation to return both a normal value and a \"pullback\" closure that produces a piece of the gradient.\n",
    "\n",
    "This form of representation is called a \"Vector Jacobian Product\" (cite??) so we will name them VJPs.  They look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Returns x*x and the pullback for the gradient of x*x.\n",
    "func squareVJP(_ x: TF) -> (TF, \n",
    "                            (TF) -> TF) {\n",
    "  return (trace(x*x), \n",
    "          { ddx in ùõÅsquarePB(x: x, ddx: ddx) })    \n",
    "}\n",
    "\n",
    "// Returns the mean of x and the pullback for the mean.\n",
    "func meanVJP(_ x: TF) -> (TF,\n",
    "                          (TF) -> TF) {\n",
    "  return (trace(x.mean()),\n",
    "          { ddx in ùõÅmeanPB(x: x, ddx: ddx) })\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this, we can now implement `mseInner` in the same \"VJP\" form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// We implement mean(square(x)) by calling each of the VJPs in turn.\n",
    "func mseInnerVJP(_ x: TF) -> (TF, \n",
    "                              (TF) -> TF) {\n",
    "\n",
    "  let (squareResult, squarePB) = squareVJP(x)\n",
    "  let (  meanResult,   meanPB) = meanVJP(squareResult)\n",
    "\n",
    "  // The result is the combination of the results and the pullbacks.\n",
    "  return (meanResult,\n",
    "          // The mseInner pullback calls the functions in reverse order.\n",
    "          { v in squarePB(meanPB(v)) })\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can choose to evaluate just the forward computation, or we can choose to run both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling the forward function:\n",
      "squareVJP(_:)\n",
      "meanVJP(_:)\n",
      "\n",
      "Calling the backward function:\n",
      "ùõÅmeanPB(x:ddx:)\n",
      "ùõÅsquarePB(x:ddx:)\n",
      "\n",
      "[0.5, 1.0, 1.5, 2.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Calling the forward function:\")\n",
    "let (mseInnerResult2, mseInnerPB2) = mseInnerVJP(exampleData)\n",
    "print()\n",
    "\n",
    "testSame(mseInnerResult2, mseInnerResult1)\n",
    "\n",
    "\n",
    "print(\"Calling the backward function:\")\n",
    "let mseInnerGrad2 = mseInnerPB2(TF(1))\n",
    "print()\n",
    "\n",
    "print(mseInnerGrad2)\n",
    "// Check that we get the same result.\n",
    "testSame(mseInnerGrad2, builtinGrad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, great - we only ran each piece of the computation once, and we gained a single conceptual abstraction that bundles everything we need together.\n",
    "\n",
    "Now we have all of the infrastructure and scaffolding necessary to define and compose computations and figure out their backwards versions from the chain rule.  Let's jump up a level to define Jeremy's example using the VJP form of the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Relu, MSE, and Lin with VJPs\n",
    "\n",
    "Lets come back to our earlier examples and define pullbacks for our primary functions in the simple model function example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func reluVJP(_ x: TF) -> (TF, (TF) -> TF) {\n",
    "    return (max(x, 0),\n",
    "            // Pullback for max(x, 0)\n",
    "            { ùõÅout -> TF in\n",
    "              (x .> 0).selecting(ùõÅout, TF(zeros: x.shape))\n",
    "            })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func linVJP(_ inp: TF, _ w: TF, _ b: TF) -> (TF, (TF) -> (TF, TF, TF)) {\n",
    "    return (inp ‚Ä¢ w + b,\n",
    "            // Pullback for inp ‚Ä¢ w + b.  Three results because 'lin' has three args.\n",
    "            { ùõÅout in\n",
    "              (ùõÅout ‚Ä¢ w.transposed(), \n",
    "               inp.transposed() ‚Ä¢ ùõÅout,\n",
    "               ùõÅout.unbroadcast(to: b.shape))\n",
    "    })\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func mseVJP(_ inp: TF, _ targ: TF) -> (TF, (TF) -> (TF)) {\n",
    "    let tmp = inp.squeezingShape(at: -1) - targ\n",
    "    \n",
    "    // We already wrote a VJP for x.square().mean(), so just reuse it.\n",
    "    let (mseInnerResult, mseInnerPB) = mseInnerVJP(tmp)\n",
    "    \n",
    "    // Return the result, and a pullback that expands back out to\n",
    "    // the input shape.\n",
    "    return (mseInnerResult, \n",
    "            { v in mseInnerPB(v).expandingShape(at: -1) })\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then our forward and backward can be refactored in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func forwardAndBackward(_ inp: TF, _ targ: TF) -> (TF, TF, TF, TF, TF){\n",
    "    //forward pass:\n",
    "    let (l1, l1PB)    = linVJP(inp, w1, b1)\n",
    "    let (l2, l2PB)    = reluVJP(l1)\n",
    "    let (out, outPB)  = linVJP(l2, w2, b2)\n",
    "    //we don't actually need the loss in backward, but we need the pullback.\n",
    "    let (loss, lossPB) = mseVJP(out, targ)\n",
    "    \n",
    "    //backward pass:\n",
    "    let ùõÅloss = TF(1) //We don't really need it but the gradient of the loss with respect to itself is 1\n",
    "    let ùõÅout = lossPB(ùõÅloss)\n",
    "    let (ùõÅl2, ùõÅw2, ùõÅb2) = outPB(ùõÅout)\n",
    "    let ùõÅl1 = l2PB(ùõÅl2)\n",
    "    let (ùõÅinp, ùõÅw1, ùõÅb1) = l1PB(ùõÅl1)\n",
    "    return (ùõÅinp, ùõÅw1, ùõÅb1, ùõÅw2, ùõÅb2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squareVJP(_:)\r\n",
      "meanVJP(_:)\r\n",
      "ùõÅmeanPB(x:ddx:)\r\n",
      "ùõÅsquarePB(x:ddx:)\r\n"
     ]
    }
   ],
   "source": [
    "let (ùõÅxTrain, ùõÅw1, ùõÅb1, ùõÅw2, ùõÅb2) = forwardAndBackward(xTrain, yTrainF)\n",
    "// Check this is still all correct\n",
    "testSame(inp.grad, ùõÅxTrain)\n",
    "testSame(w1a.grad, ùõÅw1)\n",
    "testSame(b1a.grad, ùõÅb1)\n",
    "testSame(w2a.grad, ùõÅw2)\n",
    "testSame(b2a.grad, ùõÅb2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we said before, swift operates in a different way. If we go back to what is happening in the backward pass, we go from the end result (our loss) which allows us to compute the gradient of that loss with respect to the last activations. Then consider all the layers we went through during the forward pass in the reversed order (from the last one to the first one) and for each of them, compute the gradients of the loss with respect to the inputs (and potentially parameters) from the gradients of the loss with respect to the outputs.\n",
    "\n",
    "For instance if we go back to the basic `relu_grad` function we had in python:\n",
    "```\n",
    "def relu_grad(inp, out):\n",
    "    # grad of relu with respect to input activations\n",
    "    inp.g = (inp>0).float() * out.g\n",
    "```\n",
    "we explain how we infer the gradients of the loss with respect to the inputs of the relu (`inp.g`) from the gradients of the loss with respect to the outputs of that same relu (`out.g`).\n",
    "\n",
    "Swift implements differentation in a more functional way than PyTorch: there is no grad attribute, instead we just provide that function that will take the gradients with respect to the outputs and returns the gradients with respect to the inputs (and potentially, parameters).\n",
    "\n",
    "The tricky thing is that that gradient computation often requires to have the inputs/outputs of the layer: if we look at `relu_grad` up there, it needs to know the value of `inp`. That's why we don't just write a function that does `ùõÅOut -> ùõÅInp`, but a function that will take the input and return that pullback:\n",
    "```\n",
    "(Inp) -> ((ùõÅOut) -> ùõÅInp)\n",
    "```\n",
    "\n",
    "Let's look at what it gives us for the relu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func diffRelu(_ inp: TF) -> ((TF) -> TF) {\n",
    "    return {ùõÅout -> TF in\n",
    "        (inp .> 0).selecting(ùõÅout, TF(zeros: inp.shape))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we go through our relu layer, we won't just ask for `y = relu(x)`, but will also store the pullback `pb = diffRelu(x)` for the backward pass. This will automatically capture a reference to the value of `x` that is used inside that pullback. \n",
    "\n",
    "Other differentiation functions look a bit the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func diffMse(_ inp: TF, _ targ: TF) -> ((TF) -> TF) {\n",
    "    return { ùõÅloss in\n",
    "        2.0 * (inp.squeezingShape(at: -1) - targ).expandingShape(at: -1) / Float(inp.shape[0])\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`diffMse` doesn't return gradients for targ because the function isn't differentiable with respect to that variable in general (we could of course differentiate it in this case, but we don't need those gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func diffLin(_ inp: TF, _ w: TF, _ b: TF) -> ((TF) -> (TF, TF, TF)) {\n",
    "    return { ùõÅout in\n",
    "        (ùõÅout ‚Ä¢ w.transposed(), inp.transposed() ‚Ä¢ ùõÅout, ùõÅout.unbroadcast(to: b.shape))\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`diffLin` returns the gradients with respect to all its inputs (more like inputs and parameters).\n",
    "\n",
    "Then the backward and forward pass is written like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func forwardAndBackward(_ inp: TF, _ targ: TF) -> (TF, TF, TF, TF, TF){\n",
    "    //forward pass:\n",
    "    let (l1, pbL1)    = (lin(inp, w1, b1), diffLin(inp, w1, b1))\n",
    "    let (l2, pbL2)    = (myRelu(l1), diffRelu(l1))\n",
    "    let (out, pbOut)  = (lin(l2, w2, b2), diffLin(l2, w2, b2))\n",
    "    //we don't actually need the loss in backward, but we need the pullback.\n",
    "    let (loss, pbLoss) = (mse(out, targ), diffMse(out, targ))\n",
    "    \n",
    "    //backward pass:\n",
    "    let ùõÅloss = TF(1) //We don't really need it but the gradient of the loss with respect to itself is 1\n",
    "    let ùõÅout = pbLoss(ùõÅloss)\n",
    "    let (ùõÅl2, ùõÅw2, ùõÅb2) = pbOut(ùõÅout)\n",
    "    let ùõÅl1 = pbL2(ùõÅl2)\n",
    "    let (ùõÅinp, ùõÅw1, ùõÅb1) = pbL1(ùõÅl1)\n",
    "    return (ùõÅinp, ùõÅw1, ùõÅb1, ùõÅw2, ùõÅb2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (ùõÅxTrain, ùõÅw1, ùõÅb1, ùõÅw2, ùõÅb2) = forwardAndBackward(xTrain, yTrainF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Check the gradients computed both way are the same.\n",
    "testNearZero(ùõÅxTrain - inp.grad)\n",
    "testNearZero(ùõÅw1     - w1a.grad)\n",
    "testNearZero(ùõÅb1     - b1a.grad)\n",
    "testNearZero(ùõÅw2     - w2a.grad)\n",
    "testNearZero(ùõÅb2     - b2a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the S4TF Language Integrated Autodiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare to the language-integrated Swift for TensorFlow autodiff now. We have to mark the function as `@differentiable`.  This informs the compiler that we want it to automatically generate its gradients, and causes it to emit errors if there is anything contributing to the result of the function that cannot be differentiated.\n",
    "\n",
    "The `@differentiable` attribute is normally optional in a S4TF standalone environment, but is currently required in Jupyter notebooks.  The S4TF team is planning to relax this limitation over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@differentiable\n",
    "func forward(_ inp: TF, _ targ: TF, w1: TF, b1: TF, w2: TF, b2: TF) -> TF {\n",
    "    let l1 = matmul(inp, w1) + b1\n",
    "    let l2 = relu(l1)\n",
    "    let l3 = matmul(l2, w2) + b2\n",
    "    return (l3.squeezingShape(at: -1) - targ).squared().mean()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can ask for the gradients w.r.t. any individual parameter like this: (ùõÅ‚ÇÇ is for second way of computing gradients, not gradients squared, or second order gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let ùõÅ‚ÇÇxTrain = gradient(at: xTrain) {xTrain in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)}\n",
    "let ùõÅ‚ÇÇw1 = gradient(at: w1) {w1 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)}\n",
    "let ùõÅ‚ÇÇb1 = gradient(at: b1) {b1 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)}\n",
    "let ùõÅ‚ÇÇw2 = gradient(at: w2) {w2 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)}\n",
    "let ùõÅ‚ÇÇb2 = gradient(at: b2) {b2 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)}\n",
    "\n",
    "// Check that they agree with the manually calculated gradients.\n",
    "testNearZero(ùõÅ‚ÇÇxTrain - ùõÅxTrain)\n",
    "testNearZero(ùõÅ‚ÇÇw1     - ùõÅw1)\n",
    "testNearZero(ùõÅ‚ÇÇb1     - ùõÅb1)\n",
    "testNearZero(ùõÅ‚ÇÇw2     - ùõÅw2)\n",
    "testNearZero(ùõÅ‚ÇÇb2     - ùõÅb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also ask for gradients with respect to multiple things at the same time, but unfortunately, current AD bugs prevent getting more than two gradients at a time.  We can do a little bit better than the above code like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (ùõÅ‚ÇÉxTrain, ùõÅ‚ÇÉw1) = gradient(at: xTrain, w1) {\n",
    "    xTrain, w1 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)\n",
    "}\n",
    "let (ùõÅ‚ÇÉb1, ùõÅ‚ÇÉw2) = gradient(at: b1, w2) {\n",
    "    b1, w2 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)\n",
    "}\n",
    "let ùõÅ‚ÇÉb2 = gradient(at: b2) {b2 in forward(xTrain, yTrainF, w1:w1, b1:b1, w2:w2, b2:b2)}\n",
    "\n",
    "// Check that they agree.\n",
    "testNearZero(ùõÅ‚ÇÇxTrain - ùõÅ‚ÇÉxTrain)\n",
    "testNearZero(ùõÅ‚ÇÇw1     - ùõÅ‚ÇÉw1)\n",
    "testNearZero(ùõÅ‚ÇÇb1     - ùõÅ‚ÇÉb1)\n",
    "testNearZero(ùõÅ‚ÇÇw2     - ùõÅ‚ÇÉw2)\n",
    "testNearZero(ùõÅ‚ÇÇb2     - ùõÅ‚ÇÉb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is currently pretty ugly, and even when the bugs are fixed, it still won't be very idiomatic.  A more common thing is to wrap up all your parameters into a struct, and differentiate w.r.t. all of them at the same time (which, when we refactor the code, will be our model itself).\n",
    "\n",
    "Here is an example of that:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct myParams: Differentiable {\n",
    "    public var x, w1, b1, w2, b2: TF\n",
    "}\n",
    "\n",
    "let allParams = myParams(x: xTrain, w1: w1, b1: b1, w2: w2, b2: b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// We can now get all of the gradients at once with a single call, and a single forward computation.\n",
    "let grads = gradient(at: allParams) {\n",
    "  allParams in\n",
    "    forward(allParams.x, yTrainF,\n",
    "            w1: allParams.w1, \n",
    "            b1: allParams.b1,\n",
    "            w2: allParams.w2, \n",
    "            b2: allParams.b2)\n",
    "}\n",
    "\n",
    "// Check that this still calculates the same thing.\n",
    "testNearZero(ùõÅ‚ÇÇxTrain  - grads.x)\n",
    "testNearZero(ùõÅ‚ÇÇw1      - grads.w1)\n",
    "testNearZero(ùõÅ‚ÇÇb1      - grads.b1)\n",
    "testNearZero(ùõÅ‚ÇÇw2      - grads.w2)\n",
    "testNearZero(ùõÅ‚ÇÇb2      - grads.b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted the value for your loss as well as the gradients, you just have to use `valueWithGradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (loss,grads) = valueWithGradient(at: allParams) { \n",
    "    allParams in forward(allParams.x, yTrainF, w1: allParams.w1, b1: allParams.b1, w2: allParams.w2, b2: allParams.b2)\n",
    "}\n",
    "\n",
    "testNearZero(ùõÅ‚ÇÇxTrain  - grads.x)\n",
    "testNearZero(ùõÅ‚ÇÇw1      - grads.w1)\n",
    "testNearZero(ùõÅ‚ÇÇb1      - grads.b1)\n",
    "testNearZero(ùõÅ‚ÇÇw2      - grads.w2)\n",
    "testNearZero(ùõÅ‚ÇÇb2      - grads.b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of timing our implementation gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time(repeating: 10) { _ = forwardAndBackward(xTrain, yTrainF) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time(repeating: 10) {\n",
    "    _ = valueWithGradient(at: allParams) { \n",
    "        allParams in forward(allParams.x, \n",
    "                             yTrainF, \n",
    "                             w1: allParams.w1, \n",
    "                             b1: allParams.b1, \n",
    "                             w2: allParams.w2, \n",
    "                             b2: allParams.b2)\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactor with valueWithPullback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now one thing you will have noticed, is that in our forward and backward, we often ask for a value and the pullback at the same time, that's why it's often implemented together in the primitives of S4TF:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NotebookExport\n",
    "let exporter = NotebookExport(Path.cwd/\"02_fully_connected.ipynb\")\n",
    "print(exporter.export(usingPrefix: \"FastaiNotebook_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
