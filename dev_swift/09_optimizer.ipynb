{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages:\n",
      "\t.package(path: \"/home/ubuntu/fastai_docs/dev_swift/FastaiNotebook_08a_heterogeneous_dictionary\")\n",
      "\t\tFastaiNotebook_08a_heterogeneous_dictionary\n",
      "With SwiftPM flags: []\n",
      "Working in: /tmp/tmpnfhh_xyv/swift-install\n",
      "Fetching https://github.com/mxcl/Path.swift\n",
      "Fetching https://github.com/JustHTTP/Just\n",
      "Completed resolution in 3.93s\n",
      "Cloning https://github.com/JustHTTP/Just\n",
      "Resolving https://github.com/JustHTTP/Just at 0.7.1\n",
      "Cloning https://github.com/mxcl/Path.swift\n",
      "Resolving https://github.com/mxcl/Path.swift at 0.16.2\n",
      "Compile Swift Module 'Just' (1 sources)\n",
      "Compile Swift Module 'Path' (9 sources)\n",
      "Compile Swift Module 'FastaiNotebook_08a_heterogeneous_dictionary' (13 sources)\n",
      "Compile Swift Module 'jupyterInstalledPackages' (1 sources)\n",
      "Linking ./.build/x86_64-unknown-linux/debug/libjupyterInstalledPackages.so\n",
      "Initializing Swift...\n",
      "Installation complete!\n"
     ]
    }
   ],
   "source": [
    "%install '.package(path: \"$cwd/FastaiNotebook_08a_heterogeneous_dictionary\")' FastaiNotebook_08a_heterogeneous_dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('inline', 'module://ipykernel.pylab.backend_inline')\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import FastaiNotebook_08a_heterogeneous_dictionary\n",
    "%include \"EnableIPythonDisplay.swift\"\n",
    "IPythonDisplay.shell.enable_matplotlib(\"inline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "import Path\n",
    "import TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will train on imagenette when it's possible without OOM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "//let path = downloadImagette()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "//let il = ItemList(fromFolder: path, extensions: [\"jpeg\", \"jpg\"])\n",
    "//let sd = SplitData(il, fromFunc: {grandParentSplitter(fName: $0, valid: \"val\")})\n",
    "//var (procItem,procLabel) = (NoopProcessor<Path>(),CategoryProcessor())\n",
    "//let sld = SplitLabeledData(sd, fromFunc: parentLabeler, procItem: &procItem, procLabel: &procLabel)\n",
    "//var rawData = sld.toDataBunch(itemToTensor: pathsToTensor, labelToTensor: intsToTensor)\n",
    "//let data = transformData(rawData, tfmItem: { openAndResize(fname: $0, size: 128) })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "let data = mnistDataBunch(flat: true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "let (n,m) = (60000,784)\n",
    "let c = 10\n",
    "let nHid = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "func modelInit() -> BasicModel {return BasicModel(nIn: m, nHid: nHid, nOut: c)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "//Expandable enum to have easier names than LearningRate.self.\n",
    "public struct HyperParams {\n",
    "    public static let lr = LearningRate.self\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "let hd = HeterogeneousDictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hd[HyperParams.lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "open class StatDelegate {\n",
    "    open var name: String { return \"\" }\n",
    "    var defaultConfig: HeterogeneousDictionary { return HeterogeneousDictionary() }\n",
    "    public init() {}\n",
    "    public func update(\n",
    "        state: inout [String: Tensor<Float>],\n",
    "        for param: Tensor<Float>,\n",
    "        along direction: Tensor<Float>,\n",
    "        config: inout HeterogeneousDictionary\n",
    "    ) { }\n",
    "}\n",
    "\n",
    "//export\n",
    "open class StepDelegate {\n",
    "    var defaultConfig: HeterogeneousDictionary { return HeterogeneousDictionary() }\n",
    "    public init() {}\n",
    "    public func update(\n",
    "        param: inout Tensor<Float>,\n",
    "        along direction: inout Tensor<Float>,\n",
    "        state: [String: Tensor<Float>],\n",
    "        config: inout HeterogeneousDictionary\n",
    "    ) { }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public class StatefulOptimizer<Model: Layer>\n",
    "    where Model.AllDifferentiableVariables == Model.CotangentVector{\n",
    "    public var configs: [HeterogeneousDictionary]\n",
    "    public var splitFunc: (Int) -> Int\n",
    "    public var states: [String: Model.AllDifferentiableVariables]\n",
    "    public var statDelegates: [StatDelegate]\n",
    "    public var stepDelegates: [StepDelegate]\n",
    "    public init(\n",
    "        for model: __shared Model,\n",
    "        stepDelegates: [StepDelegate],\n",
    "        statDelegates: [StatDelegate],\n",
    "        configs: [HeterogeneousDictionary],\n",
    "        splitFunc: @escaping (Int) -> Int\n",
    "    ) {\n",
    "        self.configs = Array(repeating: HeterogeneousDictionary(), count: configs.count)\n",
    "        states = [:]\n",
    "        for stepDelegate in stepDelegates {\n",
    "            for i in self.configs.indices { self.configs[i].merge(stepDelegate.defaultConfig) { (_, new) in new } }\n",
    "        }\n",
    "        for statDelegate in statDelegates {\n",
    "            for i in self.configs.indices { self.configs[i].merge(statDelegate.defaultConfig) { (_, new) in new } }\n",
    "            states[statDelegate.name] = Model.AllDifferentiableVariables.zero\n",
    "        }\n",
    "        for i in 0..<configs.count {\n",
    "            self.configs[i].merge(configs[i]) { (_, new) in new }\n",
    "        }\n",
    "        self.stepDelegates = stepDelegates\n",
    "        self.statDelegates = statDelegates\n",
    "        self.splitFunc = splitFunc\n",
    "    }\n",
    "        \n",
    "    func update(\n",
    "        _ model: inout Model.AllDifferentiableVariables,\n",
    "        along direction: Model.CotangentVector\n",
    "    ) {\n",
    "        for (i,kp) in model.keyPaths.enumerated() {\n",
    "            var grad = direction[keyPath: kp]\n",
    "            var state = states.mapValues(){$0[keyPath: kp]}\n",
    "            var config = configs[splitFunc(i)]\n",
    "            for statDelegate in statDelegates {\n",
    "                statDelegate.update(\n",
    "                    state: &state,\n",
    "                    for: model[keyPath: kp],\n",
    "                    along: grad,\n",
    "                    config: &config\n",
    "                )\n",
    "            }\n",
    "            for n in states.keys { states[n]![keyPath: kp] = state[n]! }\n",
    "            for stepDelegate in stepDelegates {\n",
    "                stepDelegate.update(\n",
    "                    param: &model[keyPath: kp],\n",
    "                    along: &grad,\n",
    "                    state: state,\n",
    "                    config: &config\n",
    "                )\n",
    "            }\n",
    "            configs[splitFunc(i)] = config\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conformance to the optimizer protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "extension StatefulOptimizer: Optimizer{\n",
    "    public var learningRate: Float {\n",
    "        get { return configs.last![HyperParams.lr] } \n",
    "        set { \n",
    "            for i in configs.indices {self.configs[i][HyperParams.lr] = newValue }\n",
    "        }\n",
    "    }\n",
    "    public var learningRates: [Float] {\n",
    "        get {\n",
    "            var res: [Float] = []\n",
    "            for config in configs {res.append(config[HyperParams.lr])}\n",
    "            return res\n",
    "        }\n",
    "        set { \n",
    "            for i in configs.indices {self.configs[i][HyperParams.lr] = newValue[i] } \n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convenience init when there are no param groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "extension StatefulOptimizer{\n",
    "    public convenience init (for model: __shared Model,\n",
    "                             stepDelegates: [StepDelegate],\n",
    "                             statDelegates: [StatDelegate],\n",
    "                             config: HeterogeneousDictionary) {\n",
    "        self.init(for: model,\n",
    "                  stepDelegates: stepDelegates,\n",
    "                  statDelegates: statDelegates,\n",
    "                  configs: [config],\n",
    "                  splitFunc: { _ in return 0 })\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public class SGDStep: StepDelegate {\n",
    "    override public func update(\n",
    "        param: inout Tensor<Float>,\n",
    "        along direction: inout Tensor<Float>,\n",
    "        state: [String: Tensor<Float>],\n",
    "        config: inout HeterogeneousDictionary\n",
    "    ) {\n",
    "        param -= direction * config[HyperParams.lr]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test discriminative learning rates/freezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "func splitFunc(_ a: Int) -> Int { return a < 2 ? 0 : 1 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "var configs = [HeterogeneousDictionary(HyperParams.lr, 0.0), HeterogeneousDictionary(HyperParams.lr, 0.01)]\n",
    "func optFunc(_ model: BasicModel) -> StatefulOptimizer<BasicModel> {\n",
    "    return StatefulOptimizer(\n",
    "        for: model,\n",
    "        stepDelegates: [SGDStep()],\n",
    "        statDelegates: [],\n",
    "        configs: configs,\n",
    "        splitFunc: splitFunc)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "let params = learner.model.allDifferentiableVariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.011346576,   0.07041741,   0.04828647,  -0.01670533,  0.030135188, 0.0103605315,\r\n",
      "  0.026507542,  -0.06794764, 0.0073679807,  -0.06864424,   0.04513281,  0.021142509,\r\n",
      " -0.033019923,  -0.03901811, -0.033462703,  0.008261628, -0.031037694,  0.042118628,\r\n",
      "   0.07596889,  0.036865607,  0.051948726, -0.030733285, -0.033218313, -0.026557663,\r\n",
      "   0.02765668,   0.02337122, -0.055277444, -0.069751285,   0.08353857,  -0.06666172,\r\n",
      "   0.06451453,  0.068301104,   0.04553501,  -0.08128223, -0.003940547,  -0.07533541,\r\n",
      " -0.052567877,  0.008566229,  -0.07603608,   0.05354427, -0.083799586,   0.03110369,\r\n",
      "  -0.08248626,   0.07308779,  0.014221372,  -0.03750168, -0.076474555,  0.038552217,\r\n",
      "  0.006278651,  0.046993166]\r\n",
      "0.0\r\n",
      "[  0.27484033,   0.10599142,  -0.23542562, -0.019449878,  0.035718746,  -0.26821455,\r\n",
      "  -0.25172606, -0.007853702,   0.31435993,  0.032631565]\r\n",
      "0.0\r\n"
     ]
    }
   ],
   "source": [
    "for kp in params.recursivelyAllWritableKeyPaths(to: TF.self) { \n",
    "    print(params[keyPath: kp][0]) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: [1.2091199, 0.6484]                                                    \n",
      "Epoch 1: [0.98678046, 0.7155]                                                   \n",
      "                                                                              \r"
     ]
    }
   ],
   "source": [
    "learner.fit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.011346576,   0.07041741,   0.04828647,  -0.01670533,  0.030135188, 0.0103605315,\r\n",
      "  0.026507542,  -0.06794764, 0.0073679807,  -0.06864424,   0.04513281,  0.021142509,\r\n",
      " -0.033019923,  -0.03901811, -0.033462703,  0.008261628, -0.031037694,  0.042118628,\r\n",
      "   0.07596889,  0.036865607,  0.051948726, -0.030733285, -0.033218313, -0.026557663,\r\n",
      "   0.02765668,   0.02337122, -0.055277444, -0.069751285,   0.08353857,  -0.06666172,\r\n",
      "   0.06451453,  0.068301104,   0.04553501,  -0.08128223, -0.003940547,  -0.07533541,\r\n",
      " -0.052567877,  0.008566229,  -0.07603608,   0.05354427, -0.083799586,   0.03110369,\r\n",
      "  -0.08248626,   0.07308779,  0.014221372,  -0.03750168, -0.076474555,  0.038552217,\r\n",
      "  0.006278651,  0.046993166]\r\n",
      "0.0\r\n",
      "[   0.3455905,  -0.07183893,  -0.24448909,   0.42230362, 0.0035256494, -0.026883557,\r\n",
      "  -0.18190405, -0.054050997,  0.013086483,   -0.2244681]\r\n",
      "-0.056190923\r\n"
     ]
    }
   ],
   "source": [
    "let params = learner.model.allDifferentiableVariables\n",
    "for kp in params.recursivelyAllWritableKeyPaths(to: TF.self) { \n",
    "    print(params[keyPath: kp][0]) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public struct WeightDecayKey: HetDictKey { public static var defaultValue: Float = 0 }\n",
    "\n",
    "public extension HyperParams {\n",
    "    static let wd = WeightDecayKey.self\n",
    "}\n",
    "\n",
    "public class WeightDecay: StepDelegate {\n",
    "    override public func update(\n",
    "        param: inout Tensor<Float>,\n",
    "        along direction: inout Tensor<Float>,\n",
    "        state: [String: Tensor<Float>],\n",
    "        config: inout HeterogeneousDictionary\n",
    "    ) {\n",
    "        param *= 1 - config[HyperParams.lr] * config[HyperParams.wd]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public class L2Regularization: StepDelegate {\n",
    "    override public func update(\n",
    "        param: inout Tensor<Float>,\n",
    "        along direction: inout Tensor<Float>,\n",
    "        state: [String: Tensor<Float>],\n",
    "        config: inout HeterogeneousDictionary\n",
    "    ) {\n",
    "        direction += config[HyperParams.wd] * param\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "//Expandable enum to have tab completes/typo-proof for state variable names.\n",
    "public struct StateKeys {\n",
    "    public static let avgGrad = \"averageGrad\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "\n",
    "public struct Momentum: HetDictKey { public static var defaultValue: Float = 0.9 }\n",
    "public struct MomentumDampening: HetDictKey, Equatable { public static var defaultValue: Float = 0.1 }\n",
    "public extension HyperParams {\n",
    "    static let mom = Momentum.self\n",
    "    static let momDamp = MomentumDampening.self\n",
    "}\n",
    "\n",
    "public class AverageGrad: StatDelegate {\n",
    "    public let dampened: Bool\n",
    "    public init(dampened: Bool = false) { self.dampened = dampened }\n",
    "    override public var name: String { return StateKeys.avgGrad }\n",
    "    override public func update(\n",
    "        state: inout [String: Tensor<Float>],\n",
    "        for param: Tensor<Float>,\n",
    "        along direction: Tensor<Float>,\n",
    "        config: inout HeterogeneousDictionary\n",
    "    ) {\n",
    "        state[StateKeys.avgGrad]! *= config[HyperParams.mom]\n",
    "        config[HyperParams.momDamp] = 1.0 - (dampened ? config[HyperParams.mom] : 0.0)\n",
    "        state[StateKeys.avgGrad]! += config[HyperParams.momDamp] * direction\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public class MomentumStep: StepDelegate {\n",
    "    override public func update(\n",
    "        param: inout Tensor<Float>,\n",
    "        along direction: inout Tensor<Float>,\n",
    "        state: [String: Tensor<Float>],\n",
    "        config: inout HeterogeneousDictionary\n",
    "    ) {\n",
    "        param -= state[StateKeys.avgGrad]! * config[HyperParams.lr]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func optFunc(_ model: BasicModel) -> StatefulOptimizer<BasicModel> {\n",
    "    return StatefulOptimizer(\n",
    "        for: model,\n",
    "        stepDelegates: [MomentumStep()],\n",
    "        statDelegates: [AverageGrad()],\n",
    "        config: HeterogeneousDictionary(HyperParams.lr, 0.01))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: [0.15794644, 0.9506]                                                   \n",
      "Epoch 1: [0.12316208, 0.9624]                                                   \n",
      "                                                                              \r"
     ]
    }
   ],
   "source": [
    "learner.fit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public struct SquareMomentum: HetDictKey { public static var defaultValue: Float = 0.99 }\n",
    "public struct SquareMomentumDampening: HetDictKey { public static var defaultValue: Float = 0.99 }\n",
    "\n",
    "public extension HyperParams {\n",
    "    static let ²mom = Momentum.self\n",
    "    static let ²momDamp = MomentumDampening.self\n",
    "}\n",
    "\n",
    "public extension StateKeys {\n",
    "    static let avgSqr = \"averageSquaredGrad\"\n",
    "}\n",
    "\n",
    "public class AverageSquaredGrad: StatDelegate {\n",
    "    let dampened: Bool\n",
    "    public init(dampened: Bool = true) { self.dampened = dampened }\n",
    "    override public var name: String { return StateKeys.avgSqr }\n",
    "    override public func update(\n",
    "        state: inout [String: Tensor<Float>],\n",
    "        for param: Tensor<Float>,\n",
    "        along direction: Tensor<Float>,\n",
    "        config: inout HeterogeneousDictionary\n",
    "    ) {\n",
    "        state[StateKeys.avgSqr]! *= config[HyperParams.²mom]\n",
    "        config[HyperParams.²momDamp] = 1.0 - (dampened ? config[HyperParams.²mom] : 0.0)\n",
    "        state[StateKeys.avgSqr]! += config[HyperParams.²momDamp] * direction.squared()\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public extension StateKeys {\n",
    "    static let step = \"stepCount\"\n",
    "}\n",
    "\n",
    "public class StepCount: StatDelegate {\n",
    "    override public var name: String { return StateKeys.step }\n",
    "    override public func update(\n",
    "        state: inout [String: Tensor<Float>],\n",
    "        for param: Tensor<Float>,\n",
    "        along direction: Tensor<Float>,\n",
    "        config: inout HeterogeneousDictionary\n",
    "    ) {\n",
    "        state[StateKeys.step]! += 1.0\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public struct Epsilon: HetDictKey { public static var defaultValue: Float = 1e-5 }\n",
    "public extension HyperParams {\n",
    "    static let eps = Epsilon.self\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//export\n",
    "public class AdamStep: StepDelegate {\n",
    "    override public func update(\n",
    "        param: inout Tensor<Float>,\n",
    "        along direction: inout Tensor<Float>,\n",
    "        state: [String: Tensor<Float>],\n",
    "        config: inout HeterogeneousDictionary\n",
    "    ) {\n",
    "        let step = state[StateKeys.step]!\n",
    "        let (mom,damp) = (config[HyperParams.mom],config[HyperParams.momDamp])\n",
    "        let debias1 = damp * (1 - pow(mom, step)) / (1 - mom)\n",
    "        let num = debias1 * state[StateKeys.avgGrad]!\n",
    "        \n",
    "        let (²mom,²damp) = (config[HyperParams.²mom],config[HyperParams.²momDamp])\n",
    "        let debias2 = ²damp * (1 - pow(²mom, step)) / (1 - ²mom)\n",
    "        let denom = sqrt(state[StateKeys.avgSqr]!/debias2) + config[HyperParams.eps]\n",
    "        \n",
    "        param -= config[HyperParams.lr] * num / denom\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func optFunc(_ model: BasicModel) -> StatefulOptimizer<BasicModel> {\n",
    "    return StatefulOptimizer(\n",
    "        for: model,\n",
    "        stepDelegates: [AdamStep()], \n",
    "        statDelegates: [AverageGrad(dampened: true), AverageSquaredGrad(), StepCount()], \n",
    "        config: HeterogeneousDictionary(HyperParams.lr, 0.01))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let learner = Learner(data: data, lossFunc: softmaxCrossEntropy, optFunc: optFunc, modelInit: modelInit)\n",
    "let recorder = learner.makeDefaultDelegates(metrics: [accuracy])\n",
    "learner.delegates.append(learner.makeNormalize(mean: mnistStats.mean, std: mnistStats.std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: [0.26115474, 0.9323]                                                   \n",
      "Epoch 1: [0.24847078, 0.9338]                                                   \n",
      "                                                                              \r"
     ]
    }
   ],
   "source": [
    "learner.fit(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambStep: StepDelegate {\n",
    "    override var defaultConfig: HeterogeneousDictionary {\n",
    "        return HeterogeneousDictionary(HyperParams.eps, 1e-6, HyperParams.wd, 0.0)\n",
    "    }\n",
    "    override func update(\n",
    "        param: inout Tensor<Float>,\n",
    "        along direction: inout Tensor<Float>,\n",
    "        state: [String: Tensor<Float>],\n",
    "        config: inout HeterogeneousDictionary\n",
    "    ) {\n",
    "        let stepCount = state[StateKeys.step]!\n",
    "        let (mom,damp) = (config[HyperParams.mom],config[HyperParams.momDamp])\n",
    "        let debias1 = damp * (1 - pow(mom, stepCount)) / (1 - mom)\n",
    "        let num = debias1 * state[StateKeys.avgGrad]!\n",
    "        \n",
    "        let (²mom,²damp) = (config[HyperParams.²mom],config[HyperParams.²momDamp])\n",
    "        let debias2 = ²damp * (1 - pow(²mom, stepCount)) / (1 - ²mom)\n",
    "        let denom = sqrt(state[StateKeys.avgSqr]!/debias2) + config[HyperParams.eps]\n",
    "        \n",
    "        let step = num / denom + config[HyperParams.wd] * param\n",
    "        let r1 = sqrt((param * param).mean())\n",
    "        let r2 = sqrt((step * step).mean())\n",
    "        let factor = min(r1 / r2, Float(10.0))\n",
    "        param -= config[HyperParams.lr] * factor * step\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making convenienc functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func SGDOpt<Model>(for model: Model, lr: Float, mom: Float = 0.9, wd: Float = 0.0, dampening: Bool = false\n",
    "                         ) -> StatefulOptimizer<Model> {\n",
    "    var steppers = (mom != 0) ? [MomentumStep()] : [SGDStep()]\n",
    "    if wd != 0 { steppers.append(WeightDecay()) }\n",
    "    let stats = (mom != 0) ? [AverageGrad(dampened: dampening)] : []\n",
    "    var config = HeterogeneousDictionary(HyperParams.lr, lr)\n",
    "    if mom != 0 { config[HyperParams.mom] = mom }\n",
    "    if wd != 0  { config[HyperParams.wd ] = wd  }\n",
    "    return StatefulOptimizer(for: model, stepDelegates: steppers, statDelegates: stats, config: config)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// export\n",
    "public func AdamOpt<Model>(for model: Model, lr: Float, mom: Float = 0.9, beta: Float=0.99, wd: Float = 0.0, eps: Float = 1e-5\n",
    "                         ) -> StatefulOptimizer<Model> {\n",
    "    var steppers: [StepDelegate] = [AdamStep()]\n",
    "    if wd != 0 { steppers.append(WeightDecay()) }\n",
    "    let stats = [AverageGrad(dampened: true), AverageSquaredGrad(), StepCount()]\n",
    "    var config = HeterogeneousDictionary(HyperParams.lr, lr)\n",
    "    config[HyperParams.mom] = mom\n",
    "    config[HyperParams.²mom] = beta\n",
    "    config[HyperParams.eps] = eps\n",
    "    if wd != 0  { config[HyperParams.wd ] = wd  }\n",
    "    return StatefulOptimizer(for: model, stepDelegates: steppers, statDelegates: stats, config: config)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebookToScript(fname: (Path.cwd / \"09_optimizer.ipynb\").string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Swift",
   "language": "swift",
   "name": "swift"
  },
  "language_info": {
   "file_extension": ".swift",
   "mimetype": "text/x-swift",
   "name": "swift",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
